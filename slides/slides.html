<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=UTF-8" http-equiv="Content-Type" />
<meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible" />
<meta content="Asciidoctor 1.5.2" name="generator" />
<title>DS320: Spark</title>
<link href="deck.js/themes/style/font.css" rel="stylesheet" />
<style>
.conum { display: inline-block; color: white !important; background-color: #222222; -webkit-border-radius: 100px; border-radius: 100px; text-align: center; width: 1.2em; height: 1.2em; font-size: 0.9em; font-weight: bold; line-height: 1.2; font-family: Arial, sans-serif; font-style: normal; position: relative; top: -0.1em; }
.conum * { color: white !important; }
.conum + b { display: none; }
.conum:after { content: attr(data-value); }
.conum:not([data-value]):empty { display: none; }
.colist table td:first-of-type { padding-right: 0.25em; }
</style>
<style>
/* Stylesheet for CodeRay to match GitHub theme | MIT License | http://foundation.zurb.com */
/*pre.CodeRay {background-color:#f7f7f8;}*/
.CodeRay .line-numbers{border-right:1px solid #d8d8d8;padding:0 0.5em 0 .25em}
.CodeRay span.line-numbers{display:inline-block;margin-right:.5em;color:rgba(0,0,0,.3)}
.CodeRay .line-numbers strong{font-weight: normal}
table.CodeRay{border-collapse:separate;border-spacing:0;margin-bottom:0;border:0;background:none}
table.CodeRay td{vertical-align: top}
table.CodeRay td.line-numbers{text-align:right}
table.CodeRay td.line-numbers>pre{padding:0;color:rgba(0,0,0,.3)}
table.CodeRay td.code{padding:0 0 0 .5em}
table.CodeRay td.code>pre{padding:0}
.CodeRay .debug{color:#fff !important;background:#000080 !important}
.CodeRay .annotation{color:#007}
.CodeRay .attribute-name{color:#000080}
.CodeRay .attribute-value{color:#700}
.CodeRay .binary{color:#509}
.CodeRay .comment{color:#998;font-style:italic}
.CodeRay .char{color:#04d}
.CodeRay .char .content{color:#04d}
.CodeRay .char .delimiter{color:#039}
.CodeRay .class{color:#458;font-weight:bold}
.CodeRay .complex{color:#a08}
.CodeRay .constant,.CodeRay .predefined-constant{color:#008080}
.CodeRay .color{color:#099}
.CodeRay .class-variable{color:#369}
.CodeRay .decorator{color:#b0b}
.CodeRay .definition{color:#099}
.CodeRay .delimiter{color:#000}
.CodeRay .doc{color:#970}
.CodeRay .doctype{color:#34b}
.CodeRay .doc-string{color:#d42}
.CodeRay .escape{color:#666}
.CodeRay .entity{color:#800}
.CodeRay .error{color:#808}
.CodeRay .exception{color:inherit}
.CodeRay .filename{color:#099}
.CodeRay .function{color:#900;font-weight:bold}
.CodeRay .global-variable{color:#008080}
.CodeRay .hex{color:#058}
.CodeRay .integer,.CodeRay .float{color:#099}
.CodeRay .include{color:#555}
.CodeRay .inline{color:#00}
.CodeRay .inline .inline{background:#ccc}
.CodeRay .inline .inline .inline{background:#bbb}
.CodeRay .inline .inline-delimiter{color:#d14}
.CodeRay .inline-delimiter{color:#d14}
.CodeRay .important{color:#555;font-weight:bold}
.CodeRay .interpreted{color:#b2b}
.CodeRay .instance-variable{color:#008080}
.CodeRay .label{color:#970}
.CodeRay .local-variable{color:#963}
.CodeRay .octal{color:#40e}
.CodeRay .predefined{color:#369}
.CodeRay .preprocessor{color:#579}
.CodeRay .pseudo-class{color:#555}
.CodeRay .directive{font-weight:bold}
.CodeRay .type{font-weight:bold}
.CodeRay .predefined-type{color:inherit}
.CodeRay .reserved,.CodeRay .keyword {color:#000;font-weight:bold}
.CodeRay .key{color:#808}
.CodeRay .key .delimiter{color:#606}
.CodeRay .key .char{color:#80f}
.CodeRay .value{color:#088}
.CodeRay .regexp .delimiter{color:#808}
.CodeRay .regexp .content{color:#808}
.CodeRay .regexp .modifier{color:#808}
.CodeRay .regexp .char{color:#d14}
.CodeRay .regexp .function{color:#404;font-weight:bold}
.CodeRay .string{color:#d20}
.CodeRay .string .string .string{background:#ffd0d0}
.CodeRay .string .content{color:#d14}
.CodeRay .string .char{color:#d14}
.CodeRay .string .delimiter{color:#d14}
.CodeRay .shell{color:#d14}
.CodeRay .shell .delimiter{color:#d14}
.CodeRay .symbol{color:#990073}
.CodeRay .symbol .content{color:#a60}
.CodeRay .symbol .delimiter{color:#630}
.CodeRay .tag{color:#008080}
.CodeRay .tag-special{color:#d70}
.CodeRay .variable{color:#036}
.CodeRay .insert{background:#afa}
.CodeRay .delete{background:#faa}
.CodeRay .change{color:#aaf;background:#007}
.CodeRay .head{color:#f8f;background:#505}
.CodeRay .insert .insert{color:#080}
.CodeRay .delete .delete{color:#800}
.CodeRay .change .change{color:#66f}
.CodeRay .head .head{color:#f4f}
</style>
<link href="deck.js/core/deck.core.css" rel="stylesheet" />
<link href="deck.js/extensions/scale/deck.scale.css" media="screen" rel="stylesheet" />
<link href="deck.js/themes/style/datastax.css" media="screen" rel="stylesheet" />
<link href="deck.js/themes/transition/fade.css" media="screen" rel="stylesheet" />
<link href="deck.js/core/print.css" media="print" rel="stylesheet" />
<script src="deck.js/modernizr.custom.js"></script>
</head>
<body class="article">
<div class="deck-container">
<section class="slide" id="title-slide">
<h1>DS320: Spark</h1>
</section>
<section class="slide" id="spark-data-analytics-data-analytics">
<h2>Data Analytics</h2>
<div class="paragraph"><p><em><strong>Definition</strong></em></p></div>
<div class="verseblock">
<pre class="content">Science and craft of building applications from data analysis steps to
discover useful information and support data-driven decision making</pre>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="paragraph"><p><em><strong>Use cases</strong></em></p></div>
<div class="ulist">
<ul>
<li>Recommendations</li>
<li>Fraud detection</li>
<li>Social networks and Web link analysis</li>
<li>Marketing and advertising decisions</li>
<li>Customer 360</li>
<li>Sales and stock market analytics</li>
<li>IoT analytics</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Data analytics is a big and complex field, affecting all areas of our life.</p></div>
</div>
</div>
</section>
<section class="slide" id="analysis-steps">
<h2>Analysis Steps</h2>
<div class="ulist">
<ul>
<li>Statistical analysis</li>
<li>Classification</li>
<li>Clustering</li>
<li>Regression</li>
<li>Similarity matching</li>
<li>Collaborative filtering</li>
<li>Profiling</li>
<li>Dimensionality reduction</li>
<li>Feature extraction</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Analytical applications or analysis workflows are typically composed of many analysis steps.</p></div>
</div>
</div>
</section>
<section class="slide" id="analytical-application-lifecycle">
<h2>Analytical Application Lifecycle</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="design" src="images/spark/data-analytics/data-analytics/design.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This illustration shows CRISP-DM&#8201;&#8212;&#8201;Cross Industry Standard Process for Data Mining.</p></div>
<div class="paragraph"><p>There is a related standard called SEMMA&#8201;&#8212;&#8201;Sample, Explore, Modify, Model,	Assess.
SEMMA is similar to CRISP-DM but does not cover <em>Business understanding</em>
and <em>Deployment</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="the-roles-of-cassandra-and-spark">
<h2>The Roles of Cassandra and Spark</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="design2" src="images/spark/data-analytics/data-analytics/design2.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Cassandra stores huge amounts of data.</p></div>
<div class="paragraph"><p>Spark is a computation engine:</p></div>
<div class="ulist">
<ul>
<li><p>
<em>Data preparation</em>:<div class="ulist">
<ul>
<li>Sanitize, normalize, and scale data</li>
<li>Convert data to a more suitable format</li>
<li>Integrate data from multiple sources</li>
</ul>
</div></p></li>
<li><p>
<em>Modeling</em>:<div class="ulist">
<ul>
<li>(Mathematical) predictive, causal, descriptive models</li>
<li>Machine learning</li>
<li>Data mining</li>
</ul>
</div></p></li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="cassandra">
<h2>Cassandra</h2>
<div class="paragraph"><p><strong>Distributed transactional database designed for big data and high availability</strong></p></div>
<div class="ulist">
<ul>
<li>Millions transactions per second</li>
<li>1000-node cluster scalability</li>
<li>Millisecond response time and linear scalability</li>
<li>Extremely high availability and partition tolerance</li>
<li>Seamless multi data center support</li>
</ul>
</div>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="cassandra" src="images/spark/data-analytics/data-analytics/cassandra.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Cassandra is a perfect fit for the job!</p></div>
</div>
</div>
</section>
<section class="slide" id="spark">
<h2>Spark</h2>
<div class="paragraph"><p><strong>Distributed computation engine designed for big data and in-memory processing</strong></p></div>
<div class="ulist">
<ul>
<li>Interactive and batch analytics</li>
<li>Up to 100x faster than Apache Hadoop&#8482;</li>
<li>5-10x less code than Apache Hadoop&#8482;</li>
<li>Efficiency and scalability</li>
<li>Fault-tolerance</li>
</ul>
</div>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="spark" src="images/spark/data-analytics/data-analytics/spark.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark is a perfect fit for the job!</p></div>
</div>
</div>
</section>
<section class="slide" id="datastax-enterprise">
<h2>DataStax Enterprise</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="dse" src="images/spark/data-analytics/data-analytics/dse.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This is the ultimate solution for your enterprise!</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-architecture-architecture">
<h2>Spark Architecture</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="spark" src="images/spark/architecture/architecture/spark.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="ulist">
<ul>
<li>Master-worker architecture</li>
<li>Master (aka cluster manager) manages Workers and their resources</li>
<li>Workers instantiate Executors and give them resources (cores and memory)</li>
<li>Driver schedules computation directly with Executors</li>
<li><p>
Failure tolerance<div class="ulist">
<ul>
<li>Worker/Executor - no problem; computation is picked up by remaining Workers/Executors</li>
<li>Master - new Master is elected (DSE feature); running applications are not affected; new applications
are only affected temporarily until new Master is started</li>
</ul>
</div></p></li>
<li><p>
Notes:<div class="ulist">
<ul>
<li>DSE Spark cluster manager is standalone (open-source Spark also supports Apache Mesos&#8482; and Apache Hadoop&#8482; YARN)</li>
<li>If Driver requests more resources than a cluster can supply, Master will respond with "not enough resources"</li>
<li>Master, Worker, Executor are all separate JVMs</li>
</ul>
</div></p></li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="spark-computation-dag-jobs-stages-and-tasks">
<h2>Spark Computation: DAG, Jobs, Stages, and Tasks</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="computation" src="images/spark/architecture/architecture/computation.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="ulist">
<ul>
<li>DAG stands for Directed Acyclic Graph of Operations</li>
<li>DAG is divided into Jobs (Job per action)</li>
<li>Job is divided into Stages (a shuffling operation or action delimits a Stage)</li>
<li>Stages have identical Tasks (Task per RDD partition)</li>
<li>Tasks are executed by Executors in parallel</li>
<li>Executors hold input and output RDD partitions in cache (and sometimes on disk)</li>
<li>Intermediate results are never transferred to Driver but <em>SparkContext</em> knows about
their locations to schedule subsequent computation</li>
<li>Note:  Stages 3 and 4 will be skipped if the results of Stages 0 and 1 are cached</li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="cassandra-architecture">
<h2>Cassandra Architecture</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="cassandra" src="images/spark/architecture/architecture/cassandra.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This is a quick review of what you already know about Cassandra:</p></div>
<div class="ulist">
<ul>
<li>Peer-to-peer architecture</li>
<li>Failure tolerance/availability</li>
<li>Cassandra token ring</li>
<li>Data structures (table)</li>
<li>Data distribution (partition key)</li>
<li>Data replication (replication factor)</li>
<li>Data consistency (consistency level)</li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="dse-integration-of-spark-and-cassandra">
<h2>DSE Integration of Spark and Cassandra</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="integration" src="images/spark/architecture/architecture/integration.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="ulist">
<ul>
<li>Each physical node runs both Cassandra and Spark JVMs to achieve the best data locality</li>
<li><em>Spark-Cassandra Connector</em> enables communication between Spark and Cassandra components</li>
<li><em>Spark-Cassandra Connector</em> is a library for (primarily) retrieving data from and storying data into Cassandra</li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="multi-data-center-deployment">
<h2>Multi Data Center Deployment</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="deployment" src="images/spark/architecture/architecture/deployment.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In production, it is very important to separate transactional and analytical workloads into two or more data centers.</p></div>
<div class="ulist">
<ul>
<li><em>DC Operations</em> (Cassandra only): fast real-time transactions</li>
<li><em>DC Analytics</em> (Cassandra + Spark): batch and near-real-time, interactive jobs</li>
</ul>
</div>
<div class="paragraph"><p><em>DC Operations</em> collects data and serves real-time applications. Data is typically replicated to <em>DC Analytics</em> automatically.</p></div>
<div class="paragraph"><p><em>DC Analytics</em> uses Spark to retrieve data from Cassandra, run expensive analysis, and store results back to Cassandra.
Results may or may not be replicated back to <em>DC Operations</em>, depending on application
requirements (readily controlled by replication strategy settings for a keyspace).</p></div>
<div class="paragraph"><p>There can be, of course, multiple operational and analytical data centers.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-tools-spark-shell">
<h2>Spark Shell</h2>
<div class="paragraph"><p><strong>Interactive Scala-REPL-based Spark client</strong></p></div>
<div class="ulist">
<ul>
<li>Interpreter for Scala and Spark Scala API</li>
<li><p>
Interpreter-aware, predefined objects<div class="ulist">
<ul>
<li><em>SparkContext</em>&#8201;&#8212;&#8201;<em>sc</em></li>
<li><em>SQLContext</em>&#8201;&#8212;&#8201;<em>sqlContext</em></li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Scala REPL = Scala Read-Eval-Print-Loop, which is also known as interpreter.</p></div>
<div class="paragraph"><p>We will use Spark Shell most of the time to demonstrate various
Spark Scala API calls.</p></div>
<div class="paragraph"><p>There are also Python and R interactive clients called <em>pyspark</em> and <em>sparkR</em>, respectively.</p></div>
</div>
</div>
</section>
<section class="slide" id="starting-dse-spark-shell">
<h2>Starting DSE Spark Shell</h2>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>$ dse spark

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.x.x
      /_/

Using Scala version 2.x.x

Type in expressions to have them evaluated.
Type :help for more information.

scala&gt;</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Command <em>dse spark</em> starts the DSE variant of Spark Shell.
In open-source Spark, the command is <em>spark-shell</em>.</p></div>
<div class="paragraph"><p>This is a sample output with some lines omitted.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-shell-options">
<h2>Spark Shell Options</h2>
<div class="paragraph"><p><strong>An incomplete list of options to get you started</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:28%" />
<col style="width:57%" />
<col style="width:14%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Option</th>
<th class="tableblock halign-left valign-top">Description</th>
<th class="tableblock halign-left valign-top">Dafault value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--help</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Output usage information.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">N/A</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--master MASTER_URL</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Specify a Master URL as <em>spark://host:port</em> or <em>local[N]</em>,
where <em>N</em> is a desired number of threads.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>local</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--name NAME</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Specify a name for your Spark Shell application.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Spark shell</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--driver-memory MEM</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Specify how much memory to use for Driver.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>512M</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--executor-memory MEM</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Specify how much memory to use per Executor.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>1G</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--total-executor-cores NUM</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Specify a total number of cores for all Executors.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">All available</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>There are a number of other options that you should explore.</p></div>
</div>
</div>
</section>
<section class="slide" id="examples-of-using-options">
<h2>Examples of Using Options</h2>
<div class="listingblock">
<div class="title"><em>Connecting to a remote cluster</em></div>
<div class="content">
<pre class="CodeRay"><code>dse spark --master spark://172.31.20.18:7077 \
          --name "Artems's Shell" \
          --executor-memory 2G \
          --total-executor-cores 4</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Running locally</em></div>
<div class="content">
<pre class="CodeRay"><code>dse spark --master local[4] \
          --name "Tim's Shell"</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Artem&#8217;s Shell is going to connect to a remote Master to request for resources.
Note that 7077 is the default port that can be changed if needed.</p></div>
<div class="paragraph"><p>Tim&#8217;s Shell is going to connect to Spark on Tim&#8217;s laptop.</p></div>
</div>
</div>
</section>
<section class="slide" id="examples-of-entering-expressions">
<h2>Examples of Entering Expressions</h2>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>scala&gt; println("Hello Spark!")
Hello Spark!

scala&gt; val four = 2 + 2
four: Int = 4

scala&gt; val numbers = sc.parallelize(List(1,2,3,4,5,6,7,8,9))
numbers: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:55

scala&gt; val movies = sc.cassandraTable("killr_video","movies")
movies: com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow] ...</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The first two expressions are pure Scala. The last two use <em>sc</em> and Spark Scala API.</p></div>
</div>
</div>
</section>
<section class="slide" id="auto-completion-and-copy-pasting">
<h2>Auto-Completion and Copy-Pasting</h2>
<div class="listingblock">
<div class="title"><em>Auto-Completion</em></div>
<div class="content">
<pre class="CodeRay"><code>scala&gt; sc.&lt;Tab&gt;
accumulable                accumulableCollection      accumulator                ...
appName                    applicationId              asInstanceOf               ...
...                        ...                        ...                        ...</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Copy-Pasting Multi-Line Expressions</em></div>
<div class="content">
<pre class="CodeRay"><code>scala&gt; :paste
// Entering paste mode (ctrl-D to finish)

sc.parallelize(List(1,2,3,4,5,6,7,8,9))
  .reduce(_+_)

// Exiting paste mode, now interpreting.

res11: Int = 45</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>&lt;Tab&gt; is used for auto-complete. If you write code in an editor to copy-paste later, do not use tabulation!</p></div>
<div class="paragraph"><p>The <em>:paste</em> mode must be used with multi-line expressions. Otherwise, Spark Shell will interpret each line as
a separate expression.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-tools-spark-web-ui">
<h2>Spark Web UI</h2>
<div class="paragraph"><p><strong>Monitoring your cluster and applications</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:14%" />
<col style="width:28%" />
<col style="width:57%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">UI</th>
<th class="tableblock halign-left valign-top">URL</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Master Web UI</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a class="bare" href="http://&lt;master-host&gt;:7080">http://&lt;master-host&gt;:7080</a></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Displays information about Master, Workers, and running and completed applications.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Application Web UI</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a class="bare" href="http://&lt;driver-host&gt;:4040">http://&lt;driver-host&gt;:4040</a></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Displays information about running and completed jobs, stages, tasks, as well as
information about persisted datasets, environment settings, and Executors that are controlled by an application.
If more than one application is running, the Web UI ports are assigned as <code>4040</code>, <code>4041</code>, and so forth.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The Spark Web UI is extremely useful when designing, debugging, troubleshooting, optimizing, executing, and monitoring
your cluster and applications.</p></div>
</div>
</div>
</section>
<section class="slide" id="master-web-ui">
<h2>Master Web UI</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="master ui" src="images/spark/tools/spark-web-ui/master-ui.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Notice the Master URL.</p></div>
<div class="paragraph"><p>Here we have a 3-node cluster and one running application that takes a half of the available resources.</p></div>
</div>
</div>
</section>
<section class="slide" id="application-web-ui">
<h2>Application Web UI</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="application ui" src="images/spark/tools/spark-web-ui/application-ui.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Only one tab is shown. Hopefully you are already familiar with Stages and Tasks.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-essentials-hello-world">
<h2>WordCount is the New "Hello, World!"</h2>
<div class="imageblock center">
<div class="content">
<img alt="tag cloud" src="images/spark/spark-essentials/hello-world/tag-cloud.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To demonstrate the basic syntax of Spark and its Scala API, lets consider the WordCount problem,
which became the new "Hello, World!" in distributed computing. We will tackle this problem in
the context of the KillrVideo dataset, whose tag cloud is shown in the slide. To visualize
different video genres and their frequencies, we must count how many times each genre is assigned
to any given video. Lets begin!</p></div>
</div>
</div>
</section>
<section class="slide" id="the-wordcount-problem">
<h2>The WordCount Problem</h2>
<div class="paragraph"><p><strong>General Steps</strong></p></div>
<div class="olist arabic">
<ol class="arabic">
<li>
Load records from a data source
</li>
<li>
Parse records and generate words
</li>
<li>
Count how many times each word appears in the dataset
</li>
<li>
Output the result
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph"><p>Spark will automatically parallelize computation</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The algorithm for the WordCount problem includes four simple steps. First, we
need to load data into Spark from an external data source, such as a file or a database.
Second, we need to parse records into words that can be counted.
Next, we need to count generated words.
Finally, we push the result into an external system or output it to the screen.</p></div>
<div class="paragraph"><p>We will implement each of these steps in the following slides.
Spark will parallelize computation for us automatically.</p></div>
</div>
</div>
</section>
<section class="slide" id="step-1-loading-records">
<h2>Step 1: Loading Records</h2>
<div class="paragraph"><p><strong>Reading video records from a local CSV file</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val records = sc.textFile("file:///home/videos.csv")</code></pre>
</div>
</div>
<div class="imageblock center">
<div class="content">
<img alt="step1" src="images/spark/spark-essentials/hello-world/step1.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Step 1 loads video records from a CSV file in the local file system.
The <em>textFile</em> method is called on the <em>sc</em> object that is a predefined <em>SparkContext</em> available in <em>Spark shell</em>.
<em>SparkContext</em> serves as an entry point to Spark functionality.
The resulting <em>records</em> object is an RDD or Resilient Distributed Dataset.
For now, lets think of <em>records</em> as a dataset representation in Spark.</p></div>
<div class="paragraph"><p>As you can see from the illustration, each line in the CSV file becomes an element of type <em>String</em> in the RDD.</p></div>
</div>
</div>
</section>
<section class="slide" id="step-2-parsing-records">
<h2>Step 2: Parsing Records</h2>
<div class="paragraph"><p><strong>Splitting video records into words and dropping video identifiers</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val words = records.flatMap(record =&gt; record.split(",").drop(1))</code></pre>
</div>
</div>
<div class="imageblock center">
<div class="content">
<img alt="step2" src="images/spark/spark-essentials/hello-world/step2.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To parse the video records, we next apply transformation <em>flatMap</em> on the <em>records</em> RDD to obtain the <em>words</em> RDD.
<em>flatMap</em> takes an anonymous function as a parameter. The body of this function is applied to each element of the input RDD <em>records</em>.
In particular, each <em>record</em> is split based on the comma delimiter into multiple literals and the first literal representing a video identifier is dropped.
All the remaining literals representing video genres become elements in the <em>words</em> RDD.</p></div>
<div class="paragraph"><p>You may notice that <em>flatMap</em> is a one-to-many mapping, such that one element from the input RDD can result in many elements in the output RDD.
<em>flatMap</em> is only one of many transformations defined for Spark RDDs. We will see examples of two other transformations in the next slide.</p></div>
</div>
</div>
</section>
<section class="slide" id="step-3-counting-words">
<h2>Step 3: Counting Words</h2>
<div class="paragraph"><p><strong>Counting video genres</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val counts = words.map(word =&gt; (word,1)).reduceByKey{case (x,y) =&gt; x + y}</code></pre>
</div>
</div>
<div class="imageblock center">
<div class="content">
<img alt="step3" src="images/spark/spark-essentials/hello-world/step3.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Counting is implemented using two transformations.
First, we <em>map</em> each word or genre to a pair consisting of a key and a value.
In our example, a key is represented by a genre from the <em>words</em> RDD and value is always hard-coded as <em>1</em>.
The intermediate RDD resulting from the <em>map</em> transformation contains key-value pairs and therefore, frequently called a Key-Value Pair RDD.
The second transformation, <em>reduceByKey</em>, is applied on the intermediate RDD to aggregate values of pairs with the same key.
In this example, the aggregation function is simply defined as addition of two numeric values.</p></div>
<div class="paragraph"><p>The illustration shows the effects of these two transformations.</p></div>
<div class="paragraph"><p>Note that, unlike <em>flatMap</em>, it is easy to see from the illustration that <em>map</em> allows for a one-to-one mapping.</p></div>
</div>
</div>
</section>
<section class="slide" id="step-4-outputing-the-result">
<h2>Step 4: Outputing The Result</h2>
<div class="paragraph"><p><strong>Collecting and printing the result</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code class="sql language-sql">counts.collect().foreach(println)</code></pre>
</div>
</div>
<div class="imageblock center">
<div class="content">
<img alt="step4" src="images/spark/spark-essentials/hello-world/step4.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In this last step, we are collecting and printing the result of our computation.
We apply <em>collect</em> on the <em>counts</em> RDD to transfer all data from the distributed dataset represented by this RDD to a Scala Array on the client machine.
Unlike transformations that take an RDD as input and return an RDD as output, <em>collect</em> is an action; it triggers computation of the final result and makes it available to the client program.</p></div>
<div class="paragraph"><p>The output array is then processed using regular Scala Array API to print each element to the screen.</p></div>
</div>
</div>
</section>
<section class="slide" id="final-solution">
<h2>Final Solution</h2>
<div class="paragraph"><p><strong>Taking advantage of the Scala fluent interface</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>sc.textFile("file:///home/videos.csv")
  .flatMap(line =&gt; line.split(",").drop(1))
  .map(word =&gt; (word,1))
  .reduceByKey{case (x,y) =&gt; x + y}
  .collect()
  .foreach(println)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Finally, we can take advantage of a couple of Scala syntactic features to make our code more compact and sometimes more readable.</p></div>
<div class="paragraph"><p>The fluent interface is one such syntactic optimization.
Here we have the same four steps of the WordCount problem implemented as one statement with method chaining.
The result of this computation will be the same as before.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Taking advantage of the Scala unnamed parameters</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>sc.textFile("file:///home/videos.csv")
  .flatMap(_.split(",").drop(1))
  .map((_,1))
  .reduceByKey(_+_)
  .collect
  .foreach(println)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In addition, if you are familiar with unnamed parameters in Scala represented by <em>underscores</em>,
our code can be further simplified as shown in this slide.
Again, the result of this computation will be the same as before.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-essentials-rdd">
<h2>What is an RDD?</h2>
<div class="paragraph"><p><strong>Programming abstraction of a dataset for Spark in-memory computation</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Property</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Dataset</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Dataset contains primitive values, records, tuples, class objects</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Distributed</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Data may reside on different nodes in a cluster</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Resilient</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Data is recomputed based on lineage in case of a failure</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Immutable</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Dataset is transformed into a new dataset rather than mutated</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">In-memory</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Data is kept in memory as much as possible</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Resilient Distributed Dataset or RDD is a programming abstraction of a dataset
for Spark in-memory computation. RDD has a number of properties that
we will discuss in more detail in the following slides.</p></div>
</div>
</div>
</section>
<section class="slide" id="rdd-as-a-dataset">
<h2>RDD as a Dataset</h2>
<div class="ulist">
<ul>
<li>Collection of data objects</li>
<li>Object types can affect operations</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="dataset" src="images/spark/spark-essentials/rdd/dataset.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>First, think of RDD as a dataset or a collection of data objects of a known type.
Types of objects can affect operations that are applicable to a particular RDD.
For example, the RDD in the illustration holds key-value pairs,
where keys correspond to people names
and values correspond to people ages, such as Alice is 21 y.o.
Key-Value Pair RDDs constitute a special class of RDDs with many useful and unique operations.</p></div>
<div class="paragraph"><p>It is important to understand that until computation is triggered by an action,
an RDD does not hold any data; it is instead just a recipe of how data objects
can be computed.</p></div>
</div>
</div>
</section>
<section class="slide" id="rdd-as-a-distributed-dataset">
<h2>RDD as a Distributed Dataset</h2>
<div class="ulist">
<ul>
<li>RDD is divided into <em>partitions</em></li>
<li>Partitions are distributed across nodes in a cluster</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="partitions" src="images/spark/spark-essentials/rdd/partitions.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Second, Spark automatically partitions an RDD into smaller collections called <em>partitions</em>
and distributes them among <em>Executors</em> on different nodes in a cluster.</p></div>
<div class="paragraph"><p>In our example, we have four partitions distributed across the three nodes.</p></div>
<div class="paragraph"><p>How partitioning is performed may depend on many factors,
including a data source, such as Cassandra or HDFS,
number of cores available to an application, and types of operations applied to an RDD.
An application will frequently have to control partitioning
to achieve optimal performance.</p></div>
</div>
</div>
</section>
<section class="slide" id="rdd-as-a-resilient-distributed-dataset">
<h2>RDD as a Resilient Distributed Dataset</h2>
<div class="ulist">
<ul>
<li>Spark remembers lineage of all data it computes to achieve fault-tolerance</li>
<li>Spark automatically recomputes partitions that were lost due to a failure</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="failure" src="images/spark/spark-essentials/rdd/failure.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Third, RDD is resilient or fault-tolerant because Spark remembers lineage or pedigree
of all data it computes. In case of a node or process failure, lost partitions
are recomputed automatically.</p></div>
<div class="paragraph"><p>This process is illustrated in our example, where the node with two partitions failed
and Spark recomputed those partitions on the two other nodes.
Reliability of an external data source is important for result reproducibility.
Spark should be able to retrieve data again if need be!</p></div>
</div>
</div>
</section>
<section class="slide" id="rdd-is-immutable">
<h2>RDD is Immutable</h2>
<div class="ulist">
<ul>
<li>RDD is read-only</li>
<li>RDD can be transformed</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="immutable" src="images/spark/spark-essentials/rdd/immutable.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Next, the RDD property that is frequently overlooked is immutability.</p></div>
<div class="paragraph"><p>It is helpful to think about RDD data as read-only. To change data, we must transform
an RDD into a new RDD.</p></div>
<div class="paragraph"><p>In this example, we are applying the <em>filer</em> transformation on the input RDD to
produce the output RDD with key-value pairs where value (person&#8217;s age) is greater
or equal to 21.</p></div>
</div>
</div>
</section>
<section class="slide" id="rdd-is-for-in-memory-computation">
<h2>RDD is for In-Memory Computation</h2>
<div class="ulist">
<ul>
<li>RDD partitions are processed in memory</li>
<li>RDD (as a whole) does not have to fit into memory</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="in memory" src="images/spark/spark-essentials/rdd/in-memory.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Finally, Spark is about parallel, in-memory computation.
Partitions do have to be in memory to be processed, however an RDD as a whole
does not need to be in memory at one given moment.</p></div>
<div class="paragraph"><p>Conceptually, imagine that computation is organized into multiple pipelines
with a known throughput, such that each pipeline can handle some number of partitions
at a time. The data that is waiting for its turn to get into a pipeline is simply sitting
at the data source. Once a pipeline is cleared and its output is to an external system,
it is capable to serve more partitions.
The more cluster resources are allocated to your application,
the more pipelines and parallelism you can have.</p></div>
<div class="paragraph"><p>It should be noted that some operations on RDDs,
such as those that involve data shuffling, require disk I/O.
In addition, an application will frequently have to control how an RDD is cached or persisted
to achieve optimal performance.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-essentials-rdd-creating">
<h2>How Do You Create an RDD?</h2>
<div class="paragraph"><p><strong>A few prerequisites</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Prerequisite</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>SparkContext</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A <em>SparkContext</em> represents a connection to a Spark cluster.
In <em>Spark shell</em>, object <em>sc</em> is created automatically.
In a standalone application, a <em>SparkContext</em> must be constructed.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>Spark-Cassandra Connector</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A library that allows moving data between Spark and Cassandra.
<em>DataStax Enterprise</em> comes with Spark and Cassandra integrated.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>Data source</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Data is initially loaded into Spark from an external source.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Before we can create a Spark RDD, we need to discuss some prerequisites.</p></div>
<div class="paragraph"><p>First, we need to have a <em>SparkContext</em> object that represents a connection
to a Spark cluster and serves as an entry point to Spark functionality.
In this presentation, we assume that such object is predefined and denoted as <em>sc</em>,
which is exactly the case how it is defined in <em>Spark shell</em>.</p></div>
<div class="paragraph"><p>Second, if we want to interact with Cassandra, we need a special library called
<em>Spark-Cassandra Connector</em>. This library is already part of DataStax Enterprise (DSE)
and therefore, we can just start using its functions.</p></div>
<div class="paragraph"><p>Finally, remember that Spark is not a storage system or a database like Cassandra;
any data has to be loaded
from an external system. Depending on an initial data source for our RDDs,
we will discuss different approaches to creating an RDD.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Three main approaches</strong></p></div>
<div class="olist arabic">
<ol class="arabic">
<li>
Parallelize an <em>existing collection</em>
</li>
<li>
Load data from a <em>stable storage</em>
</li>
<li>
Transform an <em>existing RDD</em>
</li>
</ol>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In particular, we will discuss how to parallelize an existing Scala collection,
such as a list, array, or sequence; how to load data from a stable storage, such as a file or
the Cassandra database; and how to create an RDD from an existing RDD using transformations,
such as <em>filter</em> or <em>map</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="creating-an-rdd-from-an-existing-collection">
<h2>Creating an RDD from an Existing Collection</h2>
<div class="listingblock">
<div class="title"><em>Parallelizing a Scala List</em></div>
<div class="content">
<pre class="CodeRay"><code>val lst = List( ("Frozen", 2013), ("Toy Story", 1995), ("WALL-E", 2008) )
// lst: List[(String, Int)]

val rdd = sc.parallelize(lst)
// rdd: org.apache.spark.rdd.RDD[(String, Int)]</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>This approach is used with small datasets for development, testing,
and educational purposes.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To create an RDD from a Scala collection, we first define a local collection,
such as the list of movies in this example. Each movie is represented
by a Scala tuple with a movie title and a movie year. Notice the type
of value <em>lst</em> is; it is <em>List[(String, Int)]</em>.</p></div>
<div class="paragraph"><p>Then, we simply pass this list to method <em>parallelize</em>, which returns a Spark RDD
as you can see in the example. Again, notice the type of value <em>rdd</em>; it is now
<em>RDD[(String, Int)]</em>.</p></div>
<div class="paragraph"><p>This is how simple to create an RDD from a collection you can define in your client
program. <em>parallelize</em> is quite useful for quick development and testing things out,
and is perfect for educational examples. You are unlikely to use it in production however,
as large datasets are most likely to be loaded from Cassandra or a distributed file system.</p></div>
</div>
</div>
</section>
<section class="slide" id="creating-an-rdd-from-a-file">
<h2>Creating an RDD from a File</h2>
<div class="listingblock">
<div class="title"><em>Local file system</em></div>
<div class="content">
<pre class="CodeRay"><code>val rdd = sc.textFile("file:///home/videos.csv")
// rdd: org.apache.spark.rdd.RDD[String]</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Distributed file system</em></div>
<div class="content">
<pre class="CodeRay"><code>val rdd = sc.textFile("cfs:///home/videos.csv")
// rdd: org.apache.spark.rdd.RDD[String]</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p><em>SparkContext</em> supports <em>hadoopFile</em>, <em>newAPIHadoopFile</em>, <em>sequenceFile</em>, and <em>objectFile</em>
for other types of files.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Creating an RDD from a text file is achieved by calling the <em>textFile</em> method on the <em>SparkContext</em> object.</p></div>
<div class="paragraph"><p>In the first example, we are supplying the file location in a local file system.
This file must exist on every worker node in a cluster.</p></div>
<div class="paragraph"><p>In the second example, we are passing the file location in Cassandra File System, which
is HDFS-compatible file system.</p></div>
<div class="paragraph"><p>In both cases, the resulting RDD contains elements of type <em>String</em>, because each element
corresponds to exactly one line in the file.</p></div>
<div class="paragraph"><p>There are a few additional methods (listed in the slide) that allow creating RDDs from files of other formats.</p></div>
</div>
</div>
</section>
<section class="slide" id="creating-an-rdd-from-a-cassandra-table">
<h2>Creating an RDD from a Cassandra Table</h2>
<div class="listingblock">
<div class="title"><em>Reading from a Cassandra table</em></div>
<div class="content">
<pre class="CodeRay"><code>val rdd = sc.cassandraTable("killr_video", "videos")
// rdd: com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow]</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p><em>Spark-Cassandra Connector</em> supports many other functions to customize and control
data retrieval from a Cassandra table, including <em>select</em>, <em>where</em>, <em>limit</em>,
<em>withAscOrder</em>, and <em>withDescOrder</em>.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>It is also very straightforward to create an RDD from a Cassandra table by calling
method <em>cassandraTable</em> supplied by <em>Spark-Cassandra Connector</em>.</p></div>
<div class="paragraph"><p>In this example, we are loading all data from table <em>videos</em> that is defined in keyspace
<em>killr_video</em>. You should notice that the returned RDD type is coming from the connector
package and is defined as an RDD of Cassandra rows with the structure that matches the underlying
Cassandra table rows.</p></div>
<div class="paragraph"><p>The connector supports additional methods to select specific table columns, retrieve
only rows satisfying a certain predicate, or limit a number of rows in a resulting RDD.
While we will not discuss these methods here, it is worth mentioning that, internally,
data for a Cassandra RDD will be retrieved with an efficient CQL query.</p></div>
</div>
</div>
</section>
<section class="slide" id="creating-an-rdd-from-an-existing-rdd">
<h2>Creating an RDD from an Existing RDD</h2>
<div class="listingblock">
<div class="title"><em>Transforming an RDD into another RDD</em></div>
<div class="content">
<pre class="CodeRay"><code>val rdd1 = sc.parallelize(List( ("Frozen", 2013), ("Toy Story", 1995), ("WALL-E", 2008) ) )
// rdd1: org.apache.spark.rdd.RDD[(String, Int)]

val rdd2 = rdd1.filter{ case (title,year) =&gt; year &gt; 2010 }
// rdd2: org.apache.spark.rdd.RDD[(String, Int)]</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>The Spark RDD API supports many useful transformations, including <em>filter</em>, <em>map</em>, <em>flatMap</em>, <em>union</em>, and <em>intersection</em>.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The last but not least, an RDD is frequently created from another RDD using a transformation.
Transformations are used frequently because they actually do some useful processing of data.</p></div>
<div class="paragraph"><p>Here, given <em>rdd1</em> created by parallelizing a collection, we are applying the
<em>filter</em> transformation to obtain <em>rdd2</em>. The <em>filter</em>'s parameter is the anonymous function
that is applied on every element of <em>rdd1</em>; the result of this function is either <em>true</em> (the
element passes the filtering condition and will appear in <em>rdd2</em>) or <em>false</em> (the element is eliminated).</p></div>
<div class="paragraph"><p>As you may have already computed, <em>rdd2</em> will only contain information for movie "Frozen".</p></div>
</div>
</div>
</section>
<section class="slide" id="there-is-more-to-it-than-that">
<h2>There is More to It than That &#8230;&#8203;</h2>
<div class="paragraph"><p><strong>Two related topics</strong></p></div>
<div class="ulist">
<ul>
<li>Lazy evaluation</li>
<li>Partitioning</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Finally, lets briefly introduce two internal topics that we will talk about elsewhere in a greater detail.</p></div>
<div class="paragraph"><p>All the discussed methods, such as <em>parallelize</em>, <em>textFile</em>, <em>cassandraTable</em>, and <em>filter</em>,
are evaluated lazily by Spark. That means that, in all our examples, Spark only records
metadata about
how an RDD can be created but does not access actual data until later time. Such
time has to be defined by an action, which belongs to a special class of operations.</p></div>
<div class="paragraph"><p>It is also worth mentioning that, for presented approaches, a number of resulting RDD partitions
may be different depending on a data source used. One can also control a number of partitions by
passing a second, optional parameter to <em>parallelize</em>, <em>textFile</em>, and some of the RDD transformations
(but not <em>filter</em>). Partitioning of a Cassandra RDD will depend on a data size, as well as Cassandra partitioning to
benefit from data locality. Partitioning is very important for computation parallelism.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-essentials-rdd-transformations">
<h2>What are RDD Transformations?</h2>
<div class="paragraph"><p><strong>Two types of RDD operations</strong></p></div>
<div class="ulist">
<ul>
<li><p>
<em>Transformations</em><div class="ulist">
<ul>
<li>Transform an RDD to a new RDD</li>
<li>Lazy evaluation</li>
</ul>
</div></p></li>
<li><p>
<em>Actions</em><div class="ulist">
<ul>
<li>Perform computation on an RDD and output results
to a client or store results to a stable storage</li>
<li>Trigger computation</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark defines two types of operations on RDDs - transformations and actions.</p></div>
<div class="paragraph"><p>A transformation always takes an RDD as input and returns an RDD as output.
In other words, a transformation creates a new RDD from an existing RDD.
All transformations in Spark are evaluated lazily, such that they do not
compute their results immediately but rather record metadata about how to compute
their results.</p></div>
<div class="paragraph"><p>On the other hand, an action triggers computation: it takes an RDD and computes the final result,
which is either transferred to the client driver application or stored into
an external system like Cassandra.</p></div>
<div class="paragraph"><p>This presentation focuses on common transformations.</p></div>
</div>
</div>
</section>
<section class="slide" id="common-transformations">
<h2>Common Transformations</h2>
<div class="paragraph"><p><strong>Unary transformations</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>filter</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD is formed by selecting those elements of the source RDD
on which a function<em>f</em>returns <em>true</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>map</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD is formed by applying a function <em>f</em> on each element of
the source RDD. There is a <em>one-to-one</em> correspondence between input and output elements.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>flatMap</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD is formed by applying a function <em>f</em> on each element of
the source RDD. There is a <em>one-to-many</em> correspondence between input and output elements
if <em>f</em> returns a <em>Seq</em> with more than one element.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>distinct</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD is formed by distinct elements of the source RDD.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>,[<em>seed</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD is formed by a <em>fraction</em> of elements of the source RDD
using sampling with or without replacement. A random number generator <em>seed</em> is optional.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here are a few common unary transformations, which can be applied on a single, source RDD.</p></div>
<div class="paragraph"><p><em>filter</em> takes a function <em>f</em> as a parameter and returns a new RDD that is formed
by selecting those elements of the source RDD on which <em>f</em> returns <em>true</em>.</p></div>
<div class="paragraph"><p>In the case of both <em>map</em> and <em>flatMap</em>, a function <em>f</em> is applied on every element
of the source
RDD and the function return value becomes an element in a new RDD. The key difference
between these two transformations is that <em>flatMap</em> further flattens or unnests any collection
returned by function <em>f</em>, such that every element in the collection becomes
a separate element in the new RDD.</p></div>
<div class="paragraph"><p><em>distinct</em> returnes an RDD that contains all distinct elements of the source RDD.</p></div>
<div class="paragraph"><p><em>sample</em> allows randomly selecting only a fraction of elements of the source RDD,
which is useful for large dataset exploration.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Binary transformations</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>union</strong>(<em>otherRDD</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD contains the union of elements from the source RDD and <em>otherRDD</em>.
Duplicates are allowed. Input RDDs must be <em>union-compatible</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>intersection</strong>(<em>otherRDD</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD contains the intersection of elements from the source RDD and <em>otherRDD</em>.
Duplicates are eliminated. Input RDDs must be <em>union-compatible</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>subtract</strong>(<em>otherRDD</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD contains those elements from the source RDD that are not in <em>otherRDD</em>.
Duplicates are allowed. Input RDDs must be <em>union-compatible</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>cartesian</strong>(<em>otherRDD</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD contains all possible pairs of elements from the source RDD and <em>otherRDD</em>.
RDD[<em>T</em>] <em>x</em> RDD[<em>U</em>] &#8594; RDD[(<em>T</em>,<em>U</em>)].
Duplicates are allowed.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>These four transformations are binary, so they take the source RDD and the other RDD
as input and return a new RDD.</p></div>
<div class="paragraph"><p><em>union</em>, <em>intersection</em>, <em>subtract</em>, and <em>cartesian</em> work just like the corresponding
mathematical operations of sets.</p></div>
<div class="paragraph"><p>This is, of course, an incomplete list of transformations available in Spark.
We will see many more transformations in other presentations, especially in the context of
Key-Value Pair RDDs.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformation-examples">
<h2>Transformation Examples</h2>
<div class="paragraph"><p><strong>Demonstrating <em>filter</em>, <em>map</em>, <em>flatMap</em>, <em>distinct</em>, and <em>cartesian</em></strong></p></div>
<div class="listingblock">
<div class="title"><em>Starting with an RDD of movies</em></div>
<div class="content">
<pre class="CodeRay"><code>val movies = sc.parallelize( Array("Frozen, 2013", "Toy Story, 1995", "WALL-E, 2008", "Despicable Me, 2010", "Shrek, 2001", "The Lego Movie, 2014", "Alice in Wonderland, 2010") )</code></pre>
</div>
</div>
<div class="imageblock center">
<div class="content">
<img alt="input" src="images/spark/spark-essentials/rdd-transformations/input.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>We are going to demonstrate some of the discussed transformations in the following examples.
Let us start by creating the <em>movies</em> RDD with 7 elements, where each element is
a string literal containing a movie title and a movie release year.</p></div>
</div>
</div>
</section>
<section class="slide" id="em-filter-em">
<h2><em>filter</em></h2>
<div class="listingblock">
<div class="title"><em>Find movies from 2010</em></div>
<div class="content">
<pre class="CodeRay"><code>val movies2010 = movies.filter(m =&gt; m.substring(m.length-4,m.length).toInt == 2010)

// alternatively, the same result can be computed with this statement
val movies2010 = movies.filter(m =&gt; m.split(",").last.trim.toInt == 2010)</code></pre>
</div>
</div>
<div class="imageblock center">
<div class="content">
<img alt="filter" src="images/spark/spark-essentials/rdd-transformations/filter.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Our first challenge is to find movies from 2010.</p></div>
<div class="paragraph"><p>Here we are using the <em>filter</em> transformation to select only those movies whose
release years are equal to 2010. A minor difficulty is that we first need to extract
a year from a string literal. There are a couple of alternatives shown. First,
we can use method <em>substring</em> to extract the last 4 characters in a literal and convert
the result to an integer. Second, we can use method <em>split</em> to decompose a literal based on
the comma delimiter, take the last component, trim leading spaces, and convert the result
to <em>Int</em>. In both cases, the result is exactly the same - two movies are selected for
the new RDD.</p></div>
</div>
</div>
</section>
<section class="slide" id="em-map-em">
<h2><em>map</em></h2>
<div class="listingblock">
<div class="title"><em>Add a set of genres to each movie</em></div>
<div class="content">
<pre class="CodeRay"><code>val familyMovies = movies2010.map(m =&gt; (m, Set("Family","Animation")))</code></pre>
</div>
</div>
<div class="imageblock center">
<div class="content">
<img alt="map" src="images/spark/spark-essentials/rdd-transformations/map.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Next, we are going to add a predefined set of genres ("Family" and "Animation")
to each movie from 2010.</p></div>
<div class="paragraph"><p><em>map</em> takes each input RDD element of type <em>String</em> and returns a tuple with
two components of type <em>String</em> and <em>Set</em> for the new <em>familyMovies</em> RDD.
This is a one-to-one mapping.</p></div>
</div>
</div>
</section>
<section class="slide" id="em-flatmap-em-and-em-distinct-em">
<h2><em>flatMap</em> and <em>distinct</em></h2>
<div class="listingblock">
<div class="title"><em>Extract distinct genres</em></div>
<div class="content">
<pre class="CodeRay"><code>val familyGenres = familyMovies.flatMap{case (m,g) =&gt; g }
                               .distinct</code></pre>
</div>
</div>
<div class="imageblock center">
<div class="content">
<img alt="flatmap" src="images/spark/spark-essentials/rdd-transformations/flatmap.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This time, we want to extract distinct genres from the previous RDD.</p></div>
<div class="paragraph"><p>Because the genres are stored in the <em>Set</em> collection, we are using <em>flatMap</em>
to get down to individual elements in a set and map them to elements of the
intermediate RDD. This is a one-to-many mapping. We are then eliminating duplicates with the <em>distinct</em> transformation
that gives us RDD <em>familyGenres</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="em-cartesian-em-and-em-filter-em">
<h2><em>cartesian</em> and <em>filter</em></h2>
<div class="listingblock">
<div class="title"><em>Compute all possible pairs of non-repeating genres</em></div>
<div class="content">
<pre class="CodeRay"><code>val pairs = familyGenres.cartesian(familyGenres)
                        .filter{case (g1,g2) =&gt; g1 != g2}</code></pre>
</div>
</div>
<div class="imageblock center">
<div class="content">
<img alt="cartesian" src="images/spark/spark-essentials/rdd-transformations/cartesian.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In our final example, we are computing all possible pairs of
non-repeating genres using transformations <em>cartesian</em> and <em>filter</em>.</p></div>
<div class="paragraph"><p>Computing a Cartesian product can produce a large result, yet it is a
useful transformation in real life applications. For example, to find similar items in a set,
you may have to compare every item with every other item in this set. This is where
<em>cartesian</em> helps you generate all possible item pairs to do the comparison.</p></div>
<div class="paragraph"><p>Our <em>filter</em> transformation eliminates pairs where both genres are the same.</p></div>
<div class="paragraph"><p>Go ahead and try other transformations we discussed!</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-essentials-rdd-actions">
<h2>What are RDD Actions?</h2>
<div class="paragraph"><p><strong>Two types of RDD operations</strong></p></div>
<div class="ulist">
<ul>
<li><p>
<em>Transformations</em><div class="ulist">
<ul>
<li>Transform an RDD to a new RDD</li>
<li>Lazy evaluation</li>
</ul>
</div></p></li>
<li><p>
<em>Actions</em><div class="ulist">
<ul>
<li>Perform computation on an RDD and output results
to a client or store results to a stable storage</li>
<li>Trigger computation</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark defines two types of operations on RDDs - transformations and actions.</p></div>
<div class="paragraph"><p>A transformation always takes an RDD as input and returns an RDD as output.
In other words, a transformation creates a new RDD from an existing RDD.
All transformations in Spark are evaluated lazily, such that they do not
compute their results immediately but rather record metadata about how to compute
their results.</p></div>
<div class="paragraph"><p>On the other hand, an action triggers computation: it takes an RDD and computes the final result,
which is either transferred to the client driver application or stored into
an external system like Cassandra.</p></div>
<div class="paragraph"><p>This presentation focuses on common actions.</p></div>
</div>
</div>
</section>
<section class="slide" id="common-actions">
<h2>Common Actions</h2>
<div class="paragraph"><p><strong>Actions that return results to the driver program</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Action</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>collect</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns an <em>Array</em> with all elements of the source RDD.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>count</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns a total number of elements in the source RDD.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>reduce</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns an aggregate value computed by applying function <em>f</em> to elements of
the source RDD.
Function <em>f</em>must take two arguments and return one value, and should
be commutative and associative for correct parallel computation.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>take</strong>(<em>n</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns an <em>Array</em> with the first<em>n</em>elements of the source RDD.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>first</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns the first element of the source RDD.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here are some frequently used actions that are evaluated in parallel on the source RDD
and whose results are returned to the driver program.</p></div>
<div class="paragraph"><p><em>collect</em> simply returns all elements of the source RDD as an <em>Array</em>.</p></div>
<div class="paragraph"><p><em>count</em> counts how many elements are in the source RDD and returns that value.</p></div>
<div class="paragraph"><p><em>reduce</em> uses a function <em>f</em> to compute a single aggregate value from all elements
of the source RDD.</p></div>
<div class="paragraph"><p><em>take</em> returns an <em>Array</em> with the first<em>n</em>elements of the source RDD.</p></div>
<div class="paragraph"><p><em>first</em> only returns the first element of the source RDD.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Actions with side effects</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Action</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>foreach</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Executes a function<em>f</em>on each element of the source RDD.
The function usually implements a side effect, such as updating an <em>accumulator</em> variable
or interacting with an external system.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>saveAsTextFile</strong>(<em>path</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Saves all elements of the source RDD into a text file.
Spark calls <em>toString</em> on each element to convert it to a line of text in the file.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>saveToCassandra</strong>(<em>keyspace</em>, <em>table</em>, [<em>columns</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Stores all elements of the source RDD into a Cassandra <em>table</em> in a given <em>keyspace</em>.
Table <em>columns</em> may be specified if needed. <em>saveToCassandra</em> is available through
<em>Spark-Cassandra Connector</em>.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Some actions do not return their results to the driver program but are rather used
for side effects.</p></div>
<div class="paragraph"><p><em>foreach</em> takes function <em>f</em> as a parameter and executes it on every element of the
source RDD. The function usually pushes data to an external system, updates
<em>accumulator</em> variables, or performs other side effects.</p></div>
<div class="paragraph"><p><em>saveAsTextFile</em> can be used to save all elements of the source RDD into a text file.</p></div>
<div class="paragraph"><p>And, finally, <em>saveToCassandra</em> allows saving all elements of the source RDD into
a Cassandra table. <em>saveToCassandra</em> is one of the actions supported by
<em>Spark-Cassandra Connector</em>, which we will discuss in another presentation.</p></div>
</div>
</div>
</section>
<section class="slide" id="action-examples">
<h2>Action Examples</h2>
<div class="paragraph"><p><strong>Demonstrating <em>collect</em>, <em>count</em>, <em>reduce</em>, and <em>foreach</em></strong></p></div>
<div class="listingblock">
<div class="title"><em>Starting with an RDD of movies</em></div>
<div class="content">
<pre class="CodeRay"><code>val movies = sc.parallelize( Array("Frozen, 2013", "Toy Story, 1995", "WALL-E, 2008", "Despicable Me, 2010", "Shrek, 2001", "The Lego Movie, 2014", "Alice in Wonderland, 2010") )</code></pre>
</div>
</div>
<div class="imageblock center">
<div class="content">
<img alt="input" src="images/spark/spark-essentials/rdd-actions/input.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>We are going to demonstrate some of the discussed actions in the following examples.
Let us start by creating the <em>movies</em> RDD with 7 elements, where each element is
a string literal containing a movie title and a movie release year.</p></div>
</div>
</div>
</section>
<section class="slide" id="em-collect-em">
<h2><em>collect</em></h2>
<div class="listingblock">
<div class="title"><em>Output movies from 2010</em></div>
<div class="content">
<pre class="CodeRay"><code>movies.filter(m =&gt; m.substring(m.length-4,m.length).toInt == 2010)
      .collect
      .foreach(println)</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Output</em></div>
<div class="content">
<pre class="CodeRay"><code>Despicable Me, 2010
Alice in Wonderland, 2010</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Our first example prints movies from 2010 to a console on the client side.</p></div>
<div class="paragraph"><p>Here we are using the <em>filter</em> transformation to select only those movies whose
release years are equal to 2010. We are then returning the result to the client
as an <em>Array</em> with <em>collect</em>. We finally iterate over the array elements and print each one
of them locally.</p></div>
<div class="paragraph"><p><em>collect</em> should be used with caution when working with large datasets. You
do not want to transfer huge amounts of data to the client application from your Spark cluster.
Instead, you want to apply transformations like <em>filter</em> to decrease your data size
before collecting the result.</p></div>
</div>
</div>
</section>
<section class="slide" id="em-count-em">
<h2><em>count</em></h2>
<div class="listingblock">
<div class="title"><em>Output the total number of movies in the dataset</em></div>
<div class="content">
<pre class="CodeRay"><code>val totalCount = movies.count

println(totalCount)</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Output</em></div>
<div class="content">
<pre class="CodeRay"><code>7</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Next, we are going to count how many movies we have in the original RDD.</p></div>
<div class="paragraph"><p>This is straightforward to do using action <em>count</em>. Counting is done in parallel
by Spark and the final result is returned to the driver program and stored in local
variable <em>totalCount</em>. We output value <em>7</em> with <em>println</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="em-reduce-em">
<h2><em>reduce</em></h2>
<div class="listingblock">
<div class="title"><em>Output a sum of all movie title lengths</em></div>
<div class="content">
<pre class="CodeRay"><code>val totalLength =  movies.map(m =&gt; m.substring(0, m.length - 6).length)
                         .reduce{case (x,y) =&gt; x + y}

println(totalLength)</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Output</em></div>
<div class="content">
<pre class="CodeRay"><code>72</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This time, we want to add up all movie title lengths just for demostration purposes.</p></div>
<div class="paragraph"><p>We are applying the <em>map</em> transformation to extract movie titles and compute a number
of characters in each one. We are then using the <em>reduce</em> action that adds any two
length values together and does it for all RDD elements and intermediate results in parallel.
The final result is returned to the driver program, stored into the local variable, and printed.</p></div>
<div class="paragraph"><p>Instead of using <em>reduce</em>, we could have used action <em>sum</em> to achieve the same result.
Look it up!</p></div>
</div>
</div>
</section>
<section class="slide" id="em-foreach-em">
<h2><em>foreach</em></h2>
<div class="listingblock">
<div class="title"><em>Output an average of all movie title lengths</em></div>
<div class="content">
<pre class="CodeRay"><code>val totalCount  = sc.accumulator(0)
val totalLength = sc.accumulator(0)

movies.map(m =&gt; m.substring(0,m.length - 6).length)
      .foreach{ l =&gt; totalCount += 1; totalLength += l }

println(totalLength.value / totalCount.value)
println(totalLength.value.toDouble / totalCount.value)</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Output</em></div>
<div class="content">
<pre class="CodeRay"><code>10
10.285714285714286</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Our last example of computing an average movie title length is a bit more involved.</p></div>
<div class="paragraph"><p>Notice that, to compute the average, we can use <em>totalCount</em> and <em>totalLength</em>
from the previous two examples. Divide 72 by 7 locally and we are done! However,
that would require us using two actions (<em>count</em> and <em>reduce</em>) and therefore, we will
have to do, so to speak, two passes over our dataset of movies.</p></div>
<div class="paragraph"><p>We can do better than that with accumulators!</p></div>
<div class="paragraph"><p>First, we are declaring two <em>accumulator</em> variables, <em>totalCount</em> and <em>totalLength</em>,
with initial values that are equal to zero. Accumulators are special variables in Spark that can
be added to in parallel but their final aggregate values can only be read locally by
the driver program.</p></div>
<div class="paragraph"><p>Second, we are applying the <em>map</em> transformation to get individual title lengths.</p></div>
<div class="paragraph"><p>Third, we are using the <em>foreach</em> action to update our accumulators. For each
RDD element, we are adding one to <em>totalCount</em> and the element value to <em>totalLength</em>.</p></div>
<div class="paragraph"><p>Last, we are accessing accumulator values locally in the driver program, calculating
the average and printing the result.</p></div>
<div class="paragraph"><p>On a final note, instead of using <em>foreach</em> and <em>accumulator</em> variables,
we could have used action <em>mean</em> for this example. Try it out!</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-cassandra-connector-cassandra-retrieve-data">
<h2>Cassandra as a Data Source for Spark</h2>
<div class="paragraph"><p><strong>Example Cassandra table from the KillrVideo domain</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-cassandra-connector/cassandra-retrieve-data/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>CREATE TABLE movies_by_actor (
  actor TEXT,
  release_year INT,
  movie_id UUID,
  title TEXT,
  genres SET&lt;TEXT&gt;,
  rating FLOAT,
  PRIMARY KEY ((actor), release_year, movie_id)
) WITH CLUSTERING ORDER BY (release_year DESC, movie_id ASC);</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>We present how to use Cassandra as a data source for Spark in the context of our
KillrVideo database. Here is table <em>movies_by_actor</em> with partition key <em>actor</em>
and clustering columns <em>release_year</em> and <em>movie_id</em>. Each partition in this table
may store many rows representing all movies with a particular actor, such as Johnny Depp.
There is a specific clustering order defined for this table.</p></div>
</div>
</div>
</section>
<section class="slide" id="the-challenge">
<h2>The Challenge</h2>
<div class="paragraph"><p><strong>Retrieve five most recent movies featuring Johnny Depp that were released after 2010</strong></p></div>
<div class="listingblock">
<div class="title"><em>CQL solution</em></div>
<div class="content">
<pre class="CodeRay"><code>SELECT title, release_year
FROM movies_by_actor
WHERE actor = 'Johnny Depp' AND release_year &gt; 2010
ORDER BY release_year DESC
LIMIT 5;</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="paragraph"><p><strong>How can you express this logic using <em>Spark-Cassandra Connector</em> API?</strong></p></div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Our challenge is to retrieve information about five most recent movies that were
released after 2010 and had Johnny Depp as an actor.
Moreover, for the result, we are only interested in movie titles
and release years that should be displayed in descending order of release years.</p></div>
<div class="paragraph"><p>Since this is a data retrieval problem, we can readily express the query using
Cassandra Query Language (CQL). However, our goal here is to express the same query using
<em>Spark-Cassandra Connector</em> API, such that data becomes available in Spark as an RDD
that can be used in further data processing or analysis.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-cassandra-connector-api">
<h2>Spark-Cassandra Connector API</h2>
<div class="paragraph"><p><strong>Common methods for data retrieval</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">API Call</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>cassandraTable</strong>(<em>keyspace</em>, <em>table</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns an RDD that contains all rows from a Cassandra <em>table</em> in a specified <em>keyspace</em>.
This method is called on a <em>SparkContext</em> object.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>select</strong>(<em>columns</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Optionally used with <em>cassandraTable</em>() to specify which table <em>columns</em> to retain in the result.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>where</strong>(<em>condition</em>,[<em>parameters</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Optionally used with <em>cassandraTable</em>() to specify a CQL <em>condition</em> to only retrieve
rows that satisfy the <em>condition</em>. The <em>condition</em> may optionally be parameterized.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>withAscOrder</strong> or
<strong>withDescOrder</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Optionally used with <em>cassandraTable</em>() to specify how to order retrieved rows from a single Cassandra partition
based on clustering columns.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>limit</strong>(<em>n</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Optionally used with <em>cassandraTable</em>() to specify how many rows to retrieve.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To achieve our goal, we need to learn about <em>Spark-Cassandra Connector</em> API for data retrieval.</p></div>
<div class="paragraph"><p>The most important method that should be called on a <em>SparkContext</em> object is <em>cassandraTable</em>.
It returns a Cassandra RDD with objects of type <em>CassandraRow</em> that correspond to all rows
from a given Cassandra <em>table</em> in a specified <em>keyspace</em>.</p></div>
<div class="paragraph"><p>The other API calls are optionally used together with <em>cassandraTable</em> to refine the result.</p></div>
<div class="paragraph"><p><em>select</em> allows selecting only desired <em>columns</em> from a table.</p></div>
<div class="paragraph"><p><em>where</em> allows specifying a CQL condition that is used to filter rows.</p></div>
<div class="paragraph"><p><em>withAscOrder</em> or <em>withDescOrder</em> allow ordering rows in a multi-row partition based on
clustering columns.</p></div>
<div class="paragraph"><p>Finally, <em>limit</em> allows specifying a desired number of rows to retrieve.</p></div>
<div class="paragraph"><p>Similarly to RDD transformations, all of these methods are evaluated lazily.</p></div>
</div>
</div>
</section>
<section class="slide" id="mapping-spark-cassandra-connector-api-to-cql">
<h2>Mapping Spark-Cassandra Connector API to CQL</h2>
<div class="paragraph"><p><strong>As simple as it can be &#8230;&#8203;</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>select(columns)                    --&gt;    SELECT columns
cassandraTable(keyspace, table)    --&gt;    FROM keyspace.table
where(condition, [parameters])     --&gt;    WHERE condition
withAscOrder | withDescOrder       --&gt;    ORDER BY clustering_columns ASC | DESC
limit(n)                           --&gt;    LIMIT n</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>It is helpful to see which API calls correspond to which CQL query clauses.
If you are familiar with CQL, <em>Spark-Cassandra Connector</em> API should look familiar, too.</p></div>
<div class="paragraph"><p>Indeed, internally, <em>Spark-Cassandra Connector</em> translates API calls into CQL queries.
The exact translation is, of course, more complex than what is shown in this slide.
The additional complexity comes from the fact that results have to be returned as an RDD,
which is a distributed dataset containing Spark partitions. Intelligent mapping of data
from Cassandra partitions to Spark partitions requires some additional logic.</p></div>
</div>
</div>
</section>
<section class="slide" id="our-challenge-solution">
<h2>Our Challenge Solution</h2>
<div class="paragraph"><p><strong>Retrieve 5 most recent movies featuring Johnny Depp that were released after 2010</strong></p></div>
<div class="listingblock">
<div class="title"><em>Solution</em></div>
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable("killr_video","movies_by_actor")
  .select("title","release_year")
  .where("actor = 'Johnny Depp' AND release_year &gt; 2010")
  .withDescOrder
  .limit(5)
  .collect
  .foreach(println)</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Results</em></div>
<div class="content">
<pre class="CodeRay"><code>CassandraRow{title: Pirates of the Caribbean: Dead Men Tell No Tales, release_year: 2017}
CassandraRow{title: Alice Through the Looking Glass, release_year: 2016}
CassandraRow{title: Black Mass, release_year: 2015}
CassandraRow{title: Yoga Hosers, release_year: 2015}
CassandraRow{title: Mortdecai, release_year: 2015}</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>I am sure you know how to solve our challenge now!</p></div>
<div class="paragraph"><p>Here we are using <em>Spark-Cassandra Connector</em> API to retrieve rows from table <em>movies_by_actor</em>.
We are selecting columns <em>title</em> and <em>release_year</em>,
filtering based on columns <em>actor</em> and <em>release_year</em>,
using descending order, and limiting the result to 5 rows.
We are then using Spark action <em>collect</em> on the resulting RDD and printing the resulting rows.</p></div>
<div class="paragraph"><p>This solution is very efficient. It pushes all data retrieval logic to Cassandra, which
only hands five rows to Spark.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Correct but NOT efficient solution</strong></p></div>
<div class="listingblock">
<div class="title"><em>How not to retrieve data from Cassandra</em></div>
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable("killr_video","movies_by_actor")
  .filter(row =&gt; row.getString("actor") == "Johnny Depp")
  .map(row =&gt; (row.getInt("release_year"), row.getString("title")))
  .sortByKey(false)
  .take(5)
  .foreach(println)</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph"><p>This is not an efficient way to retrieve data from Cassandra!</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>For educational purposes only, we are also showing an alternative solution to our
challenge that is much less efficient than the first one. This is how NOT to retrieve
data from Cassandra!</p></div>
<div class="paragraph"><p>Here we are only using method <em>cassandraTable</em> for data retrieval. All rows from table
<em>movies_by_actors</em> are transferred to Spark and post-processed with transformations <em>filter</em>,
<em>map</em>, <em>sortByKey</em>, and action <em>take</em>.</p></div>
<div class="paragraph"><p>Indeed, we are getting the same five rows in the result but at what cost? Instead of
pushing data retrieval logic to Cassandra, this solution hands a large dataset
to Spark and relies on transformations and actions to get the desired five rows.</p></div>
<div class="paragraph"><p>Remember that, for efficiency, you should always push as much data retrieval logic to Cassandra as possible.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-cassandra-connector-cassandra-process-data">
<h2>Retrieving Data from a Cassandra Table</h2>
<div class="paragraph"><p><strong>Example Cassandra table from the KillrVideo domain</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-cassandra-connector/cassandra-process-data/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val movies = sc.cassandraTable("killr_video","movies_by_actor")
               .where("actor = 'Johnny Depp'")

// movies: com.datastax.spark.connector.rdd.CassandraTableScanRDD[
//                      com.datastax.spark.connector.CassandraRow]</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>We present how to access data retrieved from a Cassandra table in the context of our
KillrVideo database. Here is table <em>movies_by_actor</em> with partition key <em>actor</em>
and clustering columns <em>release_year</em> and <em>movie_id</em>.
To begin with, we create Cassandra RDD <em>movies</em> by retrieving data from table <em>movies_by_actor</em>.
In particular, we retrieve all rows from a single partition with partition key "Johnny Depp".
Notice that our resulting RDD contains objects of type <em>CassandraRow</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="the-challenge-2">
<h2>The Challenge</h2>
<div class="paragraph"><p><strong>Processing CassandraRow objects in a Cassandra RDD</strong></p></div>
<div class="ulist">
<ul>
<li><strong>Problem 1</strong>: Output all movies with word "pirate" in their titles</li>
<li><strong>Problem 2</strong>: Output all movies with genre "Adventure" and a rating of 7.5 or higher</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph"><p>The output should be formatted as <em>Movie Title</em> (<em>year</em>) [<em>rating</em>].</p></div>
<div class="paragraph"><p>For example: Alice in Wonderland (2010) [6.5]</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The challenge that we need to solve in this presentation has two problems.</p></div>
<div class="paragraph"><p>First, let us find all movies with word "pirate" in their titles.
Second, let us find movies with genre "Adventure" and a rating of 7.5 or higher.
We also have specific requirements for the format to display the results.</p></div>
<div class="paragraph"><p>To address this challenge, we need to know how to access specific column values, such as title,
genre, rating, and release year. These values are encapsulated in objects of type <em>CassandraRow</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="data-type-conversions">
<h2>Data Type Conversions</h2>
<div class="paragraph"><p><strong>You must know a type before you can read a value</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:50%" />
<col style="width:50%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Cassandra Type</th>
<th class="tableblock halign-left valign-top">Scala Type</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>ascii</em>, <em>text</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>String</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>bigint</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>Long</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>blob</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>ByteBuffer</em>, <em>Array[Byte]</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>boolean</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>Boolean</em>, <em>Int</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>counter</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>Long</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>decimal</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>BigDecimal</em>, <em>java.math.BigDecimal</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>double</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>Double</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>float</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>Float</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>inet</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>java.net.InetAddress</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>int</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>Int</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>list</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>Vector</em>, <em>List</em>, <em>Iterable</em>, <em>Seq</em>, <em>IndexedSeq</em>, <em>java.util.List</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>map</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>Map</em>, <em>TreeMap</em>, <em>java.util.HashMap</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>set</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>Set</em>, <em>TreeSet</em>, <em>java.util.HashSet</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>text</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>String</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>timestamp</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>Long</em>, <em>java.util.Date</em>, <em>java.sql.Date</em>, <em>org.joda.time.DateTime</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>uuid</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>java.util.UUID</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>timeuuid</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>java.util.UUID</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>varchar</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>String</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>varint</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>BigInt</em>, <em>java.math.BigInteger</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>tuple</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>TupleValue</em>, <em>scala.Product</em>, <em>org.apache.commons.lang3.tuple.Pair</em>, <em>org.apache.commons.lang3.tuple.Triple</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>user defined type</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>UDTValue</em></p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To be able to access values in a <em>CassandraRow</em>, we first must know their Scala data types.</p></div>
<div class="paragraph"><p>This table lists how Cassandra types are converted to Scala types. For example,
Cassandra&#8217;s TEXT, INT, and FLOAT map to Scala&#8217;s <em>String</em>, <em>Int</em>, and <em>Float</em>, respectively.
In some cases, there are multiple type conversion alternatives, such as for Cassandra&#8217;s SET
that can be converted to <em>scala.collection.immutable.Set</em>, <em>scala.collection.immutable.TreeSet</em>,
or <em>java.util.HashSet</em>. When accessing values, we simply specify which data type among
available alternatives to use.</p></div>
</div>
</div>
</section>
<section class="slide" id="em-cassandrarow-em-api">
<h2><em>CassandraRow</em> API</h2>
<div class="paragraph"><p><strong>Reading column values</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">API Call</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>get<em>Type</em></strong>(<em>column</em>) or <strong>get</strong>[<em>Type</em>](<em>column</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns a <em>column</em> value of a known <em>Type</em>, where <em>Type</em> is a Scala equivalent
of a Cassandra column type that can be primitive, collection, UDT, or tuple.
</p>
<p class="tableblock"></p>
<p class="tableblock"><br />
</p>
<p class="tableblock">For example, <strong>getString</strong>(<em>&#8230;&#8203;</em>), <strong>getSet</strong>[<em>Int</em>](<em>&#8230;&#8203;</em>), <strong>getUDTValue</strong>(<em>&#8230;&#8203;</em>),
<strong>get</strong>[<em>String</em>](<em>&#8230;&#8203;</em>), <strong>get</strong>[<em>Set</em>[<em>Int</em>]](<em>&#8230;&#8203;</em>), <strong>get</strong>[<em>UDTValue</em>](<em>&#8230;&#8203;</em>),
<strong>get</strong>[<em>Pair</em>[<em>Boolean</em>,<em>Double</em>]](<em>&#8230;&#8203;</em>).</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>If a <em>column</em> value does not exist, a <em>NullPointerException</em> is thrown.</p></div>
<div class="paragraph"><p>For a
collection <em>column</em> value that does not exist, an empty collection is returned.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>We are now ready to talk about the <em>CassandraRow</em> API.</p></div>
<div class="paragraph"><p>To access a value in a <em>CassandraRow</em>, we must know a Cassandra table <em>column</em> name,
such as <em>title</em>, <em>release_year</em>, <em>rating</em>, or <em>genres</em>, and a Scala data type for this value,
such as <em>String</em>, <em>Int</em>, <em>Float</em>, and <em>Set</em>.</p></div>
<div class="paragraph"><p>There are two versions of getters: <em>getType</em> and <em>get[Type]</em>. Both return a <em>column</em> value
of a known <em>Type</em>, where <em>Type</em> is a Scala equivalent
of a Cassandra column type that can be primitive, collection, UDT, or tuple.
Here are some examples of <strong>getString</strong>(<em>&#8230;&#8203;</em>) and <strong>getSet</strong>[<em>Int</em>](<em>&#8230;&#8203;</em>), as well as the examples of
<strong>get</strong>[<em>String</em>](<em>&#8230;&#8203;</em>) and <strong>get</strong>[<em>Set</em>[<em>Int</em>]](<em>&#8230;&#8203;</em>).</p></div>
<div class="paragraph"><p>It is possible that for a particular <em>CassandraRow</em>, a <em>column</em> value does not exist.
When reading a value that does not exist, a <em>NullPointerException</em> is thrown, except for collection columns when
an empty Scala collection is returned.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Reading column values that may not exist</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">API Call</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>get<em>Type</em>Option</strong>(<em>column</em>) or <strong>get</strong>[<em>Option</em>[<em>Type</em>]](<em>column</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns a <em>column</em> value of a known <em>Type</em> as a Scala <em>Option</em> value, where <em>Type</em> is a Scala equivalent
of a Cassandra column type that can be primitive, UDT, or tuple. These methods are not useful for
collection columns.
</p>
<p class="tableblock"><br />
</p>
<p class="tableblock"></p>
<p class="tableblock">For example, <strong>getStringOption</strong>(<em>&#8230;&#8203;</em>), <strong>getUDTValueOption</strong>(<em>&#8230;&#8203;</em>),
<strong>get</strong>[<em>Option</em>[<em>String</em>]](<em>&#8230;&#8203;</em>), <strong>get</strong>[<em>Option</em>[<em>UDTValue</em>]](<em>&#8230;&#8203;</em>),
<strong>get</strong>[<em>Option</em>[<em>Pair</em>[<em>Boolean</em>,<em>Double</em>]]](<em>&#8230;&#8203;</em>).</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>These methods should be used when reading potentially <em>null</em> data to prevent getting a <em>NullPointerException</em>.
Existing and non-existing <em>column</em> values become objects <em>Some</em>(<em>value</em>) and <em>None</em> of Scala type <em>Option</em>, respectively.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To handle missing or <em>null</em> values correctly, we should use these two getters that
wrap a <em>column</em> value as a Scala <em>Option</em> type. The <em>Option</em> type can result in
object <em>Some</em>(<em>value</em>) with an actual value or object <em>None</em> with no value. We can
use <em>Option</em>'s methods <em>isDefined</em>, <em>get</em>, and <em>getOrElse</em> to further process <em>Some</em>(<em>value</em>)
or <em>None</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="our-challenge-solution-2">
<h2>Our Challenge Solution</h2>
<div class="paragraph"><p><strong>Problem 1: Output all movies with word "pirate" in their titles</strong></p></div>
<div class="listingblock">
<div class="title"><em>Solution</em></div>
<div class="content">
<pre class="CodeRay"><code>movies.filter(row =&gt; row.getString("title").toLowerCase.contains("pirate"))
      .map{ row =&gt; row.getString("title") +
            " (" + row.getInt("release_year") + ")" +
            " [" + row.getFloatOption("rating").getOrElse("Not rated yet") + "]" }
      .collect
      .foreach(println)</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Results</em></div>
<div class="content">
<pre class="CodeRay"><code>Pirates of the Caribbean: Dead Men Tell No Tales (2017) [Not rated yet]
Pirates of the Caribbean: On Stranger Tides (2011) [6.7]
Pirates of the Caribbean: At World's End (2007) [7.1]
Pirates of the Caribbean: Dead Man's Chest (2006) [7.3]
Pirates of the Caribbean: The Curse of the Black Pearl (2003) [8.1]</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>We are now properly equipped so solve our challenge.</p></div>
<div class="paragraph"><p>To output all movies with word "pirate" in their titles, we are applying
transformation <em>filter</em> with the following condition. For each Cassandra row,
get a column "title" value using <em>getString("title")</em>, convert the value to lowercase,
and verify that the value contains substring "pirate".</p></div>
<div class="paragraph"><p>We are using transformation <em>map</em> to convert each Cassandra row into a string literal in
the required format. <em>getString("title")</em>, followed by <em>getInt("release_year")</em> in parenthesis,
followed by <em>getFloatOption("rating")</em> in square brackets are all concatenated into a single
value. The "rating" value gets a special <em>Option</em> treatment because it may not exist for every movie in
the table. Notice how the final results are displayed.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Problem 2: Output all movies with genre "Adventure" and a rating of 7.5 or higher</strong></p></div>
<div class="listingblock">
<div class="title"><em>Solution</em></div>
<div class="content">
<pre class="CodeRay"><code>movies.filter{ row =&gt; row.getSet[String]("genres").contains("Adventure") &amp;&amp;
                      row.get[Option[Float]]("rating").isDefined &amp;&amp;
                      row.get[Option[Float]]("rating").get &gt;= 7.5 }
      .map{ row =&gt; row.getString("title") +
            " (" + row.getInt("release_year") + ")" +
            " [" + row.getFloat("rating") + "]" }
      .collect
      .foreach(println)</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Results</em></div>
<div class="content">
<pre class="CodeRay"><code>Pirates of the Caribbean: The Curse of the Black Pearl (2003) [8.1]</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Finally, outputting all movies with genre "Adventure" and a rating of 7.5 or higher can be achieved
using <em>filter</em> and <em>map</em> in a similar fashion.</p></div>
<div class="paragraph"><p>The <em>filter</em> predicate is more complex. We are getting a set value for "genres" using
<em>getSet[String]("genres")</em> and verifying that the set contains "Adventure" as its element.
In addition, we are accessing "rating" and verifying that its value exists and if so, we are
checking that it is greater or equal to 7.5.</p></div>
<div class="paragraph"><p>The <em>map</em> logic is actually simpler than in the previous example. In particular,
a "rating" value is assumed to exist because all <em>null</em> or not defined values were filtered out in
the preceding step.</p></div>
<div class="paragraph"><p>Only one movie that satisfies our query is returned in the output.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-cassandra-connector-cassandra-convert-data">
<h2>Retrieving an RDD of Cassandra Rows</h2>
<div class="paragraph"><p><strong>Example Cassandra table from the KillrVideo domain</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-cassandra-connector/cassandra-convert-data/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val movies = sc.cassandraTable("killr_video","movies_by_actor")
               .where("actor = 'Johnny Depp'")
               .select("release_year","title","rating")

// movies: com.datastax.spark.connector.rdd.CassandraTableScanRDD[
//                      com.datastax.spark.connector.CassandraRow]</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In many cases, you will retrieve data from a Cassandra table as an RDD of <em>CassandraRow</em> objects.
Like in this example, we are retrieving data from table <em>movies_by_actor</em> with partition key <em>actor</em>
and clustering columns <em>release_year</em> and <em>movie_id</em>. The <em>movies</em> RDD
contains <em>CassandraRow</em> objects that correspond to rows in this table. Each resulting Cassandra row
has columns "release_year", "title", and "rating" that describe a movie featuring
"Johnny Depp".</p></div>
</div>
</div>
</section>
<section class="slide" id="the-challenge-3">
<h2>The Challenge</h2>
<div class="paragraph"><p><strong>Converting Cassandra rows to tuples or objects</strong></p></div>
<div class="ulist">
<ul>
<li>Important to support certain RDD operations</li>
<li>Convenient to work with for some applications</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Besides <em>CassandraRow</em> objects, it is also common to use other structures for holding data retrieved from Cassandra,
such as
Scala tuples or Scala case class objects. This can be important to support certain RDD operations, including
many Spark transformations and actions that are defined for Key-Value Pair RDDs. It can also
be convenient to deal with class objects from  the application development point of view.</p></div>
<div class="paragraph"><p>In this presentation, our challenge is to establish good practices for converting Cassandra rows to
tuples or case class objects.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-cassandra-connector-api-2">
<h2>Spark-Cassandra Connector API</h2>
<div class="paragraph"><p><strong>Retrieving and converting Cassandra data</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">API Call</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>cassandraTable</strong>[<em>Type</em>](<em>keyspace</em>, <em>table</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns an RDD that contains all rows from a Cassandra <em>table</em> in a specified <em>keyspace</em>.
The resulting RDD elements are of type <em>Type</em>, which is usually a Scala tuple definition
or case class name.
Ordering of tuple components and naming of case class properties have to match
column ordering and naming, respectively.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>as</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Optionally used with <em>cassandraTable</em>() to define a mapping <em>f</em> from column values to
Scala tuple components, case class object constructor parameters, or other constructs. This is
the most generic method for data conversion on the Cassandra side.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>keyBy</strong>[<em>KeyType</em>](<em>columns</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Optionally used with <em>cassandraTable</em>[<em>ValueType</em>](&#8230;&#8203;) to convert Cassandra rows to pairs
of objects, where a pair key is of type <em>KeyType</em> and a pair value is of type <em>ValueType</em>.
<em>KeyType</em> and <em>ValueType</em> are usually defined as Scala tuples and/or case classes.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Primarily, we should be using the following three methods provided by the <em>Spark-Cassandra Connector</em> API
for retrieving data from Cassandra in a desired format. The conversion is performed on the Cassandra side,
making it much more efficient than if we were to convert using Spark transformations.</p></div>
<div class="paragraph"><p>First, method <em>cassandraTable</em> allows specifying a <em>Type</em> that should be used for resulting RDD elements. This type
can be a primitive type, tuple, or user-defined class. When using tuples,  ordering of tuple components must
match ordering of row columns retrieved from Cassandra. When using case classes, property names must match to column names
exactly or by converting the underscore naming convention used in Cassandra to the camelCase naming convention used for Scala.
<em>casssandraTable</em> provides a short and convenient syntax for simple conversions.</p></div>
<div class="paragraph"><p>Second, method <em>as</em> can be used together with <em>cassandraTable</em> to convert data based on a supplied mapping
function <em>f</em>. The function takes row columns as parameters and maps them to any desirable representation,
such as a tuple, object, or you name it. This is the most generic method for data conversion
on the Cassandra side that you may ever need. When using method <em>as</em>, you can control ordering or naming
as needed.</p></div>
<div class="paragraph"><p>Finally, there is method <em>keyBy</em> that can be used together with <em>cassandraTable</em> to represent data as
key-value pairs, where both key and value can be of primitive or complex data type.</p></div>
</div>
</div>
</section>
<section class="slide" id="solution-1-rows-to-tuples">
<h2>Solution 1: Rows-to-Tuples</h2>
<div class="listingblock">
<div class="title"><em>Using method cassandraTable[Type](&#8230;&#8203;)</em></div>
<div class="content">
<pre class="CodeRay"><code>val movies = sc.cassandraTable[(Int,String,Option[Float])]("killr_video","movies_by_actor")
               .where("actor = 'Johnny Depp'").select("release_year","title","rating")
// movies: com.datastax.spark.connector.rdd.CassandraTableScanRDD[(Int, String, Option[Float])]</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Using method as(f)</em></div>
<div class="content">
<pre class="CodeRay"><code>val movies = sc.cassandraTable("killr_video","movies_by_actor")
               .where("actor = 'Johnny Depp'").select("release_year","title","rating")
               .as((y:Int,t:String,r:Option[Float]) =&gt; (y,t,r))
// movies: com.datastax.spark.connector.rdd.CassandraRDD[(Int, String, Option[Float])]</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In this solution, we demonstrate row-to-tuple conversion using two of the discussed methods.</p></div>
<div class="paragraph"><p>Using <em>cassandraTable</em>, we are specifying a tuple with three components of type <em>Int</em>,
<em>String</em>, and <em>Option</em>[<em>Float</em>]. These Scala data types directly correspond to data types of columns
<em>release_year</em>, <em>title</em>, and <em>rating</em> of table <em>movies_by_actor</em>.
<em>Option</em> is used to handle <em>null</em> values in column <em>rating</em>. Notice how the resulting RDD <em>movies</em>
is defined - it contains tuples rather than <em>CassandraRow</em> objects.</p></div>
<div class="paragraph"><p>Using <em>as</em>, we are defining the function with parameters <em>y</em>, <em>t</em>, and <em>r</em> that simply returns a tuple
with the three components, resulting in the same conversion as before.</p></div>
</div>
</div>
</section>
<section class="slide" id="solution-2-rows-to-objects">
<h2>Solution 2: Rows-to-Objects</h2>
<div class="listingblock">
<div class="title"><em>Using method cassandraTable[Type](&#8230;&#8203;)</em></div>
<div class="content">
<pre class="CodeRay"><code>case class Record(releaseYear: Int, title: String, rating: Option[Float])
val movies = sc.cassandraTable[Record]("killr_video","movies_by_actor")
               .where("actor = 'Johnny Depp'").select("release_year","title","rating")
// movies: com.datastax.spark.connector.rdd.CassandraTableScanRDD[Record]</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Using method as(f)</em></div>
<div class="content">
<pre class="CodeRay"><code>case class Record(releaseYear: Int, title: String, rating: Option[Float])
val movies = sc.cassandraTable("killr_video","movies_by_actor")
               .where("actor = 'Johnny Depp'").select("release_year","title","rating")
               .as((y:Int,t:String,r:Option[Float]) =&gt; new Record(y,t,r))
// movies: com.datastax.spark.connector.rdd.CassandraRDD[Record]</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here, we are converting rows to case class objects using the same two methods.</p></div>
<div class="paragraph"><p>We are first defining case class <em>Record</em> with three property names. Note that <em>releaseYear</em> uses
the camelCase naming convention corresponding to column <em>release_year</em> with underscore. Instead of <em>releaseYear</em>,
we could have also used <em>release_year</em> for the property name.
The other two
property names, <em>title</em> and <em>rating</em>, are exactly the same as the corresponding column names.</p></div>
<div class="paragraph"><p>For the solution using <em>cassandraTable</em>, we are simply specifying <em>Record</em> as a data type of resulting RDD elements.</p></div>
<div class="paragraph"><p>For the solution using <em>as</em>, we are defining a function that calls <em>Record</em>'s constructor and returns the resulting object.</p></div>
<div class="paragraph"><p>Both approaches work great!</p></div>
</div>
</div>
</section>
<section class="slide" id="solution-3-correct-but-less-efficient">
<h2>Solution 3: Correct but Less Efficient</h2>
<div class="listingblock">
<div class="title"><em>Row-to-tuple solution using transformation map(f)</em></div>
<div class="content">
<pre class="CodeRay"><code>val movies = sc.cassandraTable("killr_video","movies_by_actor")
               .where("actor = 'Johnny Depp'").select("release_year","title","rating")
               .map(row =&gt; (row.getInt("release_year"),row.getString("title"),row.getFloatOption("rating")))
// movies: org.apache.spark.rdd.RDD[(Int, String, Option[Float])]</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Row-to-object solution using transformation map(f)</em></div>
<div class="content">
<pre class="CodeRay"><code>case class Record(releaseYear: Int, title: String, rating: Option[Float])
val movies = sc.cassandraTable("killr_video","movies_by_actor")
               .where("actor = 'Johnny Depp'").select("release_year","title","rating")
               .map(row =&gt; new Record(row.getInt("release_year"),row.getString("title"),
                                      row.getFloatOption("rating")))
// movies: org.apache.spark.rdd.RDD[Record]</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Finally, for educational purposes, we are demonstrating a solution that uses
Spark&#8217;s <em>map</em> transformation. Indeed, <em>map</em> allows us to achieve the same  result
when converting rows to tuples or rows to objects. However this solution is less efficient
than the previous two solutions. Using <em>map</em>, we are converting data on the Spark side, which implies that
data first has to be retrieved from Cassandra as <em>CassandraRow</em> objects, transferred to Spark and only then
transformed to tuples or case class objects.
This is how not to convert your data to tuples or case class objects!</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-cassandra-connector-cassandra-save-data">
<h2>Saving an RDD into a Cassandra Table</h2>
<div class="paragraph"><p><strong>Common scenarios and an example Cassandra table</strong></p></div>
<div class="imageblock" style="float: right">
<div class="content">
<img alt="favorite movies" src="images/spark/spark-cassandra-connector/cassandra-save-data/favorite_movies.svg" />
</div>
</div>
<div class="ulist">
<ul>
<li>Saving an RDD with <em>CassandraRow</em> objects</li>
<li>Saving an RDD with case class objects</li>
<li>Saving an RDD with tuples</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>A you remember, Spark is not a data storage system. Spark gets its datasets from an
external system in the form of RDDs, process and analyses them, and pushes results back to the same
or possibly different external system.
Therefore, knowing how an RDD can be saved into a Cassandra table is very important.</p></div>
<div class="paragraph"><p>We will cover the three common scenarios of saving an RDD with
<em>CassandraRow</em> objects, user-defined case class objects, and tuples, respectively.
We will use table <em>favorite_movies</em> from the <em>KillrVidoe</em> domain for our examples.
Note that this table has the primary key consisting of two partition key columns <em>title</em>
and <em>release_year</em>. The table has three columns of primitive data types (TEXT, INT, and FLOAT),
one collection column <em>genres</em> of type SET, and one UDT column <em>details</em> that stores three primitive values
for <em>country</em>, <em>language</em>, and movie <em>runtime</em> in minutes.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-cassandra-connector-api-3">
<h2>Spark-Cassandra Connector API</h2>
<div class="paragraph"><p><strong>RDD action that saves data into an existing table</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">API Call</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>saveToCassandra</strong>(<em>keyspace</em>, <em>table</em>, [SomeColumns(<em>columns</em>)])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Inserts each source RDD element as a row into an existing Cassandra <em>table</em> in a specified <em>keyspace</em>.
The last optional parameter is a column selector that is used to specify
how object properties or tuple components map to
specific <em>table</em> <em>columns</em>.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>Spark-Cassandra Connector</em> API provides RDD action <em>saveToCassandra</em> that can be called on
an RDD to insert its elements as rows into an existing Cassandra <em>table</em> in a specified <em>keyspace</em>.
The last optional parameter is a column selector, which is an object of class <em>SomeColumns</em>; it
allows specifying how object properties or tuple components should be mapped to
specific <em>table</em> <em>columns</em>. The column selector can be omitted if tuple components or
object property names match column ordering or column names in the <em>table</em>, respectively.
Yet, it always a good idea to specify <em>columns</em> explicitly for better code readability.</p></div>
<div class="paragraph"><p>To use this method, the table must already exist in Cassandra.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>RDD actions that create and save data into a new table</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">API Call</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>saveAsCassandraTable</strong>(<em>keyspace</em>, <em>table</em>, [SomeColumns(<em>columns</em>)])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Creates a Cassandra <em>table</em> in an existing <em>keyspace</em> and inserts each source RDD element as a row
into the <em>table</em>. A newly created <em>table</em> has a primary key consisting of the first column
in the <em>SomeColumns(&#8230;&#8203;)</em> parameter, if present, or a column corresponding to the first property name
in an RDD element class definition.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>saveAsCassandraTableEx</strong>(<em>tableDefinition</em>, [SomeColumns(<em>columns</em>)])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Creates a Cassandra table in an existing keyspace according to a specified <em>tableDefinition</em> and
inserts each source RDD element as a row
into the table. <em>tableDefinition</em> can be used to customize partition key and clustering key columns, as well
as any additional columns and their data types.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>These two RDD actions allow creating a new Cassandra <em>table</em> in an existing <em>keyspace</em> and inserting all RDD elements
into this table. The major difference is that <em>saveAsCassandraTable</em> creates a table with only one column in its
primary key, while <em>saveAsCassandraTableEx</em> supports a fully-customizable <em>tableDefinition</em> with desired
partition key and clustering key columns, as well
as any additional columns and their data types.</p></div>
<div class="paragraph"><p>As an alternative to these two methods, one can issue a regular CQL CREATE TABLE statement via the <em>CQL Connector</em>
(another library) to
create a table first, and then use action <em>saveToCassandra</em> to insert data.</p></div>
</div>
</div>
</section>
<section class="slide" id="saving-an-rdd-with-em-cassandrarow-em-objects">
<h2>Saving an RDD with <em>CassandraRow</em> Objects</h2>
<div class="listingblock">
<div class="title"><em>movie: RDD[CassandraRow]</em></div>
<div class="content">
<pre class="CodeRay"><code>val movie = sc.cassandraTable("killr_video","movies_by_actor")
              .where("actor = 'Johnny Depp'")
              .select("title","release_year","rating")
              .filter(row =&gt; row.getString("title") == "Alice in Wonderland")

movie.saveToCassandra("killr_video","favorite_movies",
                      SomeColumns("title","release_year","rating"))</code></pre>
</div>
</div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:12%" />
<col style="width:25%" />
<col style="width:25%" />
<col style="width:12%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">title</th>
<th class="tableblock halign-left valign-top">release_year</th>
<th class="tableblock halign-left valign-top">details</th>
<th class="tableblock halign-left valign-top">genres</th>
<th class="tableblock halign-left valign-top">rating</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Alice in Wonderland</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2010</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">null</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">null</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">6.5</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In our first example, let us save an RDD with <em>CassandraRow</em> objects into table <em>favorite_movies</em>.</p></div>
<div class="paragraph"><p>Since you are most likely to get an RDD with Cassandra rows by retrieving data from Cassandra, we
are doing the same here. The <em>movie</em> RDD is populated by retrieving data from table
<em>movies_by_actor</em>. We are only interested in Johnny Depp&#8217;s movie "Alice in Wonderland" and three columns:
<em>title</em>, <em>release_year</em>, and <em>rating</em>.</p></div>
<div class="paragraph"><p>As you can see, using <em>saveToCassandra</em> is quite straightforward and convenient, too.
A single row is saved into table <em>favorite_movies</em> as shown at the bottom. We will
insert values for <em>genres</em> and <em>details</em> in the following examples.</p></div>
</div>
</div>
</section>
<section class="slide" id="saving-an-rdd-with-case-class-objects">
<h2>Saving an RDD with Case Class Objects</h2>
<div class="listingblock">
<div class="title"><em>genres: RDD[GenresInfo]</em></div>
<div class="content">
<pre class="CodeRay"><code>case class GenresInfo (title: String, releaseYear: Int, genres: Set[String])

val genres = sc.parallelize(Seq(
                 new GenresInfo("Alice in Wonderland",2010,Set("Adventure","Family"))
                ))

genres.saveToCassandra("killr_video","favorite_movies",
                       SomeColumns("title","release_year","genres"))</code></pre>
</div>
</div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:12%" />
<col style="width:25%" />
<col style="width:25%" />
<col style="width:12%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">title</th>
<th class="tableblock halign-left valign-top">release_year</th>
<th class="tableblock halign-left valign-top">details</th>
<th class="tableblock halign-left valign-top">genres</th>
<th class="tableblock halign-left valign-top">rating</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Alice in Wonderland</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2010</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">null</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">{'Adventure', 'Family'}</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">6.5</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Our second example demonstrates how to save an RDD of case class objects into the same Cassandra table.</p></div>
<div class="paragraph"><p>Here, we are defining case class <em>GenresInfo</em> with properties <em>title</em>, <em>releaseYear</em>, and <em>genres</em>.
We do need to always specify values for columns <em>title</em> and <em>release_year</em> in table <em>favorite_movies</em> at the very minimum,
because these two columns constitute a primary key.</p></div>
<div class="paragraph"><p>We are then creating RDD <em>genres</em> with a single element of type <em>GenresInfo</em> representing movie
"Alice in Wonderland" and inserting this element into table <em>favorite_movies</em> using action <em>saveToCassandra</em>.
Technically, we are doing an upsert to update our previously inseted row with <em>genres</em> information.</p></div>
<div class="paragraph"><p>Note that we are dealing with collection column <em>genres</em> using exactly the same procedure as with the columns of primitive
types. By default, Cassandra overwrites an existing collection value, if present, but <em>Spark-Cassandra Connector</em>
also provides means to prepend, append, and remove values in collection columns of different types.</p></div>
</div>
</div>
</section>
<section class="slide" id="saving-an-rdd-with-tuples">
<h2>Saving an RDD with Tuples</h2>
<div class="listingblock">
<div class="title"><em>details: RDD[(String, Int, UDTValue)]</em></div>
<div class="content">
<pre class="CodeRay"><code>val details = sc.parallelize(Seq(
                  ("Alice in Wonderland",2010,
                   UDTValue.fromMap(Map("country"-&gt;"USA","language"-&gt;"English","runtime"-&gt;108)))
                 ))

details.saveToCassandra("killr_video","favorite_movies",
                        SomeColumns("title","release_year","details"))</code></pre>
</div>
</div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:12%" />
<col style="width:25%" />
<col style="width:25%" />
<col style="width:12%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">title</th>
<th class="tableblock halign-left valign-top">release_year</th>
<th class="tableblock halign-left valign-top">details</th>
<th class="tableblock halign-left valign-top">genres</th>
<th class="tableblock halign-left valign-top">rating</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Alice in Wonderland</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2010</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">{country: 'USA', language: 'English', runtime: 108}</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">{'Adventure', 'Family'}</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">6.5</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Finally, let us demonstrate how to save an RDD with tuples.</p></div>
<div class="paragraph"><p>We are creating the <em>details</em> RDD by parallelizing a sequence with a single tuple.
The first two tuple components correspond to the movie title ("Alice in Wonderland") and release year (2010),
which are required to uniquely identify a row. The third and last component represents a UDT value that is
created from a Scala <em>Map</em> with keys <em>country</em>, <em>language</em>, and <em>runtime</em>.</p></div>
<div class="paragraph"><p>As you can see, even though we are dealing with tuples and a UDT column, the <em>saveToCassandra</em> call is
rather similar to our previous examples. The resulting row in table <em>favorite_movie</em> after our three insert
is shown and is now complete.</p></div>
<div class="paragraph"><p>In all the examples, for brevity of our presentation, each RDD contained only one element. You
can try saving RDDs with multiple elements to insert many rows into a Cassandra table&#8201;&#8212;&#8201;programming
logic will stay the same. Also, you should give a try to actions <em>saveAsCassandraTable</em> and <em>saveAsCassandraTableEx</em>.
Have fun!</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-optimization-techniques-broadcast-variables">
<h2>Shared Variables in Spark</h2>
<div class="paragraph"><p><strong>Used to optimize your code performance</strong></p></div>
<div class="ulist">
<ul>
<li>Broadcast variables</li>
<li>Accumulator variables</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>Shared variables are purely for optimization and efficiency.</p></div>
<div class="paragraph"><p>You only need to use shared variables explicitly when a use case is right.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark supports two types of shared variables: broadcast variables and accumulator variables.
You only need to use shared variables explicitly to optimize your code in certain situations.</p></div>
<div class="paragraph"><p>In this presentation, we will discuss broadcast variables.</p></div>
</div>
</div>
</section>
<section class="slide" id="broadcast-variables">
<h2>Broadcast Variables</h2>
<div class="paragraph"><p><strong>Definition and use cases</strong></p></div>
<div class="paragraph"><p><em><strong>Definition</strong></em></p></div>
<div class="verseblock">
<pre class="content">Broadcast variables are read-only variables whose values are
broadcasted and cached on each node in a cluster and are
visible to all tasks of your application</pre>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="paragraph"><p><em><strong>Main use cases</strong></em></p></div>
<div class="ulist">
<ul>
<li>Application tasks across multiple stages need the same, relatively large and immutable dataset</li>
<li>Application tasks need the same, relatively large and immutable dataset cached in deserialized form</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Broadcast variables are read-only variables whose values are
broadcasted and cached on each node in a cluster and are
visible to all tasks of your application.</p></div>
<div class="paragraph"><p>Spark uses special algorithms that reduce communication costs when broadcasting data.
However, if a broadcast variable is used in only one task on each node in a cluster,
the benefits are negligible in comparison with a regular variable.</p></div>
<div class="paragraph"><p>The main reasons to use broadcast variables include:</p></div>
<div class="ulist">
<ul>
<li>Application tasks across multiple stages need the same, relatively large and immutable dataset</li>
<li>Application tasks need the same, relatively large and immutable dataset cached in deserialized form</li>
</ul>
</div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>API</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Statement</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">val <em>broadcastVar</em> = <em>sc</em>.<em>broadcast</em>(<em>object</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Declare and initialize broadcast variable <em>broadcastVar</em> with the <em>object</em> value of any type
using method <em>broadcast</em> of the Spark Context object <em>sc</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>broadcastVar</em>.<em>value</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Access <em>value</em> of broadcast variable <em>broadcastVar</em> in a Spark operation, such as <em>map</em>, <em>filter</em>, or <em>reduce</em>.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Broadcast variables are very simple to create and use.</p></div>
</div>
</div>
</section>
<section class="slide" id="example">
<h2>Example</h2>
<div class="paragraph"><p><strong>Not using broadcast variables</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/optimization-techniques/broadcast-variables/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val popularTitles = Set("Alice in Wonderland",
                        "Alice Through the Looking Glass", "...")

val movies = sc.cassandraTable("killr_video","movies")
               .select("title","release_year","rating","genres")
               .cache

movies.filter(row =&gt; popularTitles contains row.getString("title"))
      .saveToCassandra("killr_video", "favorite_movies",
                       SomeColumns("title","release_year","rating","genres"))

movies.filter(row =&gt; !(popularTitles contains row.getString("title")))
      .saveToCassandra("killr_video", "other_movies",
                       SomeColumns("title","release_year","rating","genres"))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To demonstrate when and how to use broadcast variables, let us explore this example first. We are not using
broadcast variables yet.</p></div>
<div class="paragraph"><p>Here, we have a regular application variable <em>popularTitles</em> (technically, it is a value) that holds
a set of movie titles. We also retrieve data from Cassandra table <em>movies</em> into RDD <em>movies</em> and cache this RDD
because it is used with multiple actions.
The next statement filters RDD <em>movies</em> and saves only those Cassandra rows into table <em>favorite_movies</em> that
have popular titles. The last statement filters RDD <em>movies</em> and saves only those Cassandra rows into
table <em>other_movies</em> that do not have popular titles.</p></div>
<div class="paragraph"><p>The computation logic is quite simple and this code runs with no errors. However, notice
that <em>popularTitles</em> is used in two <em>filter</em> transformations and therefore, its value will be copied to two
respective tasks on each node executing these transformations. If <em>popularTitles</em> is a large dataset,
this is when we should consider using
a broadcast variable to hold popular titles to optimize our code.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Using broadcast variables</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/optimization-techniques/broadcast-variables/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val popularTitles = sc.broadcast(Set("Alice in Wonderland",
                                     "Alice Through the Looking Glass", "..."))

val movies = sc.cassandraTable("killr_video","movies")
               .select("title","release_year","rating","genres")
               .cache

movies.filter(row =&gt; popularTitles.value contains row.getString("title"))
      .saveToCassandra("killr_video", "favorite_movies",
                       SomeColumns("title","release_year","rating","genres"))

movies.filter(row =&gt; !(popularTitles.value contains row.getString("title")))
      .saveToCassandra("killr_video", "other_movies",
                       SomeColumns("title","release_year","rating","genres"))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This code is now optimized!
<em>popularTitle</em> is a broadcast variable whose value is copied and cached on each node only one time.
The value of <em>popularTitles</em> is then shared by both <em>filter</em> transformation tasks running on each node.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-optimization-techniques-accumulator-variables">
<h2>Shared Variables in Spark</h2>
<div class="paragraph"><p><strong>Used to optimize your code performance</strong></p></div>
<div class="ulist">
<ul>
<li>Broadcast variables</li>
<li>Accumulator variables</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>Shared variables are purely for optimization and efficiency.</p></div>
<div class="paragraph"><p>You only need to use shared variables explicitly when a use case is right.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark supports two types of shared variables: broadcast variables and accumulator variables.
You only need to use shared variables explicitly to optimize your code in certain situations.</p></div>
<div class="paragraph"><p>In this presentation, we will discuss accumulator variables.</p></div>
</div>
</div>
</section>
<section class="slide" id="accumulator-variables">
<h2>Accumulator Variables</h2>
<div class="paragraph"><p><strong>Definition and use cases</strong></p></div>
<div class="paragraph"><p><em><strong>Definition</strong></em></p></div>
<div class="verseblock">
<pre class="content">Accumulator variables are aggregate-only variables whose values can be
updated using an associative operation by tasks running on each node in a cluster
and whose final aggregated values can be accessed in client&#8217;s driver program</pre>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="paragraph"><p><em><strong>Main use cases</strong></em></p></div>
<div class="ulist">
<ul>
<li>Counting and summation</li>
<li>Application needs to compute multiple aggregates on the same dataset</li>
<li>Application needs custom aggregation not supported by existing Spark operations</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Accumulator variables are aggregate-only variables whose values can be
updated using an associative operation by tasks running on each node in a cluster
and whose final aggregated values can be accessed in client&#8217;s driver program.</p></div>
<div class="paragraph"><p>Typical applications of accumulators are counting and summation.
The main reasons to use accumulator variables include:</p></div>
<div class="ulist">
<ul>
<li>Application needs to compute multiple aggregates on the same dataset</li>
<li>Application needs custom aggregation not supported by existing Spark operations</li>
</ul>
</div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>API</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Statement</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">val <em>accumulatorVar</em> = <em>sc</em>.<em>accumulator</em>(<em>initialValue</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Declare and initialize accumulator variable <em>accumulatorVar</em> with <em>initialValue</em> of a numeric type
using method <em>accumulator</em> of the Spark Context object <em>sc</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>accumulatorVar</em> += 1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Update <em>accumulatorVar</em> in a Spark action, such as <em>foreach</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>accumulatorVar</em>.<em>value</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Access final aggregated <em>value</em> of accumulator variable <em>accumulatorVar</em> in a client program.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Accumulator variables are very simple to create and use.</p></div>
<div class="paragraph"><p>It is worth noting that while Spark provides the build-in support for accumulator variables of
numeric types, it is also possible to extend this functionality to other types and even support situations
when an aggregate value is of a different type from input value types,
such us aggregating values into a collection.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-2">
<h2>Example</h2>
<div class="paragraph"><p><strong>Not using accumulator variables</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="favorite movies" src="images/spark/optimization-techniques/accumulator-variables/favorite_movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val movieRatings = sc.cassandraTable("killr_video","favorite_movies")
                     .select("rating")
                     .filter(row =&gt; row.getFloatOption("rating").isDefined)
                     .map(row =&gt; row.getFloat("rating"))
                     .cache

val numRatings   = movieRatings.count
val sumRatings   = movieRatings.sum
val avgRating    = sumRatings / numRatings

println(f"$avgRating%1.1f")</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To demonstrate when and how to use accumulator variables, let us explore this example first. We are
computing an average of all movie ratings stored in a Cassandra table. We are not using
accumulator variables yet.</p></div>
<div class="paragraph"><p>First, we retrieve all movie ratings from Cassandra table <em>favorite_movies</em> into RDD <em>movieRatings</em>,
<em>map</em> Cassandra rows with non-<em>null</em> ratings to just rating numeric values,
and <em>cache</em> the resulting RDD because it is used with multiple actions.</p></div>
<div class="paragraph"><p>Next, we are calling actions <em>count</em> and <em>sum</em> on RDD <em>movieRatings</em> to compute the number of ratings
and their total sum and store results as values <em>numRatings</em> and <em>sumRatings</em>, respectively.</p></div>
<div class="paragraph"><p>Finally, we are computing and printing an average rating locally (outside of Spark).</p></div>
<div class="paragraph"><p>The computation logic is quite simple and this code runs with no errors. However, notice
that we are using two actions on the same RDD and, even though we are caching the RDD to avoid re-computation,
there will be separate tasks scheduled to compute <em>count</em> and <em>sum</em>. It is like we are doing
two passes over the same dataset. With accumulator variables, we only need one pass!</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Using accumulator variables</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="favorite movies" src="images/spark/optimization-techniques/accumulator-variables/favorite_movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val numRatings = sc.accumulator(0)
val sumRatings = sc.accumulator(0.0)

sc.cassandraTable("killr_video","favorite_movies")
  .select("rating")
  .filter(row =&gt; row.getFloatOption("rating").isDefined)
  .foreach{row =&gt; numRatings += 1; sumRatings += row.getFloat("rating")}

val avgRating = sumRatings.value / numRatings.value

println(f"$avgRating%1.1f")</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This code is now optimized!
Both <em>numRating</em> and <em>sumRatings</em> are accumulator variables with initial values of 0 (of type <em>Int</em>) and
0.0 (of type <em>Double</em>), respectively. These variables are updated in the <em>foreach</em> action to do
counting and summation. The average is then computed locally (outside of Spark) by accessing final aggregate values.</p></div>
<div class="paragraph"><p>What we achieved:</p></div>
<div class="ulist">
<ul>
<li>We only use one action to do all aggregation and therefore, have to do only one pass over the dataset
retrieved from Cassandra.</li>
<li>We no longer need caching, which is an additional performance and memory-saving benefit.</li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="spark-optimization-techniques-rdd-persistence">
<h2>The Challenge: Suboptimal Code</h2>
<div class="paragraph"><p><strong>Computing percentages of comedy movies released in 2014 and 2013</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/optimization-techniques/rdd-persistence/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val movies = sc.cassandraTable("killr_video","movies")
               .select("release_year","genres")

val movies2014 = movies.filter(row =&gt; row.getInt("release_year") == 2014)
val total2014  = movies2014.count
val comedy2014 = movies2014.filter(row =&gt; row.getSet[String]("genres")
                                          contains "Comedy").count
val percentage2014 = 100.0 * comedy2014 / total2014

val movies2013 = movies.filter(row =&gt; row.getInt("release_year") == 2013)
val total2013  = movies2013.count
val comedy2013 = movies2013.filter(row =&gt; row.getSet[String]("genres")
                                          contains "Comedy").count
val percentage2013 = 100.0 * comedy2013 / total2013</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Study this simple program that serves as our running example in this presentation.</p></div>
<div class="paragraph"><p>Its goal is to analyze how the percentage of comedy movies from 2014 compares to the
percentage of comedy movies from 2013. Did we have more laughter in 2014 or in 2013?</p></div>
<div class="paragraph"><p>We first retrieve all movies from Cassandra table <em>movies</em>
and only keep information about <em>release_year</em> and <em>genres</em> for each movie.</p></div>
<div class="paragraph"><p>We then apply <em>filter</em> based on <em>release_year</em> == 2014 and <em>count</em> the total number
of movies release in 2014. We use another <em>filter</em> to only keep comedy movies from 2014 and we <em>count</em> them.</p></div>
<div class="paragraph"><p>Given the two counts, <em>total2014</em> and <em>comedy2014</em>, it is straightforward to compute the percentage of comedy
movies from 2014.</p></div>
<div class="paragraph"><p>We do the same for movies from 2013.</p></div>
<div class="paragraph"><p>Like we said, this program is simple and it works. However, it is suboptimal as we show in the following slides.
The challenge is to optimize this code to run faster!</p></div>
</div>
</div>
</section>
<section class="slide" id="dag-of-operations">
<h2>DAG of Operations</h2>
<div class="imageblock center">
<div class="content">
<img alt="dag" src="images/spark/optimization-techniques/rdd-persistence/dag.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To understand how our program is executed by Spark, let us look at the
Directed Acyclic Graph (DAG) of operations (e.g., transformations and actions).
In this case, the DAG is really a tree because we are not using any binary operations.</p></div>
<div class="paragraph"><p>Think about this DAG as a logical evaluation plan for our computation.
Notice that leaf nodes ALWAYS represent results returned by actions because only actions trigger computation.
Transformations are evaluated lazily and will not do actual computation until an action is seen.</p></div>
<div class="paragraph"><p>A physical evaluation plan may however be different.</p></div>
</div>
</div>
</section>
<section class="slide" id="stages-of-computation">
<h2>Stages of Computation</h2>
<div class="paragraph"><p><strong>Stage 1</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="stage1" src="images/spark/optimization-techniques/rdd-persistence/stage1.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark will evaluate our program in stages. Here is the first stage.</p></div>
<div class="paragraph"><p>When Spark sees the first <em>count</em> action, it starts evaluation. In particular, to return the result to a client,
Spark needs to compute all intermediate RDDs: retrieve data from Cassandra into RDD <em>movies</em>, apply <em>filter</em> to get new RDD
<em>movies2014</em> and <em>count</em> its elements.</p></div>
<div class="paragraph"><p>What is VERY IMPORTANT to understand is that intermediate data products, such RDDs <em>movies</em> and <em>movies2014</em>,
are not completely materialized in memory. Spark only needs one partition to schedule a computational task and once
the task completes, memory can be reused for another partition that will be processed by another task.
In other words, intermediate results will be lost.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Stages 1 and 2</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="stage2" src="images/spark/optimization-techniques/rdd-persistence/stage2.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Once Spark finishes Stage 1, resources are freed to deal with our second <em>count</em> action.
This is Stage 2. While tasks in each stage are executed in parallel, stages are executed sequentially.</p></div>
<div class="paragraph"><p>To compute the result of the second <em>count</em>, Spark has to compute three intermediate data products. We
have seen RDDs <em>movies</em> and <em>movies2014</em> before, in Stage 1, but their data is lost and has to be recomputed
again in Stage 2. The third RDD is unnamed: we do not have a <em>val</em> to refer to it in the program.</p></div>
<div class="paragraph"><p>Wow! Stage 2 does not reuse anything from Stage 1!</p></div>
<div class="paragraph"><p>Note: "(1)" and "(2)" on the diagram denote first and second time a dataset is computed, respectively. For
example, data for RDDs <em>movies</em> and <em>movies2014</em> is computed for the second time in Stage 2.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Stages 1, 2, and 3</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="stage3" src="images/spark/optimization-techniques/rdd-persistence/stage3.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Stage 3. You get the idea. RDD <em>movies</em> is recomputed again.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Stages 1, 2, 3, and 4</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="stage4" src="images/spark/optimization-techniques/rdd-persistence/stage4.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Final Stage 4. How inefficient! A lot of redundant re-computation!</p></div>
</div>
</div>
</section>
<section class="slide" id="the-challenge-summary">
<h2>The Challenge: Summary</h2>
<div class="paragraph"><p>Efficiency issues:</p></div>
<div class="ulist">
<ul>
<li>Reading the same data from Cassandra into RDD <em>movies</em> four times</li>
<li>Recomputing RDD <em>movies2014</em> twice</li>
<li>Recomputing RDD <em>movies2013</em> twice</li>
</ul>
</div>
<div class="paragraph"><p>Take aways:</p></div>
<div class="ulist">
<ul>
<li>Recomputing an RDD multiple times due to multiple actions on the RDD or its derivatives</li>
<li>Need a way to materialize and reuse an RDD after it is computed for the first time</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Indeed, our code is suboptimal.
We can do better!</p></div>
</div>
</div>
</section>
<section class="slide" id="rdd-persistence">
<h2>RDD Persistence</h2>
<div class="paragraph"><p><strong>One of the most important optimizations in Spark for iterative algorithms and interactive computation!</strong></p></div>
<div class="ulist">
<ul>
<li>You can instruct Spark to cache or persist any RDD in your program</li>
<li>Persisted RDD is kept in memory (by default) once it is computed for the first time</li>
<li>Persisted RDD is reused by other operations that require the same dataset</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>The persistence mechanism should be used to avoid recomputing the same dataset multiple times.
With respect to a DAG of operations, any RDD that is a common ancestor of two or more leaf nodes resulting from
actions is a good candidate for the persistence optimization.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The mechanism that allows us to materialize and reuse an RDD in Spark is called <em>RDD persistence</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="rdd-persistence-api">
<h2>RDD Persistence API</h2>
<div class="paragraph"><p><strong>Any RDD, including a Cassandra RDD, can be cached or persisted</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>persist</strong>([<em>storageLevel</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persists the source RDD according to a storage level specified by the optional parameter. The default storage level is
<em>org.apache.spark.storage.StorageLevel.MEMORY_ONLY</em>, which prescribes persisting
RDD elements as deserialized Java objects in the JVM.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>cache</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Same as <em>persist</em>() or <em>persist</em>(<em>StorageLevel.MEMORY_ONLY</em>).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>unpersist</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Unpersists the source RDD (manually). It is safe to not use this transformation
because Spark automatically monitors and unpersists least-recently-used RDD partitions.</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>When using <em>cache</em>() or <em>persist</em>(), if an RDD does not fit into memory,
some partitions will not be cached and will be recomputed on the fly when needed.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here are the three transformations of the RDD Persistence API.</p></div>
<div class="paragraph"><p>More storage levels for <em>persist</em> are discussed on the next slide.</p></div>
<div class="paragraph"><p><em>cache</em> is a convenient synonym of <em>persist</em> with the default storage level.</p></div>
<div class="paragraph"><p>You almost never need to use <em>unpersist</em> explicitly unless there is a specific need
in your application to force Spark to unpersist an RDD immediately.</p></div>
</div>
</div>
</section>
<section class="slide" id="storage-levels">
<h2>Storage Levels</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Storage Level</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>MEMORY_ONLY</em> <em>MEMORY_ONLY_SER</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting RDD elements as <em>deserialized</em> or <em>serialized</em> Java objects in the JVM, respectively. Partitions
that do not fit into memory are not cached and recomputed when needed.
<em>MEMORY_ONLY_SER</em> is more space-efficient but more CPU-intensive than <em>MEMORY_ONLY</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>MEMORY_AND_DISK</em> <em>MEMORY_AND_DISK_SER</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting RDD elements as <em>deserialized</em> or <em>serialized</em> Java objects in the JVM, respectively. Partitions
that do not fit into memory are spilled to disk.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>DISK_ONLY</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting RDD partitions on disk.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>MEMORY_ONLY_2</em>, <em>MEMORY_AND_DISK_2</em>, &#8230;&#8203;</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Same as the respective storage levels above, but with replication on two nodes in a cluster.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>OFF_HEAP</em> (experimental)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting the source RDD in <em>serialized</em> format in <em>Tachyon</em> (a memory-centric distributed storage system).</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>In some cases, recomputing partitions may be faster than reading persisted partitions from disk!</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Study the different storage level possibilities for <em>persist</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="solving-the-challenge">
<h2>Solving the Challenge</h2>
<div class="paragraph"><p><strong>Caching RDD <em>movies</em></strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="cache1" src="images/spark/optimization-techniques/rdd-persistence/cache1.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Let us use the Persistence API to optimize our program.</p></div>
<div class="paragraph"><p>This diagram shows the four stages of computation after
we <em>cache</em> RDD <em>movies</em>. As a result, RDD <em>movies</em> is computed
and cached once during Stage 1 and is reused in the remaining three stages.</p></div>
<div class="paragraph"><p>We still have redundant computation for RDDs <em>movies2014</em> and <em>movies2013</em>.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Caching RDDs <em>movies2014</em> and <em>movies2013</em></strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="cache2" src="images/spark/optimization-techniques/rdd-persistence/cache2.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>After we <em>cache</em> RDDs <em>movies2014</em> and <em>movies2013</em>, we have
optimal computation.</p></div>
</div>
</div>
</section>
<section class="slide" id="final-solution-2">
<h2>Final Solution</h2>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/optimization-techniques/rdd-persistence/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val movies = sc.cassandraTable("killr_video","movies")
               .select("release_year","genres")
               .cache

val movies2014 = movies.filter(row =&gt; row.getInt("release_year") == 2014)
                       .cache
val total2014  = movies2014.count
val comedy2014 = movies2014.filter(row =&gt; row.getSet[String]("genres")
                                          contains "Comedy").count
val percentage2014 = 100.0 * comedy2014 / total2014

val movies2013 = movies.filter(row =&gt; row.getInt("release_year") == 2013)
                       .cache
val total2013  = movies2013.count
val comedy2013 = movies2013.filter(row =&gt; row.getSet[String]("genres")
                                          contains "Comedy").count
val percentage2013 = 100.0 * comedy2013 / total2013</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here is our final solution! We only added caching for the three RDDs
to make our code run faster.</p></div>
<div class="paragraph"><p>As a final note, if you execute this code in your application and point your
browser to the Spark Application UI at port 4040, you will be able to see the
four stages of computation and storage information for persisted RDDs.
Have fun!</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-key-value-pairs-pair-rdd">
<h2>What is a Key-Value Pair RDD?</h2>
<div class="ulist">
<ul>
<li><p>
Any RDD whose elements are key-value pairs<div class="ulist">
<ul>
<li>Key-value pair is a tuple with two components: (<em>key</em>, <em>value</em>)</li>
<li>Different pairs may have the same keys</li>
<li>Both keys and values can be of primitive or complex data types</li>
</ul>
</div></p></li>
<li>Examples:</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val users = sc.parallelize(List( ("Alice",21), ("Bob",12), ("Bob",18) ))
// users: org.apache.spark.rdd.RDD[(String, Int)]

val movies = sc.cassandraTable("killr_video","movies")
               .keyBy( row =&gt; (row.getString("title"), row.getInt("release_year")) )
// movies: org.apache.spark.rdd.RDD[((String, Int), com.datastax.spark.connector.CassandraRow)]</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The first example: A Pair RDD <em>users</em> is created by parallelizing a collection with three
key-value pairs. Notice that key "Bob" is repeated twice. The resulting RDD
contains keys of type <em>String</em> and values of type <em>Int</em>.</p></div>
<div class="paragraph"><p>The second example: A Pair RDD <em>movies</em> is created by retrieving data from a
Cassandra table and applying <em>Spark-Cassandra Connector</em> transformation <em>keyBy</em>
to organize each element as a key-value pair. Each key is a <em>Tuple</em>
containing movie <em>title</em> (<em>String</em>) and <em>release_year</em> (<em>Int</em>). Each value is
a <em>CassandraRow</em> object containing all columns, including <em>title</em> and <em>release_year</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="operations-on-key-value-pair-rdds">
<h2>Operations on Key-Value Pair RDDs</h2>
<div class="paragraph"><p><strong>Key-value pair semantics enables a number of important operations</strong></p></div>
<div class="ulist">
<ul>
<li><p>
Key-based operations<div class="ulist">
<ul>
<li>Aggregation</li>
<li>Grouping and sorting</li>
<li>Inner and outer joins</li>
<li>Union, intersection, difference</li>
<li>Other "supporting" operations</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>Pair RDDs also support operations defined for generic RDDs, such as <em>filter</em>, <em>map</em>, <em>count</em>, and so forth.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In this presentation, we will focus on "supporting" transformations and actions.
We call them "supporting" because they can be helpful and convenient to use with
Pair RDDs; yet they are not primary operations Pair RDDs exist for (like aggregation, joins, etc.).</p></div>
</div>
</div>
</section>
<section class="slide" id="supporting-transformations-for-pair-rdds">
<h2>"Supporting" Transformations for Pair RDDs</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>keys</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD is formed by keys of the source RDD.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>values</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD is formed by values of the source RDD.</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
Both <em>keys</em>() and <em>values</em>() can be easily implemented using <em>map</em>(<em>f</em>).
</td>
</tr>
</table>
</div>
<div class="imageblock center">
<div class="content">
<img alt="keys" src="images/spark/key-value-pairs/pair-rdd/keys.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>pairRDD.keys</em> is equivalent to <em>pairRDD.map{case (k,v) &#8658; k}</em></p></div>
<div class="paragraph"><p><em>pairRDD.values</em> is equivalent to <em>pairRDD.map{case (k,v) &#8658; v}</em></p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>mapValues</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD is formed by applying a function <em>f</em> on each value of
the source RDD. Keys are retained without changes, which
implies that any key-based partitioning of the source is also retained.
There is a <em>one-to-one</em> correspondence between input and output elements.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>flatMapValues</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Same as above except
there is a <em>one-to-many</em> correspondence between input and output elements
if <em>f</em> returns a <em>Seq</em> with more than one element.</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="icon-warning" title="Warning"></i>
</td>
<td class="content">
It is a bad idea to implement these transformations using <em>map</em>(<em>f</em>).
</td>
</tr>
</table>
</div>
<div class="imageblock center">
<div class="content">
<img alt="map values" src="images/spark/key-value-pairs/pair-rdd/map-values.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>It is a bad idea to implement these transformations using <em>map</em>. Here is why.</p></div>
<div class="paragraph"><p>When applying <em>map</em>(<em>f</em>), Spark assumes that function <em>f</em> changes
not only values but also keys (Spark does not analyze your function code).
If an input RDD has all pairs with the same key
in the same partition, an output RDD is assumed to not retain this property.
Therefore, Spark may have to reorganized data into new partitions for subsequent operations&#8201;&#8212;&#8201;this process is called <em>shuffling</em> and is expensive.</p></div>
<div class="paragraph"><p>With <em>mapValues</em> and <em>flatMapValues</em>, Spark knows that keys cannot be affected by the transformations.</p></div>
</div>
</div>
</section>
<section class="slide" id="supporting-actions-for-pair-rdds">
<h2>"Supporting" Actions for Pair RDDs</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>lookup</strong>(<em>key</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns a <em>Seq</em> of values in the source RDD for a given key.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>collectAsMap</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns a <em>Map</em> of key-value pairs in the source RDD.</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="icon-warning" title="Warning"></i>
</td>
<td class="content">
Only use <em>collectAsMap()</em> when an RDD does not contain multiple pairs with the same key.
</td>
</tr>
</table>
</div>
<div class="imageblock center">
<div class="content">
<img alt="lookup" src="images/spark/key-value-pairs/pair-rdd/lookup.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Only use <em>collectAsMap()</em> when an RDD does not contain multiple pairs with the same key.
Otherwise, you may "lose" data in the resulting <em>Map</em>, which can only contain unique keys.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-3">
<h2>Example</h2>
<div class="paragraph"><p><strong>Find ratings of movies released in 2014</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/key-value-pairs/pair-rdd/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable[(Int,Option[Float])]("killr_video","movies")
  .select("release_year","rating")
  .mapValues(v =&gt; v.getOrElse(0.0))
  .lookup(2014)
  .foreach(println)

// Sample output:
// 6.3
// 6.0
// 5.4</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This example should be straightforward!</p></div>
<div class="paragraph"><p>Notice how we obtained a pair RDD in this example by specifying how data should be retrieved from
Cassandra and converted into tuples of the form <em>(Int,Option[Float])</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-key-value-pairs-pair-rdd-aggregation">
<h2>Computing Per-Key Aggregates</h2>
<div class="ulist">
<ul>
<li><p>
Aggregating values with the same key is a common task<div class="ulist">
<ul>
<li>Statistical analysis, summarization</li>
<li>The <em>WordCount</em> problem is classic</li>
</ul>
</div></p></li>
<li><p>
<em>KillrVideo</em> challenges for this presentation<div class="ulist">
<ul>
<li>Count how many movies featuring Johnny Depp were released per year</li>
<li>Find the highest rated movie featuring Johnny Depp for each year</li>
<li>Compute an average rating of movies featuring Johnny Depp for every year</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>We are not going to cover all operations for per-key aggregation that are available
in Spark. Instead, we will look at the four most commonly used operations with detailed examples.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformation-em-reducebykey-em">
<h2>Transformation <em>reduceByKey</em></h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>reduceByKey</strong>(<em>f</em>, [<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD of (<em>K</em>,<em>V</em>)  pairs is formed by aggregating values for each key
in the source RDD of (<em>K</em>,<em>V</em>) pairs. The reduce function <em>f</em>: <em>V x V &#8594; V</em>
takes two values of type <em>V</em> and returns a new value of type <em>V</em>. The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
</tbody>
</table>
<div class="imageblock center">
<div class="content">
<img alt="reduce by key" src="images/spark/key-value-pairs/pair-rdd-aggregation/reduce-by-key.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The simplest transformation available. Note that the resulting RDD
has the same type as the source RDD.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Count how many movies featuring Johnny Depp were released per year</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/key-value-pairs/pair-rdd-aggregation/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable("killr_video","movies_by_actor")
  .where("actor = 'Johnny Depp'")
  .select("release_year")
  .as( (year:Int) =&gt; (year,1) )
  .reduceByKey(_ + _)
  .collect
  .foreach(println)

// Sample output:
// (2010,2)
// (2000,3)
// (2014,3)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The first challenge solution.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformation-em-foldbykey-em">
<h2>Transformation <em>foldByKey</em></h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>foldByKey</strong>(<em>zeroValue</em>, [<em>numTasks</em>])(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD of (<em>K</em>,<em>V</em>)  pairs is formed by aggregating values for each key
in the source RDD of (<em>K</em>,<em>V</em>) pairs. The <em>zeroValue</em> parameter is
a neutral value, which can be "added" to the result an arbitrary number of times without affecting it
(e.g., <em>Nil</em> for list concatenation, <em>0</em> for addition, or <em>1</em> for multiplication).
The associative function <em>f</em>: <em>V x V &#8594; V</em>
takes a partially aggregated result and a value from the source RDD and returns a new result.
The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
</tbody>
</table>
<div class="imageblock center">
<div class="content">
<img alt="fold by key" src="images/spark/key-value-pairs/pair-rdd-aggregation/fold-by-key.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This transformation is simpler than it looks like!
Conceptually, think about this transformation as if you take one value at a time and
"merge" it with a partially aggregated result. The value and the aggregated result must be of the
same type.</p></div>
<div class="paragraph"><p>See transformation <em>aggregateByKey</em> (not covered here) if you need an aggregated result type to be different from an input value type.
Alternatively, the next transformation <em>combineByKey</em> in this presentation will do it, too.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Find the highest rated movie featuring Johnny Depp for each year</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/key-value-pairs/pair-rdd-aggregation/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable("killr_video","movies_by_actor")
  .where("actor = 'Johnny Depp'")
  .select("release_year","title","rating")
  .as( (y:Int,t:String,r:Option[Float]) =&gt; (y,(t,r)) )
  .filter{case (y,(t,r)) =&gt; r.isDefined}
  .mapValues{case (t,r) =&gt; (t,r.get)}
  .foldByKey( ("",0.0f) ){ case ((maxT,maxR),(t,r)) =&gt;
                               if (maxR &lt; r) (t,r)
                               else (maxT,maxR) }
  .collect.foreach(println)

// Sample output:
// (2010,(Alice in Wonderland,6.5))
// (2000,(Before Night Falls,7.3))
// (2014,(Transcendence,6.3))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The second challenge solution.</p></div>
<div class="paragraph"><p>Note that values in the source RDD are pairs themselves, each one consisting of movie <em>title</em> and <em>rating</em>.</p></div>
<div class="paragraph"><p><em>zeroValue</em> = ("",0.0f); empty <em>title</em> and zero rating. The <em>foldByKey</em> argument function
compares a movie with the currently largest rating and a new movie seen in the dataset, and returns a winning movie.
Not much different from finding the largest element in a list.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformation-em-combinebykey-em">
<h2>Transformation <em>combineByKey</em></h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>combineByKey</strong>(
<em>createCombinerF</em>,
<em>mergeValueF</em>,
<em>mergeCombinersF</em>,
[<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><div><div class="paragraph"><p>A new RDD of (<em>K</em>,<em>C</em>)  pairs is formed by aggregating values for each key
in the source RDD of (<em>K</em>,<em>V</em>) pairs. Aggregation is guided by three functions:</p></div>
<div class="olist arabic">
<ol class="arabic">
<li>
<em>createCombinerF</em>: <em>V &#8594; C</em> returns an aggregate value (combiner) of a desired type from a single value;
</li>
<li>
<em>mergeValueF</em>: <em>C x V &#8594; C</em> returns a new aggregate value (combiner) for a key by merging
the current aggregate value (combiner) for this key with a new value;
</li>
<li>
<em>mergeCombinersF</em>: <em>C x C &#8594; C</em> returns a new aggregate value (combiner)  for a key from
two partial aggregates (combiners) for this key.
</li>
</ol>
</div>
<div class="paragraph"><p>The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></div></div></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Most general transformation for per-key aggregation! The aggregate data type <em>C</em> can be different from
the value data type <em>V</em>. Very powerful transformation but also a bit more complex.</p></div>
<div class="paragraph"><p>This transformation can solve all our challenges if we want to.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Compute an average rating of movies featuring Johnny Depp for every year</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/key-value-pairs/pair-rdd-aggregation/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable[(Int,Option[Float])]("killr_video","movies_by_actor")
  .where("actor = 'Johnny Depp'").select("release_year","rating")
  .filter(_._2.isDefined).mapValues(r =&gt; r.get)
  .combineByKey(
   (rating:Float)                     =&gt;(rating, 1),
   (res:(Float,Int),rating:Float)     =&gt;(res._1 + rating, res._2 + 1),
   (res1:(Float,Int),res2:(Float,Int))=&gt;(res1._1 + res2._1, res1._2 + res2._2)
  )
  .mapValues{case (sum,count) =&gt; val avg = sum / count; f"$avg%1.1f"}
  .collect.foreach(println)

// Sample output:
// (2010,6.3)
// (2000,6.9)
// (2014,5.9)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The third challenge solution.</p></div>
<div class="paragraph"><p>The crux of this example is transformation <em>combineByKey</em> with three anonymous functions passed to it.</p></div>
<div class="paragraph"><p>The first function:
A value represented by <em>rating</em> in the source RDD is mapped to a tuple holding (<em>rating</em>, 1); this combiner holds
the current sum of ratings (= rating itself) and a number of ratings (= 1).</p></div>
<div class="paragraph"><p>The second function:
Merges a combiner with a new value to get a new combiner. In our case, we need to add a rating to the sum and
+1 to the count.</p></div>
<div class="paragraph"><p>The third function:
Merges two combiners by adding their sums and counts, respectively.</p></div>
<div class="paragraph"><p>The actual average is computed by the next transformation <em>mapValues</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="action-em-countbykey-em">
<h2>Action <em>countByKey</em></h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Action</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>countByKey</strong>([<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns a <em>Map</em> of (<em>K</em>,<em>N</em>) pairs, where <em>N</em> is the number of elements for each key in the
source RDD of (<em>K</em>,<em>V</em>) pairs.</p></td>
</tr>
</tbody>
</table>
<div class="imageblock center">
<div class="content">
<img alt="count by key" src="images/spark/key-value-pairs/pair-rdd-aggregation/count-by-key.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>As simple as it gets.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Count how many movies featuring Johnny Depp were released per year</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/key-value-pairs/pair-rdd-aggregation/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable("killr_video","movies_by_actor")
  .where("actor = 'Johnny Depp'")
  .select("release_year")
  .as( (year:Int) =&gt; (year,1) )
  .countByKey
  .foreach(println)

// Sample output:
// (2010,2)
// (2000,3)
// (2014,3)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Another way to solve the first challenge.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-key-value-pairs-pair-rdd-grouping-sorting">
<h2>Key-Based Grouping and Sorting</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:50%" />
<col style="width:50%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Grouping values with the same key</th>
<th class="tableblock halign-left valign-top">Sorting values using keys</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>Reorganizing data by a new key</li>
<li>Post-processing per-key groups</li>
</ul>
</div></div></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>Generating special-purpose datasets</li>
<li>Generating reports that require ordering</li>
</ul>
</div></div></td>
</tr>
</tbody>
</table>
<div class="ulist">
<ul>
<li><p>
<em>KillrVideo</em> challenges for this presentation<div class="ulist">
<ul>
<li>Output movies featuring Johnny Depp grouped by genre</li>
<li>Output movies with Johnny Depp and movies with Tom Hanks co-grouped by year</li>
<li>Output movies from 2010s featuring Johnny Depp ordered by rating</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>We are not going to cover all operations for key-based grouping and sorting that are available
in Spark. Instead, we will look at a few common transformations with detailed examples.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformation-em-groupbykey-em">
<h2>Transformation <em>groupByKey</em></h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>groupByKey</strong>([<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD of (<em>K</em>,<em>Iterable&lt;V&gt;</em>) pairs is formed by grouping values for each key
in the source RDD of (<em>K</em>,<em>V</em>) pairs. The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
</tbody>
</table>
<div class="imageblock center">
<div class="content">
<img alt="group by key" src="images/spark/key-value-pairs/pair-rdd-grouping-sorting/group-by-key.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">

</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Output movies featuring Johnny Depp grouped by genre</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/key-value-pairs/pair-rdd-grouping-sorting/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable[(String,Int,Set[String])]("killr_video","movies_by_actor")
  .where("actor = 'Johnny Depp'")
  .select("title","release_year","genres")
  .flatMap{case (t,y,gs) =&gt; gs.map( g =&gt;(g, t + ", " + y) )}
  .groupByKey()
  .collect
  .foreach(println)

// Sample output for one group:
// (Family,CompactBuffer(
//         Alice Through the Looking Glass, 2016,
//         Alice in Wonderland, 2010,
//         Charlie and the Chocolate Factory, 2005,
//         Finding Neverland, 2004))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The first challenge solution.</p></div>
<div class="paragraph"><p>Note that we use the <em>flatMap</em> transformation to generate key-value pairs this time.
Inside <em>flatMap</em>, <em>map</em> is not a Spark transformation but rather a method in the Scala Set API.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformations-em-cogroup-em-and-em-groupwith-em">
<h2>Transformations <em>cogroup</em> and <em>groupWith</em></h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>cogroup</strong>(<em>otherRDD</em>, [<em>numTasks</em>])
or <strong>groupWith</strong>(<em>otherRDD</em>, [<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD of (<em>K</em>,(<em>Iterable&lt;V&gt;</em>, <em>Iterable&lt;W&gt;</em>)) pairs is formed by grouping values for each key
from the source RDD of (<em>K</em>,<em>V</em>) pairs and the <em>otherRDD</em> of (<em>K</em>,<em>W</em>) pairs.
The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.
Both <em>cogroup</em> and <em>groupWith</em> refer to the same transformation.</p></td>
</tr>
</tbody>
</table>
<div class="imageblock center">
<div class="content">
<img alt="cogroup" src="images/spark/key-value-pairs/pair-rdd-grouping-sorting/cogroup.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">

</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Output movies with Johnny Depp and movies with Tom Hanks co-grouped by year</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/key-value-pairs/pair-rdd-grouping-sorting/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val johnnyMovies = sc.cassandraTable("killr_video","movies_by_actor")
                     .where("actor = 'Johnny Depp'")
                     .keyBy(row =&gt; row.getInt("release_year"))
val tomMovies = sc.cassandraTable("killr_video","movies_by_actor")
                     .where("actor = 'Tom Hanks'")
                     .keyBy(row =&gt; row.getInt("release_year"))
johnnyMovies.cogroup(tomMovies)
            .collect.foreach(println)

// Sample output for one group:
// (2010,CompactBuffer(
//         CassandraRow{actor: Johnny Depp, ..., title: The Tourist},
//         CassandraRow{actor: Johnny Depp, ..., title: Alice in Wonderland})
//       CompactBuffer(
//         CassandraRow{actor: Tom Hanks, ..., title: Toy Story 3}))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The second challenge solution.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformation-em-sortbykey-em">
<h2>Transformation <em>sortByKey</em></h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>sortByKey</strong>(
[<em>ascending</em>],
[<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD of (<em>K</em>,<em>V</em>) pairs is formed by sorting pairs in the source RDD of (<em>K</em>,<em>V</em>) pairs
based on keys in ascending (default) or descending order. The <em>K</em> type must implement trait <em>Ordered</em>.
The optional <em>ascending</em> parameter has the default value of <em>true</em>.
The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
</tbody>
</table>
<div class="imageblock center">
<div class="content">
<img alt="sort by key" src="images/spark/key-value-pairs/pair-rdd-grouping-sorting/sort-by-key.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">

</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Output movies from 2010s featuring Johnny Depp ordered by rating</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/key-value-pairs/pair-rdd-grouping-sorting/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable("killr_video","movies_by_actor")
  .where("actor = 'Johnny Depp' AND release_year &gt; 2010")
  .select("title","release_year","rating")
  .as( (t:String, y:Int, r:Option[Float]) =&gt; (r.getOrElse(0.0f),(t,y)) )
  .sortByKey(false)
  .collect
  .foreach(println)

// Sample output:
// (7.3,(Rango,2011))
// (6.7,(Pirates of the Caribbean: On Stranger Tides,2011))
// (6.5,(The Lone Ranger,2013))
// (6.3,(Transcendence,2014))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The third challenge solution.</p></div>
</div>
</div>
</section>
<section class="slide" id="grouping-and-sorting-best-practices">
<h2>Grouping and Sorting Best Practices</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:50%" />
<col style="width:50%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Grouping</th>
<th class="tableblock halign-left valign-top">Sorting</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>Expensive on large datasets</li>
<li>Do not use grouping for aggregation or joins</li>
<li>Grouping transformations may result in large key-value pairs</li>
</ul>
</div></div></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>Expensive on large datasets</li>
<li>Prefer smaller datasets</li>
<li>Prefer Cassandra clustering column ordering to sorting</li>
</ul>
</div></div></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Both are expensive because they require shuffling and do not reduce the size of a dataset</p></div>
<div class="paragraph"><p>Do not use grouping for aggregation or joins. Use aggregation/join transformations instead, because they are more efficient for the purpose.</p></div>
<div class="paragraph"><p>Grouping transformations may result in large key-value pairs.
It is important to remember that any given pair must fit into memory.
An RDD can spill to disk across keys but a key-value pair cannot.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-key-value-pairs-pair-rdd-joins">
<h2>Key-Based Joins</h2>
<div class="ulist">
<ul>
<li><p>
Joins are some of the most useful operations on Pair RDDs<div class="ulist">
<ul>
<li>Combining values from two or more Pair RDDs based on key equality</li>
<li>Evolving schema, validating data, generating new datasets</li>
<li>Generally expensive operations</li>
</ul>
</div></p></li>
<li><p>
<em>KillrVideo</em> challenges for this presentation<div class="ulist">
<ul>
<li>Perform schema evolution for table <em>playlists_by_user</em></li>
<li>Validate referential integrity constraints for table <em>playlists_by_user</em></li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Joins are generally expensive operations because they may deal with large
datasets, may return large result sets, and require data shuffling (and therefore, disk access).
No matter how efficient Spark is, joins are not to be used for processing day-to-day
user transactions in an operational data center. Instead, data should be modeled in Cassandra
such that joins are not required.
Main use cases include database schema evolution, periodic validation of data stored in Cassandra, and
generating new datasets for further, more expensive analysis in an analytical data center.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformation-em-join-em">
<h2>Transformation <em>join</em></h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>join</strong>(<em>otherRDD</em>,[<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD of (<em>K</em>,(<em>V</em>, <em>W</em>)) pairs is formed by combining all possible values for each key
from the source RDD of (<em>K</em>,<em>V</em>) pairs and the <em>otherRDD</em> of (<em>K</em>,<em>W</em>) pairs.
The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
</tbody>
</table>
<div class="imageblock center">
<div class="content">
<img alt="join" src="images/spark/key-value-pairs/pair-rdd-joins/join.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This is a classic inner join.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformation-em-leftouterjoin-em">
<h2>Transformation <em>leftOuterJoin</em></h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>leftOuterJoin</strong>(<em>otherRDD</em>,[<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><div><div class="paragraph"><p>A new RDD of (<em>K</em>,(<em>V</em>, <em>Option</em>[<em>W</em>])) pairs is formed by combining values for each key
from the source RDD of (<em>K</em>,<em>V</em>) pairs and the <em>otherRDD</em> of (<em>K</em>,<em>W</em>) pairs as:</p></div>
<div class="ulist">
<ul>
<li>(<em>K</em>,(<em>V</em>, <em>Some</em>[<em>W</em>])) pairs if a key exists in both RDDs;</li>
<li>(<em>K</em>,(<em>V</em>, <em>None</em>)) pairs if a key only exists in the source RDD.</li>
</ul>
</div>
<div class="paragraph"><p>The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></div></div></td>
</tr>
</tbody>
</table>
<div class="imageblock center">
<div class="content">
<img alt="left outer join" src="images/spark/key-value-pairs/pair-rdd-joins/left-outer-join.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Useful to, for example, generate a report about users and their uploaded movies
when the report should also contain users who never uploaded any movie.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformation-em-rightouterjoin-em">
<h2>Transformation <em>rightOuterJoin</em></h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>rightOuterJoin</strong>(<em>otherRDD</em>,[<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><div><div class="paragraph"><p>A new RDD of (<em>K</em>,(<em>Option</em>[<em>V</em>],<em>W</em>)) pairs is formed by combining values for each key
from the source RDD of (<em>K</em>,<em>V</em>) pairs and the <em>otherRDD</em> of (<em>K</em>,<em>W</em>) pairs as:</p></div>
<div class="ulist">
<ul>
<li>(<em>K</em>,(<em>Some</em>[<em>V</em>], <em>W</em>)) pairs if a key exists in both RDDs;</li>
<li>(<em>K</em>,(<em>None</em>, <em>W</em>)) pairs if a key only exists in <em>otherRDD</em>.</li>
</ul>
</div>
<div class="paragraph"><p>The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></div></div></td>
</tr>
</tbody>
</table>
<div class="imageblock center">
<div class="content">
<img alt="right outer join" src="images/spark/key-value-pairs/pair-rdd-joins/right-outer-join.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Similar to the left outer join but the direction is now switched.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformation-em-fullouterjoin-em">
<h2>Transformation <em>fullOuterJoin</em></h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>fullOuterJoin</strong>(<em>otherRDD</em>,[<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><div><div class="paragraph"><p>A new RDD of (<em>K</em>,(<em>Option</em>[<em>V</em>],<em>Option</em>[<em>W</em>])) pairs is formed by combining values for each key
from the source RDD of (<em>K</em>,<em>V</em>) pairs and the <em>otherRDD</em> of (<em>K</em>,<em>W</em>) pairs as:</p></div>
<div class="ulist">
<ul>
<li>(<em>K</em>,(<em>Some</em>[<em>V</em>], <em>Some</em>[<em>W</em>])) pairs if a key exists in both RDDs;</li>
<li>(<em>K</em>,(<em>Some</em>[<em>V</em>], <em>None</em>)) pairs if a key only exists in the source RDD;</li>
<li>(<em>K</em>,(<em>None</em>, <em>Some</em>[<em>W</em>])) pairs if a key only exists in <em>otherRDD</em>.</li>
</ul>
</div>
<div class="paragraph"><p>The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></div></div></td>
</tr>
</tbody>
</table>
<div class="imageblock center">
<div class="content">
<img alt="full outer join" src="images/spark/key-value-pairs/pair-rdd-joins/full-outer-join.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The outer join is the most expensive join as it returns the largest result set.</p></div>
</div>
</div>
</section>
<section class="slide" id="challenge-1-schema-evolution">
<h2>Challenge 1: Schema Evolution</h2>
<div class="paragraph"><p><strong>Step 1: Adding two new columns to <em>playlists_by_user</em></strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>ALTER TABLE playlists_by_user ADD genres SET&lt;TEXT&gt;;

ALTER TABLE playlists_by_user ADD rating FLOAT;</code></pre>
</div>
</div>
<div class="imageblock center">
<div class="content">
<img alt="playlists by user" src="images/spark/key-value-pairs/pair-rdd-joins/playlists_by_user.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Schema evolution is common when application or query requirements change.
To avoid joins between tables when processing user requests, you should rather add all
necessary information into one table.</p></div>
<div class="paragraph"><p>In Step 1, we are adding two columns to the table definition, which will update table metadata.
Data is unchanged and <em>null</em> values will be returned for the new columns in every row.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Step 2: Adding information about movie genres and ratings</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="challenge1" src="images/spark/key-value-pairs/pair-rdd-joins/challenge1.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val playlists =
    sc.cassandraTable("killr_video","playlists_by_user")
      .select("user_id","playlist_name","release_year","title","movie_id")
      .as((u:java.util.UUID,p:String,y:Int,t:String,m:java.util.UUID) =&gt;
          (m,(u,p,y,t)))

val movies =
    sc.cassandraTable("killr_video","movies")
      .select("movie_id","genres","rating")
      .as((m:java.util.UUID,g:Set[String],r:Option[Float]) =&gt;
          (m,(g,r)))

playlists.join(movies)
         .map{case (m,((u,p,y,t),(g,r))) =&gt; (u,p,y,t,m,g,r)}
         .saveToCassandra("killr_video","playlists_by_user")</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Step 2 takes care of inserting values for the new columns in table <em>playlists_by_user</em>
by retrieving them from table <em>movies</em>. The join is performed to match rows with the same movies
from both tables.</p></div>
</div>
</div>
</section>
<section class="slide" id="challenge-2-data-validation">
<h2>Challenge 2: Data Validation</h2>
<div class="paragraph"><p><strong>Do playlists reference non-existing movies?</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="challenge2" src="images/spark/key-value-pairs/pair-rdd-joins/challenge2.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val playlists =
    sc.cassandraTable("killr_video","playlists_by_user")
      .keyBy(row =&gt; row.getUUID("movie_id"))

val movies =
    sc.cassandraTable("killr_video","movies")
      .select("movie_id")
      .keyBy(row =&gt; row.getUUID("movie_id"))

playlists.leftOuterJoin(movies)
         .filter{case (m,(rowP,rowM)) =&gt; !rowM.isDefined}
         .map{case (m,(rowP,rowM)) =&gt; rowP}
         .collect.foreach(println)

// Sample output:
// CassandraRow{user_id: 709e42f0-5f25-4551-9d85-6e3ad39d6cde,
//              playlist_name: Pirate Movies,
//              release_year: 2017,
//              title: Pirates of DataStax, ...}</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Periodic data validation is important in Cassandra. For example, data
may be duplicated in multiple tables and we want to make sure that all
copies are the same. Or a column in one table may reference a key in another
table and we want to make sure that
we always reference an existing value (aka <em>referential integrity constraint</em>).</p></div>
<div class="paragraph"><p>In this example, we are validating that any movie in table <em>playlists_by_user</em> is
also present in table <em>movies</em>. We are finding violations by using a left outer join
and outputing those Cassandra rows from <em>playlists_by_user</em> that do not have a matching
movie in table <em>movies</em> (such as movie "Pirates of DatStax"!).</p></div>
<div class="paragraph"><p>Note that this is not the only possible solution for this challenge. Another, potentially
more efficient approach would be computing a set-difference between two sets of movies.
This is something we will explore in another presentation that covers transformation <em>subtractByKey</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-key-value-pairs-pair-rdd-set-operations">
<h2>Union, Intersection, and Difference</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:11%" />
<col style="width:22%" />
<col style="width:33%" />
<col style="width:33%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Operation</th>
<th class="tableblock halign-left valign-top">Venn Diagram</th>
<th class="tableblock halign-left valign-top">Generic RDD API</th>
<th class="tableblock halign-left valign-top">Key-Value Pair RDD API</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Union</strong></p></td>
<td class="tableblock halign-left valign-top"><div><div class="imageblock center">
<div class="content">
<img alt="union venn" src="images/spark/key-value-pairs/pair-rdd-set-operations/union-venn.svg" />
</div>
</div></div></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li><em>union</em>(<em>otherRDD</em>)</li>
<li>Duplicates: Yes</li>
</ul>
</div></div></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>N/A</li>
<li>Can be implemented</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Intersection</strong></p></td>
<td class="tableblock halign-left valign-top"><div><div class="imageblock center">
<div class="content">
<img alt="intersection venn" src="images/spark/key-value-pairs/pair-rdd-set-operations/intersection-venn.svg" />
</div>
</div></div></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li><em>intersection</em>(<em>otherRDD</em>)</li>
<li>Duplicates: No</li>
</ul>
</div></div></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>N/A</li>
<li>Can be implemented</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Difference</strong></p></td>
<td class="tableblock halign-left valign-top"><div><div class="imageblock center">
<div class="content">
<img alt="difference venn" src="images/spark/key-value-pairs/pair-rdd-set-operations/difference-venn.svg" />
</div>
</div></div></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li><em>subtract</em>(<em>otherRDD</em>)</li>
<li>Duplicates: Yes</li>
</ul>
</div></div></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li><em>subtractByKey</em>(<em>otherRDD</em>)</li>
<li>Duplicates: Yes</li>
</ul>
</div></div></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark set operations, unlike the mathematical set operations, do not always eliminate
duplicates in the result. Duplicate elimination can be done with transformation <em>distinct</em>().</p></div>
</div>
</div>
</section>
<section class="slide" id="key-based-union">
<h2>Key-Based Union</h2>
<div class="paragraph"><p><strong>Sample implementation for <em>union-compatible</em> Pair RDDs</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val A = sc.parallelize(Array(("k1","v1"), ("k2","v2"), ("k1","v3"), ("k3","v4")))
val B = sc.parallelize(Array(("k1","w1"), ("k2","w2"), ("k2","w3"), ("k4","w4")))

A.union(B)</code></pre>
</div>
</div>
<div class="imageblock center">
<div class="content">
<img alt="union" src="images/spark/key-value-pairs/pair-rdd-set-operations/union.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>A key-based union can be readily implemented using transformation <em>union</em> for generic RDDs.</p></div>
<div class="paragraph"><p>Notice the union-campatibility requirement. This means that both Pair RDDs must have
key-value pairs of the same form and type, such as (K,V).</p></div>
<div class="paragraph"><p>Duplicates are allowed.</p></div>
</div>
</div>
</section>
<section class="slide" id="key-based-intersection">
<h2>Key-Based Intersection</h2>
<div class="paragraph"><p><strong>Sample implementation for <em>union-compatible</em> Pair RDDs</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val A = sc.parallelize(Array(("k1","v1"), ("k2","v2"), ("k1","v3"), ("k3","v4")))
val B = sc.parallelize(Array(("k1","w1"), ("k2","w2"), ("k2","w3"), ("k4","w4")))

A.groupByKey
 .join(B.groupByKey)
 .flatMapValues{case (aList,bList) =&gt; aList ++ bList}</code></pre>
</div>
</div>
<div class="imageblock center">
<div class="content">
<img alt="intersection" src="images/spark/key-value-pairs/pair-rdd-set-operations/intersection.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>A key-based intersection implementation is a bit more involved. <em>groupByKey</em> prepares data for joining, such that
each resulting RDD can only have one pair with any given key. This ensures that the <em>join</em> result also has only one
pair with any given key. <em>flatMapValues</em> concatenates lists of values from both <em>A</em> and <em>B</em> for each key
and generates key-value pairs in the format of input datasets.</p></div>
<div class="paragraph"><p>Notice the union-campatibility requirement. This means that both Pair RDDs must have
key-value pairs of the same form and type, such as (K,V).</p></div>
<div class="paragraph"><p>Duplicates are allowed.</p></div>
</div>
</div>
</section>
<section class="slide" id="key-based-difference">
<h2>Key-Based Difference</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>subtractByKey</strong>(<em>otherRDD</em>,[<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD of (<em>K</em>,<em>V</em>) pairs is formed by those pairs
from the source RDD of (<em>K</em>,<em>V</em>) pairs whose keys are not present in the <em>otherRDD</em> of (<em>K</em>,<em>W</em>) pairs.
The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
</tbody>
</table>
<div class="imageblock center">
<div class="content">
<img alt="difference" src="images/spark/key-value-pairs/pair-rdd-set-operations/difference.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Input RDDs must have keys of the same type (e.g., <em>K</em>) but values may be of different types (e.g., <em>V</em> and <em>W</em>)</p></div>
<div class="paragraph"><p>Duplicates are allowed.</p></div>
</div>
</div>
</section>
<section class="slide" id="challenge-data-validation">
<h2>Challenge: Data Validation</h2>
<div class="paragraph"><p><strong>Do playlists reference non-existing movies?</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="challenge" src="images/spark/key-value-pairs/pair-rdd-set-operations/challenge.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val playlists =
    sc.cassandraTable("killr_video","playlists_by_user")
      .keyBy(row =&gt; row.getUUID("movie_id"))

val movies =
    sc.cassandraTable("killr_video","movies")
      .select("movie_id")
      .keyBy(row =&gt; row.getUUID("movie_id"))

playlists.subtractByKey(movies)
         .collect.foreach(println)

// Sample output:
// CassandraRow{user_id: 709e42f0-5f25-4551-9d85-6e3ad39d6cde,
//              playlist_name: Pirate Movies,
//              release_year: 2017,
//              title: Pirates of DataStax, ...}</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Periodic data validation is important in Cassandra. For example, data
may be duplicated in multiple tables and we want to make sure that all
copies are the same. Or a column in one table may reference a key in another
table and we want to make sure that
we always reference an existing value (aka <em>referential integrity constraint</em>).</p></div>
<div class="paragraph"><p>In this example, we are validating that any movie in table <em>playlists_by_user</em> is
also present in table <em>movies</em>. We are finding violations by computing a difference
and outputing those Cassandra rows from <em>playlists_by_user</em> that do not have a matching
movie in table <em>movies</em> (such as movie "Pirates of DatStax"!).</p></div>
<div class="paragraph"><p>Note that you might have seen this challenge before. It was previously solved
using <em>leftOuterJoin</em>. The current solution uses <em>subtractByKey</em>, which should
result in a better performance.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-tuning-partitioning-understanding-partitioning">
<h2>An RDD is a Distributed Collection of Partitions</h2>
<div class="ulist">
<ul>
<li>Spark automatically partitions RDDs</li>
<li>Spark automatically distributes partitions among nodes</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="partitions" src="images/spark/tuning-partitioning/understanding-partitioning/partitions.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In this presentation, we will overview basics of Spark partitioning. Some details
will be omitted here for brevity but will be covered in subsequent presentations
when the time is right.</p></div>
</div>
</div>
</section>
<section class="slide" id="rdd-partitioning-properties">
<h2>RDD Partitioning Properties</h2>
<div class="paragraph"><p><strong>Number of partitions</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Property</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>partitions</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns an <em>Array</em> with all partition references for the source RDD.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>partitions.size</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns a number of partitions in the source RDD.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val movies = sc.parallelize(List(("Alice in Wonderland",2016), ("Alice Through the Looking Glass",2010), ...))
println(movies.partitions.size)
// Sample output: 3

val interactions = sc.cassandraTable("killr_video","video_interactions_by_user")
println(interactions.partitions.size)
// Sample output: 4</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To find how many partitions an RDD contains, you can access RDD&#8217;s property <em>partitions</em> and get its <em>size</em>.</p></div>
<div class="paragraph"><p>In this example, we output a number of partitions for two sample RDDs: <em>movies</em> (created by parallelizing a collection)
and <em>interactions</em> (created from  a Cassandra table). Notice that this code does not use any actions and
therefore actual partitions with data are never computed. Nevertheless, Spark has a plan on how to compute such partitions
and how many of them.</p></div>
<div class="paragraph"><p>The content of the <em>partitions</em> array encodes references to partitions and is unlikely to be useful for developing
Spark applications.
An element of the <em>partitions</em> array may look similar to
<em>org.apache.spark.rdd.ParallelCollectionPartition@735</em> or
<em>CassandraPartition(0,Set(/172.31.26.129),Vector(CqlTokenRange(token("user_id") &#8656; ?,WrappedArray(-3074457345618258603))),127)</em>.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Partitioner</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Property</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>partitioner</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns an <em>Option</em>[<em>Partitioner</em>] for the source RDD, where <em>Partitioner</em>, if any, can refer to
<em>HashPartitioner</em>, <em>RangePartitioner</em>, or a custom partitioner.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val movies = sc.parallelize(List(("Alice in Wonderland",2016), ("Alice Through the Looking Glass",2010), ...))
println(movies.partitioner)
// Sample output: None

val moviesByYear = movies.map{case (t,y) =&gt; (y,t)}.groupByKey
println(moviesByYear.partitioner)
// Sample output: Some(org.apache.spark.HashPartitioner@3)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>RDD&#8217;s <em>partitioner</em> property can give you an idea about a strategy that is used to create RDD partitions.
<em>HashPartitioner</em> is the most common strategy for key-based operations. <em>RangePartitioner</em> is used when sorting by key
is required. Custom partitioners are also possible. Finally, a <em>partitioner</em> can be <em>None</em>,
which simply means that
partitioning strategy is not based on data characteristics; for example, it is random and uniform.</p></div>
<div class="paragraph"><p>In this example, in the first case, <em>partitioner</em> is None and in the second case, when <em>groupByKey</em>
is used, <em>partitioner</em> is <em>HashPartitioner</em>. When retrieving data from Cassandra, <em>partitioner</em> would be <em>None</em>
because partitioning is controlled by Spark-Cassandra Connector rather than Spark.</p></div>
</div>
</div>
</section>
<section class="slide" id="factors-that-affect-partitioning">
<h2>Factors That Affect Partitioning</h2>
<div class="ulist">
<ul>
<li>Resources available to an application</li>
<li>External data sources</li>
<li>Transformations used to derive an RDD</li>
<li>Partitioning properties of parent RDD(s)</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="ulist">
<ul>
<li>Resources available to an application, such as a number of cores</li>
<li>External data sources, such as local collections, Cassandra tables, and HDFS files</li>
<li>Transformations used to derive an RDD, such as key-based transformations</li>
<li>Partitioning properties of parent RDD(s) that are used to derive an RDD</li>
</ul>
</div>
<div class="paragraph"><p>We will discuss these factors in more detail in a separate presentation.</p></div>
</div>
</div>
</section>
<section class="slide" id="partitioning-and-computation">
<h2>Partitioning and Computation</h2>
<div class="ulist">
<ul>
<li>Partition is the smallest unit of data</li>
<li>Task is the smallest unit of computation</li>
<li><em>Number of partitions</em> = <em>Number of tasks</em></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>It is important to understand the relationship between partitioning and computation.
A separate task is scheduled to perform computation on a partition. Therefore,
in the context of one operation, the number of tasks would be equivalent to the number
of partitions.</p></div>
</div>
</div>
<div class="imageblock center">
<div class="content">
<img alt="tasks" src="images/spark/tuning-partitioning/understanding-partitioning/tasks.svg" />
</div>
</div>
</section>
<section class="slide" id="partitioning-and-default-parallelism">
<h2>Partitioning and Default Parallelism</h2>
<div class="ulist">
<ul>
<li><p>
Default level of parallelism refers to a number of tasks that can be executed concurrently<div class="ulist">
<ul>
<li>Defined as a number of cores allocated to an application in a cluster</li>
<li>General recommendation: <em>Number of partitions</em> &gt;= <em>Default Parallelism</em></li>
</ul>
</div></p></li>
</ul>
</div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Property</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>defaultParallelism</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns a default level of parallelism of a <em>SparkContext</em> object.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>println(sc.defaultParallelism)
// Sample output: 3

val movies = sc.parallelize(List(("Alice in Wonderland",2016), ("Alice Through the Looking Glass",2010), ...))
println(movies.partitions.size)
// Sample output: 3</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Another very important and related notion is a default level of parallelism for an application
and its <em>SparkContext</em> object. The default level of parallelism refers to a number of tasks
that can be executed concurrently and is defined as a number of cores allocated to an application in a cluster.
The default level of parallelism is customizable for each application via the <em>spark.default.parallelism</em> property.</p></div>
<div class="paragraph"><p>In this example, we have 3 cores that are available to an application. You can see that <em>parallelize</em> creates
3 partitions according to the default level of parallelism.</p></div>
</div>
</div>
</section>
<section class="slide" id="controlling-partitioning">
<h2>Controlling Partitioning</h2>
<div class="paragraph"><p><strong>One of the most important performance optimizations in Spark</strong></p></div>
<div class="ulist">
<ul>
<li>Special transformations for repartitioning datasets with desired partitioning properties</li>
<li>Many transformations support an additional parameter for a desired number of tasks</li>
<li>Certain application settings affect partitioning</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>Choosing appropriate partitioning properties based on characteristics of your data and computation may
drastically improve performance. A good solution is usually found via experimenting with different parameters.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To achieve an optimal performance for your application, you will frequently need to control partitioning, including
a number of partitions to increase or decrease parallelism and a partitioner that can be reused by multiple operations.</p></div>
<div class="paragraph"><p>Spark supports special transformations for repartitioning datasets with desired partitioning properties. In addition,
many transformations support a parameter for a desired number of tasks. Certain application settings affect partitioning,
including partitions created by Spark-Cassandra Connector.</p></div>
<div class="paragraph"><p>We will discuss how and when to control partitioning in more detail in a separate presentation.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-tuning-partitioning-partitioning-rules">
<h2>Default Partitioning Behavior</h2>
<div class="paragraph"><p><strong>Scenarios we will talk about</strong></p></div>
<div class="ulist">
<ul>
<li><p>
Creating an RDD from an external data source<div class="ulist">
<ul>
<li>Local Scala collection</li>
<li>Cassandra table</li>
<li>HDFS/CFS file</li>
</ul>
</div></p></li>
<li><p>
Transforming an RDD into a new RDD<div class="ulist">
<ul>
<li>Generic transformations</li>
<li>Key-based transformations</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In this presentation, we will explore rules for the <strong>default</strong> partitioning behavior.
In a separate presentation, we will talk about changing the default behavior by controlling
partitioning explicitly.</p></div>
<div class="paragraph"><p>We need to discuss two main categories of rules: external data sources and transformations.</p></div>
</div>
</div>
</section>
<section class="slide" id="parallelizing-a-scala-collection">
<h2>Parallelizing a Scala Collection</h2>
<div class="paragraph"><p><strong>Default partitioning properties</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:28%" />
<col style="width:57%" />
<col style="width:14%" />
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock"><strong>API Call</strong></p></td>
<td class="tableblock halign-left valign-top" colspan="2"><p class="tableblock"><strong>Resulting RDD Partitioning Properties</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>partitions.size</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>partitioner</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>sc</em>.<em>parallelize</em>(&#8230;&#8203;)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>sc</em>.<em>defaultParallelism</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>None</em></p></td>
</tr>
</tbody>
</table>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>println(sc.defaultParallelism)
// Sample output: 3

val movies = sc.parallelize(List(("Alice in Wonderland",2016), ("Alice Through the Looking Glass",2010), ...))
println(movies.partitions.size)
// Sample output: 3

println(movies.partitioner)
// Sample output: None</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In this example, <em>defaultParallelism</em> equals to 3; that is how many cores in a cluster are available to our application.</p></div>
</div>
</div>
</section>
<section class="slide" id="retrieving-data-from-cassandra">
<h2>Retrieving Data From Cassandra</h2>
<div class="paragraph"><p><strong>Default partitioning properties</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:28%" />
<col style="width:57%" />
<col style="width:14%" />
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock"><strong>API Call</strong></p></td>
<td class="tableblock halign-left valign-top" colspan="2"><p class="tableblock"><strong>Resulting RDD Partitioning Properties</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>partitions.size</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>partitioner</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>sc</em>.<em>cassandraTable</em>(&#8230;&#8203;)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>sc</em>.<em>defaultParallelism</em> or
approximate-data-size / 64MBs, whichever is greater</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>None</em></p></td>
</tr>
</tbody>
</table>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>println(sc.defaultParallelism)
// Sample output: 3

val interactions = sc.cassandraTable("killr_video","video_interactions_by_user")
println(interactions.partitions.size)
// Sample output: 4

println(interactions.partitioner)
// Sample output: None</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In this example, table <em>video_interactions_by_user</em> contains around 256MBs of data and therefore,
the number of partitions = 256/64 = 4. A smaller table would give us <em>defaultParallelism</em> = 3 partitions.</p></div>
</div>
</div>
</section>
<section class="slide" id="reading-data-from-an-hdfs-cfs-file">
<h2>Reading Data From an HDFS/CFS File</h2>
<div class="paragraph"><p><strong>Default partitioning properties</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:28%" />
<col style="width:57%" />
<col style="width:14%" />
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock"><strong>API Call</strong></p></td>
<td class="tableblock halign-left valign-top" colspan="2"><p class="tableblock"><strong>Resulting RDD Partitioning Properties</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>partitions.size</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>partitioner</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>sc</em>.<em>textFile</em>(&#8230;&#8203;)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>sc</em>.<em>defaultParallelism</em> or
a number of file blocks, whichever is greater</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>None</em></p></td>
</tr>
</tbody>
</table>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>println(sc.defaultParallelism)
// Sample output: 3

val records = sc.textFile("cfs:///tmp/videos.csv")
println(records.partitions.size)
// Sample output: 3

println(records.partitioner)
// Sample output: None</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Cassandra File System (CFS) is an implementation of HDFS.
HDFS/CFS files consist of distributed file blocks with the default size of 64MBs.</p></div>
<div class="paragraph"><p>In this example, the CSV file must be less than or equal to roughly 3 * 64MBs = 192MBs because we are getting
3 partitions in the resulting RDD.</p></div>
</div>
</div>
</section>
<section class="slide" id="generic-transformations">
<h2>Generic Transformations</h2>
<div class="paragraph"><p><strong>Default partitioning properties</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:28%" />
<col style="width:57%" />
<col style="width:14%" />
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock"><strong>Transformation</strong></p></td>
<td class="tableblock halign-left valign-top" colspan="2"><p class="tableblock"><strong>Resulting RDD Partitioning Properties</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>partitions.size</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>partitioner</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>filter</em>(&#8230;&#8203;), <em>map</em>(&#8230;&#8203;), <em>flatMap</em>(&#8230;&#8203;),
<em>distinct</em>(), &#8230;&#8203;</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The same number of partitions as in the parent RDD</p></td>
<td class="tableblock halign-left valign-top" rowspan="5"><p class="tableblock"><em>None</em>, except <em>filter</em> preserves parent RDD&#8217;s <em>partitioner</em>, if any</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>rdd</em>.<em>union</em>(<em>otherRDD</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>rdd</em>.<em>partitions</em>.<em>size</em> + <em>otherRDD</em>.<em>partitions</em>.<em>size</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>rdd</em>.<em>intersection</em>(<em>otherRDD</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>max</em>(<em>rdd</em>.<em>partitions</em>.<em>size</em>, <em>otherRDD</em>.<em>partitions</em>.<em>size</em>)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>rdd</em>.<em>subtract</em>(<em>otherRDD</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>rdd</em>.<em>partitions</em>.<em>size</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>rdd</em>.<em>cartesian</em>(<em>otherRDD</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>rdd</em>.<em>partitions</em>.<em>size</em> * <em>otherRDD</em>.<em>partitions</em>.<em>size</em></p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Unary transformations result in the same number of partitions as in the source RDD.</p></div>
<div class="paragraph"><p>For binary transformations, the resulting number of partitions is different for each transformation.</p></div>
<div class="paragraph"><p>All these transformations result in <em>None</em> for <em>partitioner</em>,exept <em>filter</em> can preserve parent RDD&#8217;s
<em>partitioner</em>.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Example</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val movies = sc.cassandraTable("killr_video","movies")
// Partitioning properties: 3, None

val favoriteMovies = sc.cassandraTable("killr_video","favorite_movies")
// Partitioning properties: 3, None

val allMovies = movies.union(favoriteMovies)
// Partitioning properties: 6, None</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>3 + 3 = 6</p></div>
</div>
</div>
</section>
<section class="slide" id="key-based-transformations">
<h2>Key-Based Transformations</h2>
<div class="paragraph"><p><strong>Default partitioning properties</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:28%" />
<col style="width:57%" />
<col style="width:14%" />
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock"><strong>Transformation</strong></p></td>
<td class="tableblock halign-left valign-top" colspan="2"><p class="tableblock"><strong>Resulting RDD Partitioning Properties</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>partitions.size</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>partitioner</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>reduceByKey</em>(&#8230;&#8203;), <em>foldByKey</em>(&#8230;&#8203;),
<em>combineByKey</em>(&#8230;&#8203;), <em>groupByKey</em>(), &#8230;&#8203;</p></td>
<td class="tableblock halign-left valign-top" rowspan="3"><p class="tableblock">The same number of partitions as in the parent RDD</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>HashPartitioner</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>sortByKey</em>(&#8230;&#8203;)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>RangePartitioner</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>mapValues</em>(&#8230;&#8203;), <em>flatMapValues</em>(&#8230;&#8203;)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Parent RDD&#8217;s <em>partitioner</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>cogroup</em>(&#8230;&#8203;), <em>join</em>(&#8230;&#8203;), <em>leftOuterJoin</em>(&#8230;&#8203;), <em>rightOuterJoin</em>(&#8230;&#8203;), &#8230;&#8203;</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The same number of partitions as in the source RDD or the other RDD, depending on partitioning properties of the inputs.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>HashPartitioner</em></p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Unary transformations may result in different partitioners but the number of partitions is always the same
as in parent RDDs.</p></div>
<div class="paragraph"><p>For binary transformations, the number of partitions is determined by both the <em>partitioner</em> and <em>partitions.size</em>
properties of the input RDDs. We will discuss this rule in more detail soon.</p></div>
<div class="paragraph"><p>It is very important to understand that any transformation that sets a partitioner (whether <em>HashPartitioner</em>,
<em>RangePartitioner</em>, or custom partitioner) requires data shuffling. Data is reorganized into new partitions
using the partitioner. This requires disk I/O. In other words, such transformations are more expensive.
Whenever possible, partitioning from a previous operation should be reused to avoid shuffling. Spark can and will
reuse partitioning when possible but a developer should be careful not to break partitioning in her code.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Example - unary transformations</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val moviesByYear = sc.cassandraTable("killr_video","movies")
                     .keyBy(row =&gt; row.getInt("release_year"))
                     .groupByKey
// Partitioning properties: HashPartitioner@3

val moviesByYear2010 = moviesByYear.filter{case (y,rows) =&gt; y == 2010}
// Partitioning properties: HashPartitioner@3

val movies2010 = moviesByYear2010.flatMapValues(rows =&gt; rows)
// Partitioning properties: HashPartitioner@3

val capitalizedMovies2010 = movies2010.map{case (y,row) =&gt; (y,row.getString("title").capitalize)}
// Partitioning properties: 3, None</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Both <em>cassandraTable</em> and <em>keyBy</em> result in 3, <em>None</em>.</p></div>
<div class="paragraph"><p><em>groupByKey</em> sets the <em>partitioner</em> to <em>HashPartitioner</em> with 3 partitions.</p></div>
<div class="paragraph"><p><em>filter</em> and <em>flatMapValues</em> preserve partitioning properties.</p></div>
<div class="paragraph"><p><em>map</em> causes <em>partitioner</em> to become <em>None</em>, even though the transformation does not affect keys in this example.
Spark does not analyze our anonymous function code; it simply assumes that our code may have changed keys and therefore,
key-based hash partitioning is now broken. To avoid this situation, we should have used <em>mapValues</em> instead of <em>map</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="binary-key-based-transformations">
<h2>Binary Key-Based Transformations</h2>
<div class="paragraph"><p><strong>Default number of partitions</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:16%" />
<col style="width:16%" />
<col style="width:16%" />
<col style="width:50%" />
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock"><strong>rdd.join(otherRDD)</strong>, <strong>rdd.cogroup(otherRDD)</strong>, &#8230;&#8203;</p></td>
<td class="tableblock halign-left valign-top" colspan="2"><p class="tableblock"><strong>Partionner Can Be Reused</strong></p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock"><strong>Resulting RDD partitions.size</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>rdd</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>otherRDD</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Case 1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">No</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">No</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>max</em>(<em>rdd</em>.<em>partitions</em>.<em>size</em>, <em>otherRDD</em>.<em>partitions</em>.<em>size</em>)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Case 2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Yes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">No</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>rdd</em>.<em>partitions</em>.<em>size</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Case 3</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">No</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Yes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>otherRDD</em>.<em>partitions</em>.<em>size</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Case 4</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Yes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Yes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>max</em>(<em>rdd</em>.<em>partitions</em>.<em>size</em>, <em>otherRDD</em>.<em>partitions</em>.<em>size</em>)</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here are the four scenarios to determine how Spark derives a number of partitions in the resulting RDD
when a binary key-based transformation is used.</p></div>
<div class="paragraph"><p>When one or both inputs have suitable partitioners to perform a transformation, Spark reuses
partitions without shuffling for better efficiency.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Example - binary transformations</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val users = sc.cassandraTable("killr_video","users")
              .keyBy(row =&gt; row.getInt("user_id"))
// Partitioning properties: 3, None

val interactions = sc.cassandraTable("killr_video","video_interactions_by_user")
                     .keyBy(row =&gt; row.getInt("user_id"))
                     .groupByKey
// Partitioning properties: HashPartitioner@4

val usersWithInteractions = users.join(interactions)
// Partitioning properties: HashPartitioner@4</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This is an example of Case 3: <em>interactions</em>' partitioner and number of partitions are reused to compute the join with <em>users</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-tuning-partitioning-controlling-partitioning">
<h2>Controlling Partitioning</h2>
<div class="paragraph"><p><strong>One of the most important performance optimizations in Spark</strong></p></div>
<div class="ulist">
<ul>
<li><p>
Number of partitions<div class="ulist">
<ul>
<li>Affects a number of tasks and the level of parallelism</li>
<li><em>Goal</em>: balancing task execution and scheduling times</li>
</ul>
</div></p></li>
<li><p>
Partitioner<div class="ulist">
<ul>
<li>Affects key-based operations</li>
<li><em>Goal</em>: Avoiding shuffling the same dataset multiple times</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Optimal partitioning results in optimal performance.</p></div>
<div class="paragraph"><p>By controlling a number of partitions in an RDD, we affect a number
of tasks that execute our operations and, ultimately, the level of parallelism for
each operation. If a task is running for too long and fails, re-executing the task will still
take long time. If tasks are running for a few milliseconds, then it is likely that scheduling time
will have a considerable impact on overall performance of an application. Finding a balance between
task execution and scheduling times is the first main reason to pay attention to partitioning.</p></div>
<div class="paragraph"><p>By controlling a partitioner for an RDD, we affect key-based operations that rely on specific partitioners,
such as <em>HashPartitioner</em> or <em>RangePartitioner</em>, to perform computation. Setting a partitioner for an RDD
triggers the process called shuffling, which is expensive because data has to be reorganized into
new partitions, and that requires disk I/O and network traffic. Pre-partitioning and caching an RDD in certain situations
can avoid re-shuffling the same RDD multiple times, which is the second main reason to control partitioning.</p></div>
</div>
</div>
</section>
<section class="slide" id="how-many-partitions-is-good">
<h2>How Many Partitions is Good?</h2>
<div class="paragraph"><p><strong>General insights</strong></p></div>
<div class="ulist">
<ul>
<li><p>
Too few partitions can be a problem<div class="ulist">
<ul>
<li>Less concurrency</li>
<li>Possible data skew</li>
<li>Increased memory pressure</li>
<li>Longer recovery from a failure</li>
</ul>
</div></p></li>
<li><p>
Too many partitions can be a problem<div class="ulist">
<ul>
<li>Task scheduling may take longer than task execution</li>
<li>More lineage information to maintain</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>These are some general insights on a number of partitions.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>General recommendations</strong></p></div>
<div class="ulist">
<ul>
<li>Usually between 100 and 10,000 partitions depending on data and cluster size</li>
<li>Lower bound  2x number of cores in a cluster available to an application</li>
<li>Upper bound  tasks should take 100+ ms to execute</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>You will often need to change the default number of partitions to
optimize your application performance! The best results are frequently achieved by
experimenting with different partitioning settings and monitoring the metrics in
<em>Spark Application Web UI</em>.</p></div>
</td>
</tr>
</table>
</div>
<div class="imageblock center">
<div class="content">
<img alt="screenshot ui" src="images/spark/tuning-partitioning/controlling-partitioning/screenshot-ui.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Remember that, by default, the default level of parallelism (<em>sc.defaultParallelism</em>) equals
to a number of cores available to an application, which implies the number of partitions for
<em>parallelize</em>, <em>textFile</em>, <em>cassandraTable</em>, and many Spark operations.</p></div>
<div class="paragraph"><p>You can always get metrics for scheduling and execution times from the Spark Application UI.</p></div>
</div>
</div>
</section>
<section class="slide" id="which-operations-do-require-a-partitioner">
<h2>Which Operations Do Require a Partitioner?</h2>
<div class="paragraph"><p><strong>Many key-based operations</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Partitioner</th>
<th class="tableblock halign-left valign-top">Operations</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>HashPartitioner</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>reduceByKey</em>, <em>foldByKey</em>, <em>combineByKey</em>,
   <em>groupByKey</em>, <em>cogroup</em>,
   <em>join</em>, <em>leftOuterJoin</em>, <em>rightOuterJoin</em>, <em>fullOuterJoin</em>, <em>lookup</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>RangePartitioner</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>sortByKey</em></p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>Spark allows defining custom partitioners, which may be useful when you have special requirements
for assigning a key to a partition.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Most key-based operations (transformations and actions) require a key-based partitioner.
If an input RDD has <em>None</em> as a partitioner, Spark will shuffle data and set a partitioner while performing an operation.
If an input RDD has a key-based partitioner, Spark will reuse it without shuffling to perform an operation.
We need to focus on reusing a partitioner whenever possible!</p></div>
</div>
</div>
</section>
<section class="slide" id="setting-and-reusing-a-partitioner">
<h2>Setting and Reusing a Partitioner</h2>
<div class="paragraph"><p><strong>Sample optimization scenario</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:40%" />
<col style="width:60%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Suboptimal case</th>
<th class="tableblock halign-left valign-top">Optimal case</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><div><div class="imageblock center">
<div class="content">
<img alt="suboptimal case" src="images/spark/tuning-partitioning/controlling-partitioning/suboptimal-case.svg" />
</div>
</div></div></td>
<td class="tableblock halign-left valign-top"><div><div class="imageblock center">
<div class="content">
<img alt="optimal case" src="images/spark/tuning-partitioning/controlling-partitioning/optimal-case.svg" />
</div>
</div></div></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here is a sample optimization scenario that demonstrates how to avoid re-shuffling the same RDD.
We use DAGs of operations to illustrate the concepts.</p></div>
<div class="paragraph"><p>In the suboptimal case, we perform two key-based transformations on <em>rdd1</em> with no partitioner to derive <em>rdd2</em> and <em>rdd3</em> and
one key-based action. Let us assume all the three operations require a <em>HashPartitioner</em>. There are two
more actions on <em>rdd2</em> and <em>rdd3</em>, which may or may not be key-based. Given the lazy evaluation
used in Spark and the three actions, we will have to perform shuffling of <em>rdd1</em> three times
(even if <em>rdd1</em> is cached in-memory)
to satisfy key-based partitioning requirements. That is not efficient.</p></div>
<div class="paragraph"><p>In the optimal case, we first explicitly pre-partition and cache <em>rdd1</em> using special transformations
and <em>HashPartitioner</em> into <em>rdd1'</em>. This enables the key-based transformations and action to reuse
the <em>rdd1'</em> partitioner rather then shuffle. As a result, Spark can avoid two extra shuffling operations.</p></div>
<div class="paragraph"><p>As you may have noticed, use cases for the setting a partitioner and caching optimizations have some common ground.
With respect to a DAG of operations, any RDD that has multiple descendants is a good candidate for caching.
It is also a good candidate for explicitly setting a key-based partitioner if its descendants can
reuse the partitioner. Therefore, setting a partitioner is always followed by caching.</p></div>
</div>
</div>
</section>
<section class="slide" id="mechanisms-for-controlling-partitioning">
<h2>Mechanisms for Controlling Partitioning</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Mechanism</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Application settings</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Special properties, such as <em>spark.default.parallelism</em> and <em>spark.cassandra.input.split.size_in_mb</em>,
that can affect a number of partitions for Spark and Cassandra RDDs when set on the <em>SparkConf</em> object to initialize an application <em>SparkContext</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Operation parameters</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Many API calls, transformations, and actions can take an additional parameter to specify a number of tasks to use to
compute the result, which directly affects a number of partitions. For example, <em>parallelize</em>(&#8230;&#8203;,<em>numTasks</em>),
<em>textFile</em>(&#8230;&#8203;,<em>numTasks</em>), <em>reduceByKey</em>(&#8230;&#8203;,<em>numTasks</em>), <em>groupByKey</em>(<em>numTasks</em>), <em>join</em>(&#8230;&#8203;,<em>numTasks</em>),
<em>countByKey</em>(<em>numTasks</em>), etc.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Repartitioning transformations</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>repartition</em>, <em>coalesce</em>,
 <em>partitionBy</em></p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The first two mechanisms should be straightforward.
They only allow controlling a number of partitions (level of parallelism).</p></div>
<div class="paragraph"><p>For the rest of this presentation, we will focus on the last mechanism&#8201;&#8212;&#8201;the three repartitioning transformations.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformations-em-repartition-em-and-em-coalesce-em">
<h2>Transformations <em>repartition</em> and <em>coalesce</em></h2>
<div class="paragraph"><p><strong>Repartitioning transformations for generic RDDs</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>repartition</strong>(<em>numPartitions</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD is formed by shuffling elements of the source RDD into <em>numPartitions</em> new partitions,
where <em>numPartitions</em> can be larger or smaller than the number of partitions in the source RDD. The new
RDD partitioner is set to <em>None</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>coalesce</strong>(<em>numPartitions</em>, [<em>shuffle</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD is formed by merging partitions of the source RDD into <em>numPartitions</em> new partitions,
where <em>numPartitions</em> must be smaller than the number of partitions in the source RDD. The optional
parameter disables shuffling by default (<em>shuffle = false</em>). With shuffling enabled, <em>coalesce</em> can be used
like <em>repartition</em> to decrease or increase a number of partitions. The new
RDD partitioner is set to <em>None</em>.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>These transformations always result in <em>None</em> as a partitioner, even if a parent RDD has a key-based partitioner.
Therefore, they are commonly used with generic rather than key-value pair RDDs.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Example</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>println(sc.defaultParallelism)
// Sample output: 3

val movies = sc.parallelize(List(("Alice in Wonderland",2016), ("Alice Through the Looking Glass",2010), ...))
println(movies.partitions.size)
// Sample output: 3

println(movies.repartition(2*sc.defaultParallelism).partitions.size)
// Sample output: 6

println(movies.coalesce(4).partitions.size)
// Sample output: 4</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In this example, <em>repartition</em> will shuffle but <em>coalesce</em> will not.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformation-em-partitionby-em">
<h2>Transformation <em>partitionBy</em></h2>
<div class="paragraph"><p><strong>Repartitioning transformation for key-value pair RDDs</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:20%" />
<col style="width:80%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>partitionBy</strong>(<em>Partitioner</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD is formed by shuffling elements of the source RDD  into <em>numPartitions</em> new partitions
using a specified <em>Partitioner</em>, which
can be <em>HashPartitioner</em>(<em>numPartitions</em>), <em>RangePartitioner</em>(<em>numPartitions</em>, <em>sourceRDD</em>), or
a custom partitioner.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val movies = sc.parallelize(List(("Alice in Wonderland",2016), ("Alice Through the Looking Glass",2010), ...))
               .partitionBy(new org.apache.spark.HashPartitioner(9))

println(movies.partitioner)
// Sample output: HashPartitioner@9</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The <em>partitionBy</em> transformation results in an RDD with a desired partitioner.
Therefore, it is commonly used with key-value pair RDDs.</p></div>
</div>
</div>
</section>
<section class="slide" id="challenge">
<h2>Challenge</h2>
<div class="paragraph"><p><strong>Suboptimal code</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/tuning-partitioning/controlling-partitioning/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val movies = sc.cassandraTable("killr_video","movies")
               .keyBy(row =&gt; row.getInt("release_year"))
               .repartition(2*sc.defaultParallelism)

val movieCountByYear  = movies.countByKey.foreach(println)

val moviesByYear = movies.groupByKey.collect.foreach(println)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Shuffling is done three times in this example.</p></div>
</div>
</div>
</section>
<section class="slide" id="challenge-solution">
<h2>Challenge Solution</h2>
<div class="paragraph"><p><strong>Optimized code</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/tuning-partitioning/controlling-partitioning/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val movies = sc.cassandraTable("killr_video","movies")
               .keyBy(row =&gt; row.getInt("release_year"))
               .partitionBy(
                 new org.apache.spark.HashPartitioner(2*sc.defaultParallelism))
               .cache

val movieCountByYear  = movies.countByKey.foreach(println)

val moviesByYear = movies.groupByKey.collect.foreach(println)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Shuffling is only done once in this example.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-tuning-partitioning-data-shuffling">
<h2>Data Shuffling</h2>
<div class="paragraph"><p><strong>Definition and use cases</strong></p></div>
<div class="paragraph"><p><em><strong>Definition</strong></em></p></div>
<div class="verseblock">
<pre class="content">Data shuffling is the process of reorganizing and transferring data from existing partitions
into new partitions to achieve one or both properties for the resulting partitions:
1) having a desired number of partitions
2) having pairs with the same key in the same partition</pre>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="paragraph"><p><em><strong>Main use cases</strong></em></p></div>
<div class="ulist">
<ul>
<li>Controlling the level of parallelism</li>
<li>Supporting some key-based operations</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Understanding data shuffling helps to write efficient code.</p></div>
</div>
</div>
</section>
<section class="slide" id="which-operations-may-trigger-shuffling">
<h2>Which Operations May Trigger Shuffling?</h2>
<div class="ulist">
<ul>
<li><p>
Repartitioning transformations<div class="ulist">
<ul>
<li><em>repartition</em>, <em>coalesce</em>, <em>partitionBy</em></li>
</ul>
</div></p></li>
<li><p>
Many key-based operations<div class="ulist">
<ul>
<li><em>reduceByKey</em>, <em>foldByKey</em>, <em>combineByKey</em></li>
<li><em>groupByKey</em>, <em>cogroup</em></li>
<li><em>join</em>, <em>leftOuterJoin</em>, <em>rightOuterJoin</em>, <em>fullOuterJoin</em></li>
<li><em>sortByKey</em></li>
<li><em>lookup</em></li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>These are some exceptions:</p></div>
<div class="ulist">
<ul>
<li><em>coalesce</em> may or may not require shuffling depending on its <em>shuffle</em> parameter</li>
<li>Key-based operations will not shuffle data if an RDD has an appropriate partitioner (in other words,
the RDD has been shuffled in a previous transformation)</li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="how-does-shuffling-work">
<h2>How Does Shuffling Work?</h2>
<div class="paragraph"><p><strong>Shuffling an RDD with three partitions into an RDD with four partitions</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="shuffling" src="images/spark/tuning-partitioning/data-shuffling/shuffling.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Let us consider how shuffling is done in general. This illustration shows
how an RDD with three partitions is shuffled into an RDD with four partitions.</p></div>
<div class="ulist">
<ul>
<li>Shuffle write: A <em>Map Task</em> is executed on each original partition. It repartitions data into four new buckets that
are written to disk on a worker node. Partial aggregation may be used for some operations (e.g., <em>reduceByKey</em>),
to aggregate pairs with the same key in the buckets before writing them to disk.</li>
<li>Shuffle read: Four <em>Reduce Tasks</em> read their corresponding on-disk buckets into memory to assemble new partitions:
A, B, C, and D. Final aggregation is again only required for certain operations. The new RDD now has four partitions
that can be processed by further operations.</li>
</ul>
</div>
<div class="paragraph"><p>How buckets are computed depends on an operation and its partitioner (<em>None</em>, <em>HashPartitioner</em>, <em>RangePartitioner</em>, custom partitioner).</p></div>
</div>
</div>
</section>
<section class="slide" id="types-of-shuffling">
<h2>Types of Shuffling</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:50%" />
<col style="width:50%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Sort-based shuffling</th>
<th class="tableblock halign-left valign-top">Hash-based shuffling</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>Generally more efficient</li>
<li>Default strategy in Spark 1.2+</li>
<li>Writes <em>2 x M</em> files</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="two files" src="images/spark/tuning-partitioning/data-shuffling/two-files.svg" />
</div>
</div></div></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>Generally less efficient</li>
<li>Default strategy prior Spark 1.2</li>
<li>Writes <em>M x R</em> files</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="many files" src="images/spark/tuning-partitioning/data-shuffling/many-files.svg" />
</div>
</div></div></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Sort-based shuffling is generally more efficient for big data use cases.</p></div>
<div class="paragraph"><p><em>M</em> denotes a number of map tasks.
<em>R</em> denotes a number of reduce tasks.</p></div>
<div class="paragraph"><p>The illustrations are for a single map task.</p></div>
</div>
</div>
</section>
<section class="slide" id="shuffling-is-expensive">
<h2>Shuffling is Expensive</h2>
<div class="paragraph"><p><strong>Shuffling cost factors</strong></p></div>
<div class="ulist">
<ul>
<li>Disk IO</li>
<li>Network traffic</li>
<li>Partitioning</li>
<li>External sorting</li>
<li>Serialization/deserialization</li>
<li>Data compression</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Some of these factors may be eliminated or partially eliminated, depending
on application settings and shuffling algorithms used. However Disk IO and
network traffic are guaranteed!</p></div>
</div>
</div>
</section>
<section class="slide" id="how-to-optimize-shuffling-performance">
<h2>How to Optimize Shuffling Performance?</h2>
<div class="ulist">
<ul>
<li>Control partitioning to avoid re-shuffling</li>
<li>Take advantage of aggregation when possible</li>
<li>Choose appropriate application properties</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<em>Spark Application Web UI</em> provides information about your application shuffle writes and reads.
</td>
</tr>
</table>
</div>
<div class="imageblock center">
<div class="content">
<img alt="screenshot ui" src="images/spark/tuning-partitioning/data-shuffling/screenshot-ui.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="ulist">
<ul>
<li>Control partitioning to avoid re-shuffling&#8201;&#8212;&#8201;discussed in a separate vertex</li>
<li>Take advantage of aggregation when possible&#8201;&#8212;&#8201;prefer <em>reduceByKey</em> to <em>groupByKey</em> when doing aggregation</li>
<li>Choose appropriate application properties&#8201;&#8212;&#8201;next slide</li>
</ul>
</div>
<div class="paragraph"><p>The screenshot shows shuffle write and shuffle read in the last two columns.</p></div>
</div>
</div>
</section>
<section class="slide" id="application-shuffling-properties">
<h2>Application Shuffling Properties</h2>
<div class="paragraph"><p><strong>Configurable on a <em>SparkConf</em> object</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:28%" />
<col style="width:57%" />
<col style="width:14%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Property</th>
<th class="tableblock halign-left valign-top">Description</th>
<th class="tableblock halign-left valign-top">Default</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">spark.shuffle.manager</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Specifies a data shuffling strategy to be <em>sort</em> or <em>hash</em>.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>sort</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">spark.shuffle.spill</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Enables or disables spilling data out to disk by reduce tasks.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>true</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">spark.shuffle.memoryFraction</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Specifies a spilling threshold as a fraction of Java heap.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>0.2</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">spark.shuffle.sort.bypassMergeThreshold</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Instructs sort-based shuffling to not  merge-sort data
if there is no map-side aggregation and a number of reduce tasks is not greater than
this threshold.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>200</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">spark.shuffle.compress</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Enables or disable compression for shuffle writes.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>true</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">spark.shuffle.spill.compress</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Enables or disable compression for data spilled out to disk.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>true</em></p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Some of the common application shuffling properties. Default values are generally good.</p></div>
<div class="paragraph"><p>This is not a comprehensive list of shuffling properties.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-cassandra-connector-optimizations-counting-rows">
<h2>The Challenge: Suboptimal Code</h2>
<div class="paragraph"><p><strong>Counting movies with Johnny Depp that were released before 2015</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-cassandra-connector-optimizations/counting-rows/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable("killr_video","movies_by_actor")
  .where("actor = 'Johnny Depp' AND release_year &lt; 2015")
  .count

// Sample output: 49</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Counting rows in a Cassandra table is quite simple but there is one important trick
you should know about.</p></div>
<div class="paragraph"><p>This code is suboptimal: it retrieves data from Cassandra into Spark and calls
Spark&#8217;s <em>count</em> action. We get the correct result but we should be able to solve this problem in
a more efficient manner using <em>Spark-Cassandra Connector</em> API. Let Cassandra do the counting for us.</p></div>
</div>
</div>
</section>
<section class="slide" id="counting-rows-in-a-cassandra-table">
<h2>Counting Rows in a Cassandra Table</h2>
<div class="paragraph"><p><strong>Spark-Cassandra Connector API</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Action</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>cassandraCount</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns a number of rows in the source Cassandra RDD.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>When called on a Cassandra RDD created by <em>cassandraTable</em>, <em>cassandraCount</em>
instructs Cassandra (rather than Spark) to count rows.</p></div>
</div>
</div>
</section>
<section class="slide" id="our-challenge-solution-3">
<h2>Our Challenge Solution</h2>
<div class="paragraph"><p><strong>Counting movies with Johnny Depp that were released before 2015</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-cassandra-connector-optimizations/counting-rows/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable("killr_video","movies_by_actor")
  .where("actor = 'Johnny Depp' AND release_year &lt; 2015")
  .cassandraCount

// Sample output: 49</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This solution is better: Cassandra does row counting instead of retrieving rows and handing them to Spark for counting.
The difference will be quite noticeable when counting a large number of rows.</p></div>
</div>
</div>
</section>
<section class="slide" id="however">
<h2>However &#8230;&#8203;</h2>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-cassandra-connector-optimizations/counting-rows/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="title"><em>cassandraCount is inapplicable</em></div>
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable("killr_video","movies_by_actor")
  .filter(row =&gt; row.getFloat("rating")&gt; 6.0)
  .count</code></pre>
</div>
</div>
<div class="listingblock left">
<div class="title"><em>count is a better choice</em></div>
<div class="content">
<pre class="CodeRay"><code>val movies = sc.cassandraTable("killr_video","movies_by_actor")
               .select("release_year")
               .where("actor = 'Johnny Depp' AND release_year &lt; 2015")
               .cache

println(movies.count)
movies.keyBy(row =&gt; row.getInt("release_year"))
      .countByKey.foreach(println)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In the first example, <em>cassandraCount</em> cannot replace <em>count</em> because we are using <em>filter</em> that
transforms a Cassandra RDD into a Spark RDD.</p></div>
<div class="paragraph"><p>We can modify the second example to use <em>cassandraCount</em> but it is likely to make our code
less efficient because the second action (<em>countByKey</em>) will require to retrieve rows from Cassandra anyway.
In this case, it is better to retrieve and cache data to perform both actions using Spark.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-cassandra-connector-optimizations-grouping-rows">
<h2>The Challenge: Suboptimal Code</h2>
<div class="paragraph"><p><strong>Grouping movies by actor and release year</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-cassandra-connector-optimizations/grouping-rows/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable("killr_video","movies_by_actor")
  .select("actor","release_year","title")
  .as((a:String, y:Int, t:String) =&gt; ((a,y),t))
  .groupByKey
  .takeSample(false, 100)
  .foreach(println)

// Sample output:
// ((Johnny Depp,2010),CompactBuffer(The Tourist, Alice in Wonderland))
// ((Johnny Depp,2014),CompactBuffer(Into the Woods, Transcendence, Tusk))</code></pre>
</div>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="icon-warning" title="Warning"></i>
</td>
<td class="content">
This code requires expensive shuffling.
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This code is suboptimal: it retrieves data from Cassandra into Spark and calls
Spark&#8217;s <em>groupByKey</em> to solve the problem. We get the correct result but we should be able to solve this problem in
a more efficient manner using <em>Spark-Cassandra Connector</em> API.</p></div>
<div class="paragraph"><p>Intuitively, Cassandra stores rows in a table in partitions that group rows by a partition key (<em>actor</em> in this example).
Furthermore, rows within a partition are also grouped by clustering columns. Instead of asking Spark to do grouping for us,
we should ask Cassandra to give us already grouped data. By doing this, we can avoid expensive shuffling!</p></div>
</div>
</div>
</section>
<section class="slide" id="grouping-rows-by-primary-key-columns">
<h2>Grouping Rows by Primary Key Columns</h2>
<div class="paragraph"><p><strong>Spark-Cassandra Connector API</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>spanByKey</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD of (<em>K</em>,<em>Seq[V]</em>) pairs is formed by grouping values for each key
in the source Cassandra-based RDD of (<em>K</em>,<em>V</em>) pairs.
Grouping is performed on the Cassandra side based on primary key columns.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>spanBy</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD of (<em>K</em>,<em>Seq[V]</em>) pairs is formed by grouping elements of type <em>V</em>
in the source Cassandra-based RDD for each key <em>K</em> as defined by function <em>f</em>.
Grouping is performed on the Cassandra side based on primary key columns.</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<em>spanByKey</em> and <em>spanBy</em> are only applicable when grouping by a table partition key
and, optionally, one or more clustering columns. The grouping key must respect the natural clustering key order
as defined in the table schema.
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>When called on a Cassandra RDD created by <em>cassandraTable</em>, these transformations
instruct Cassandra (rather than Spark) to group data by a partition key or a <strong>proper</strong> subset of a primary key.</p></div>
</div>
</div>
</section>
<section class="slide" id="our-challenge-solution-1">
<h2>Our Challenge Solution 1</h2>
<div class="paragraph"><p><strong>Grouping movies by actor and release year</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-cassandra-connector-optimizations/grouping-rows/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable("killr_video","movies_by_actor")
  .select("actor","release_year","title")
  .as((a:String, y:Int, t:String) =&gt; ((a,y),t))
  .spanByKey
  .takeSample(false, 100)
  .foreach(println)

// Sample output:
// ((Johnny Depp,2010),ArrayBuffer(The Tourist, Alice in Wonderland))
// ((Johnny Depp,2014),ArrayBuffer(Into the Woods, Transcendence, Tusk))</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
No shuffling is required!
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This example uses <em>spanByKey</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="our-challenge-solution-2-2">
<h2>Our Challenge Solution 2</h2>
<div class="paragraph"><p><strong>Grouping movies by actor and release year</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-cassandra-connector-optimizations/grouping-rows/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable[(String,Int,String)]("killr_video","movies_by_actor")
  .select("actor","release_year","title")
  .spanBy{case (a,y,t) =&gt; (a,y)}
  .takeSample(false, 100)
  .foreach(println)

// Sample output:
// ((Johnny Depp,2010),ArrayBuffer((Johnny Depp,2010,The Tourist),
//                                 (Johnny Depp,2010,Alice in Wonderland)))
// ((Johnny Depp,2014),ArrayBuffer((Johnny Depp,2014,Into the Woods),
//              (Johnny Depp,2014,Transcendence), (Johnny Depp,2014,Tusk)))</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
No shuffling is required!
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This example uses <em>spanBy</em>. The output is a bit more verbose and the computation is more memory intensive than Solution 1.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-cassandra-connector-optimizations-joining-tables">
<h2>The Challenge: Suboptimal Code</h2>
<div class="paragraph"><p><strong>Joining two Cassandra tables on partition keys</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="schema" src="images/spark/spark-cassandra-connector-optimizations/joining-tables/schema.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val actors = sc.cassandraTable("killr_video","actors")
               .keyBy(row =&gt; row.getString("actor"))
val movies = sc.cassandraTable("killr_video","movies_by_actor")
               .keyBy(row =&gt; row.getString("actor"))
actors.join(movies).takeSample(false, 100).foreach(println)

// Sample output:
// (Johnny Depp,(
//  CassandraRow{actor: Johnny Depp, ..., place_of_birth: Owensboro, ...},
//  CassandraRow{actor: Johnny Depp, ..., title: Pirates ...}))</code></pre>
</div>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="icon-warning" title="Warning"></i>
</td>
<td class="content">
This code requires expensive shuffling.
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This code is suboptimal: it retrieves data from two Cassandra tables into Spark and calls
Spark&#8217;s <em>join</em> to solve the problem. To perform the join, Spark shuffles both
RDDs, which is expensive.
We get the correct result but we should be able to solve this problem in
a more efficient manner using <em>Spark-Cassandra Connector</em> API.</p></div>
<div class="paragraph"><p>Intuitively, Cassandra stores rows in a table in partitions that group rows by a partition key (<em>actor</em> in both tables).
Partitions from both tables with the same partition key reside on the same Cassandra node.
Therefore, Cassandra is in a better position to use data locality to perform a join with no shuffling required.</p></div>
</div>
</div>
</section>
<section class="slide" id="joining-cassandra-tables-on-primary-key-columns">
<h2>Joining Cassandra Tables on Primary Key Columns</h2>
<div class="paragraph"><p><strong>Spark-Cassandra Connector API</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>joinWithCassandraTable</strong>(<em>keyspace</em>, <em>table</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD of (<em>E</em>, <em>R</em>) pairs is formed by combining elements <em>E</em>
from the source RDD with Cassandra rows <em>R</em> from the specified table <em>keyspace.table</em>.
The default join condition is the equality of the <em>table</em>'s partition key columns and respective
fields of the source RDD.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>on</strong>(<em>columns</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Optionally used with <em>joinWithCassandraTable</em>() to specify which table <em>columns</em> to join on.
This overrides the default join condition.</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
Columns in a join condition can include a table partition key
and, optionally, one or more clustering columns. The join condition must respect the natural clustering key order
as defined in the table schema.
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This is the join that is performed by Cassandra (<em>Spark-Cassandra Connector</em>, to be precise) rather than Spark.
The  source RDD for transformation <em>joinWithCassandraTable</em> can be an RDD with <em>CassandraRow</em> objects retrieved from
another table or any RDD that contains objects with information about fields.</p></div>
<div class="paragraph"><p><em>on</em> columns should constitute a partition key or a <strong>proper</strong> subset of a primary key.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-4">
<h2>Example</h2>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-cassandra-connector-optimizations/joining-tables/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>case class ActorYear(actor: String, release_year: Int)
val actors2014 = sc.parallelize(List(ActorYear("Johnny Depp",2014),
                                     ActorYear("Bruce Willis",2014)))

actors2014.joinWithCassandraTable("killr_video","movies_by_actor")
          .takeSample(false, 100).foreach(println)
// Sample output:
// (ActorYear(Johnny Depp,2014),CassandraRow{actor: Johnny Depp,
                                             release_year: 2010, ...})

actors2014.joinWithCassandraTable("killr_video","movies_by_actor")
          .on(SomeColumns("actor", "release_year"))
          .takeSample(false, 100).foreach(println)
// Sample output:
// (ActorYear(Johnny Depp,2014),CassandraRow{actor: Johnny Depp,
                                             release_year: 2014, ...})</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The first join is on column <em>actor</em> (default join condition). Notice the sample output for
the release year of 2010.</p></div>
<div class="paragraph"><p>The second join is on columns <em>actor</em> and <em>release_year</em> (specified explicitly).
Notice the sample output - only 2014 can be in the result.</p></div>
</div>
</div>
</section>
<section class="slide" id="how-does-the-cassandra-join-work">
<h2>How Does the Cassandra Join Work?</h2>
<div class="paragraph"><p><strong>Cassandra table is used as an index</strong></p></div>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="joinWithCassandraTable" src="images/spark/spark-cassandra-connector-optimizations/joining-tables/joinWithCassandraTable.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>Pseudocode</em>:</p></div>
<div class="ulist">
<ul>
<li><p>
For each element in the source RDD<div class="ulist">
<ul>
<li>Retrieve <strong>matching</strong> rows from a Cassandra partition with a CQL query</li>
<li>Compute all possible combinations (Cartesian product) of the element and the rows</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="paragraph"><p>This is really a variation of the classic index-based join algorithm.</p></div>
</div>
</div>
</section>
<section class="slide" id="our-challenge-solution-4">
<h2>Our Challenge Solution</h2>
<div class="paragraph"><p><strong>Joining two Cassandra tables on partition keys</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="schema" src="images/spark/spark-cassandra-connector-optimizations/joining-tables/schema.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable("killr_video","actors")
  .joinWithCassandraTable("killr_video","movies_by_actor")
  .takeSample(false, 100).foreach(println)

// Sample output:
// (CassandraRow{actor: Johnny Depp, ..., place_of_birth: Owensboro, ...},
//  CassandraRow{actor: Johnny Depp, ..., title: Pirates ...})</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
No shuffling is required!
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This example uses <em>joinWithCassandraTable</em> to solve our challenge. The difference from
the previous example is that the source RDD is a Cassandra RDD.
This is a simple optimization that may have a significant performance impact.</p></div>
</div>
</div>
</section>
<section class="slide" id="final-challenge">
<h2>Final Challenge</h2>
<div class="paragraph"><p><strong>Which solution is more efficient?</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable("killr_video","actors")
  .joinWithCassandraTable("killr_video","movies_by_actor")
  .takeSample(false, 100).foreach(println)</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>sc.cassandraTable("killr_video","movies_by_actor")
  .joinWithCassandraTable("killr_video","actors")
  .takeSample(false, 100).foreach(println)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Now that you know how the Cassandra join is evaluated, which one would you pick?</p></div>
<div class="paragraph"><p>In the case of two Cassandra tables, the transformation is <em>commutative</em>. You can pick
any of the tables for your source RDD. You better pick a table with a smaller number of rows.</p></div>
<div class="paragraph"><p>Answer: the first solution.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-cassandra-connector-optimizations-cassandra-aware-partitioning">
<h2>The Challenge: Suboptimal Code</h2>
<div class="paragraph"><p><strong>Joining an RDD with a Cassandra table on a partition key</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-cassandra-connector-optimizations/cassandra-aware-partitioning/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>case class Actor(actor: String)
val actors = sc.parallelize(List(Actor("Johnny Depp"),Actor("Bruce Willis")))

actors.joinWithCassandraTable("killr_video","movies_by_actor")
      .takeSample(false, 100).foreach(println)

// Sample output:
// (Actor(Johnny Depp),
//  CassandraRow{actor: Johnny Depp, ...,
//               title: Pirates of the Caribbean: On Stranger Tides})</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This code is suboptimal due to mismatch in the <em>actors</em> RDD partitioning and the <em>movies_by_actor</em> table partitioning.
We are not doing shuffling here but table partitions have to be transfered to nodes where actors are located to do the join. This is
still expensive. Please see the next slide for an illustration.</p></div>
<div class="paragraph"><p>We get the correct result but we should be able to solve this problem in
a more efficient manner using <em>Spark-Cassandra Connector</em> API. We should be able to repartition the <em>actors</em> RDD to match table partitioning,
such that records with the same partition key (from both the RDD and the table) are located on the same node.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>RDD and table partitioning</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:50%" />
<col style="width:50%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Suboptimal (current solution)</th>
<th class="tableblock halign-left valign-top">Optimal (desired solution)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><div><div class="imageblock" style="float: center">
<div class="content">
<img alt="mismatch" src="images/spark/spark-cassandra-connector-optimizations/cassandra-aware-partitioning/mismatch.svg" />
</div>
</div></div></td>
<td class="tableblock halign-left valign-top"><div><div class="imageblock" style="float: center">
<div class="content">
<img alt="match" src="images/spark/spark-cassandra-connector-optimizations/cassandra-aware-partitioning/match.svg" />
</div>
</div></div></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Left illustration: data has to be moved among nodes to perform the join.</p></div>
<div class="paragraph"><p>Right illustration: data for the join is already co-located.</p></div>
</div>
</div>
</section>
<section class="slide" id="cassandra-aware-repartitioning-of-an-rdd">
<h2>Cassandra-Aware Repartitioning of an RDD</h2>
<div class="paragraph"><p><strong>Spark-Cassandra Connector API</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>repartitionByCassandraReplica</strong>(<em>keyspace</em>, <em>table</em>, [<em>numPartitionsPerHost</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD is formed by shuffling elements of the source RDD into <em>numPartitionsPerHost</em> new partitions per host
(10 by default) according to the replication strategy of a given <em>table</em> and <em>keyspace</em>.
The source RDD must contain information about values that correspond to the table partition key columns.
The new
RDD partitioner is set to <em>ReplicaPartitioner</em>.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Transformation <em>repartitionByCassandraReplica</em> can be used to relocate data in an RDD to match the replication strategy of a given table and keyspace.
The transformation will look for partition key information in the given RDD and then use those values to determine which nodes in the cluster would be responsible for that data.
You can control the resulting number of partitions with parameter <em>numPartitionsPerHost</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="our-challenge-solution-5">
<h2>Our Challenge Solution</h2>
<div class="paragraph"><p><strong>Joining an RDD with a Cassandra table on a partition key</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-cassandra-connector-optimizations/cassandra-aware-partitioning/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>case class Actor(actor: String)
val actors = sc.parallelize(List(Actor("Johnny Depp"),Actor("Bruce Willis")))
               .repartitionByCassandraReplica("killr_video","movies_by_actor")

actors.joinWithCassandraTable("killr_video","movies_by_actor")
      .takeSample(false, 100).foreach(println)

// Sample output:
// (Actor(Johnny Depp),
//  CassandraRow{actor: Johnny Depp, ...,
//               title: Pirates of the Caribbean: On Stranger Tides})</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The <em>actors</em> RDD is repartitioned using Cassandra&#8217;s <em>ReplicaPartitioner</em> before the join is performed.
Data locality is the crux of this optimization!</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-dstream">
<h2>Stream Processing</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="stream processing system" src="images/spark/streaming/dstream/stream-processing-system.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This is a general stream processing system.</p></div>
<div class="ulist">
<ul>
<li><p>
Data arrives in one or more streams<div class="ulist">
<ul>
<li>Sensor data and time series</li>
<li>Internet and Web traffic</li>
<li>Events, interactions, and transactions</li>
<li>Surveillance data</li>
</ul>
</div></p></li>
<li><p>
Data stream needs to be processed immediately (in real time)<div class="ulist">
<ul>
<li>Basic activity statistics and monitoring</li>
<li>Insights, trends, recommendations</li>
<li>Anomaly, spam, fraud, and intrusion detection</li>
</ul>
</div></p></li>
<li><p>
It may be too expensive or unnecessary to store raw data<div class="ulist">
<ul>
<li>The archival storage may be optional but it is indeed useful for off-line analytics</li>
<li>Storing a summary (aggregates) of data may be sufficient in some cases</li>
</ul>
</div></p></li>
<li><p>
What happens inside the stream processor may vary:<div class="ulist">
<ul>
<li><p>
Record-at-a-time processing (traditional approach)<div class="ulist">
<ul>
<li>Storm (Twitter)</li>
<li>Samza (LinkedIn)</li>
<li>S4 (Yahoo)</li>
<li>MillWheel (Google)</li>
</ul>
</div></p></li>
<li><p>
Micro-batch computation on small time intervals<div class="ulist">
<ul>
<li>Spark Streaming</li>
<li>Some of the traditional systems now support micro-batching, too</li>
</ul>
</div></p></li>
</ul>
</div></p></li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming">
<h2>Spark Streaming</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="spark streaming" src="images/spark/streaming/dstream/spark-streaming.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="ulist">
<ul>
<li><p>
Spark Streaming + Cassandra<div class="ulist">
<ul>
<li>Spark Streaming = Stream Processor</li>
<li>Cassandra = Storage (both working and archival when applicable)</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="paragraph"><p>Spark supports a number of <em>input stream sources</em>. Each stream is served by
exactly one <em>Receiver</em> that "knows" how to deal with a specific source. It is even
possible to create custom <em>Receivers</em> to support additional stream sources.</p></div>
<div class="paragraph"><p><em>Receiver</em> combines records from a specific time interval (e.g., a few seconds) into a micro-batch
and gives each micro-batch to the Spark engine for processing.
The stream of micro-batches is called <em>Discretized Stream</em>.</p></div>
<div class="paragraph"><p>The Spark engine uses <em>Spark-Cassandra Connector</em>
to interact with Cassandra.</p></div>
</div>
</div>
</section>
<section class="slide" id="discretized-stream-dstream">
<h2>Discretized Stream&#8201;&#8212;&#8201;DStream</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="dstream" src="images/spark/streaming/dstream/dstream.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here is a closer look at the <em>Discretized Stream</em> or simply <em>DStream</em>.</p></div>
<div class="ulist">
<ul>
<li>Data stream is represented as Discretized Stream (DStream)</li>
<li>Data stream is divided into small batches of data</li>
<li>Micro-batches of data are created at regular time intervals</li>
<li>Batch interval or time step is typically between 500ms and several seconds</li>
<li>Micro-batches can be of different sizes, depending on how much data arrives at each time interval</li>
<li>Each micro-batch is represented as Resilient Distributed Dataset (RDD)</li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="dstream-transformation">
<h2>DStream Transformation</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="dstream transformation" src="images/spark/streaming/dstream/dstream-transformation.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="ulist">
<ul>
<li><em>DStream</em> is a continuous sequence of RDDs!</li>
<li>Each RDD in a DStream has lineage and fault-tolerance</li>
<li>Transformations similar to those on generic and key-value pair RDDs are applicable</li>
<li>Such transformations are called on a <em>DStream</em> and applied on each constituent RDD individually</li>
<li>There are many additional transformations and output operations that are only applicable to discretized streams, including some
that work on data from multiple RDDs in a <em>DStream</em></li>
</ul>
</div>
<div class="paragraph"><p>This example shows the result of applying the <em>filter</em> transformation on a <em>DStream</em>. It is like applying <em>filter</em>
on each RDD in the <em>DStream</em>. Note that the result is a new <em>Discretized Stream</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-architecture">
<h2>Spark Streaming Architecture</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="streaming architecture" src="images/spark/streaming/architecture/streaming-architecture.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>You should already be familiar with Spark Architecture:</p></div>
<div class="ulist">
<ul>
<li><em>Client</em>, <em>Driver</em>, <em>SparkContext</em></li>
<li><em>Master</em></li>
<li><em>Worker</em></li>
<li><em>Executor</em></li>
</ul>
</div>
<div class="paragraph"><p>Spark Streaming Architecture has additional components:</p></div>
<div class="ulist">
<ul>
<li><p>
<em>StreamingContext</em><div class="ulist">
<ul>
<li>Entry point to Spark Streaming functionality</li>
<li>Uses <em>SparkContext</em></li>
</ul>
</div></p></li>
<li><p>
<em>Data Stream Source</em><div class="ulist">
<ul>
<li>Spark supports many sources that are named on the next slide</li>
</ul>
</div></p></li>
<li><p>
<em>Receiver</em><div class="ulist">
<ul>
<li>Long task (constantly running task) that belongs to one of the <em>Executors</em></li>
<li>Creates a <em>DStream</em> from an input data stream</li>
<li>Replicates a <em>DStream</em> to the cache of another <em>Worker</em>--<em>Executor</em> by default</li>
</ul>
</div></p></li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="data-stream-sources">
<h2>Data Stream Sources</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:33%" />
<col style="width:33%" />
<col style="width:33%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Basic Sources</th>
<th class="tableblock halign-left valign-top">Advanced Sources</th>
<th class="tableblock halign-left valign-top">Custom Sources</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>File systems</li>
<li>Socket connections</li>
<li>Akka Actors</li>
</ul>
</div></div></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>Kafka</li>
<li>Flume</li>
<li>Kinesis</li>
<li>Twitter</li>
<li>ZeroMQ</li>
<li>MQTT</li>
</ul>
</div></div></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>User-defined receivers</li>
</ul>
</div></div></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Basic Sources are available through <em>StreamingContext</em> API.</p></div>
<div class="paragraph"><p>Advanced sources are available through additional libraries (utility classes).</p></div>
<div class="paragraph"><p>Spark allows extending support to custom sources as well.</p></div>
</div>
</div>
</section>
<section class="slide" id="receiver-reliability">
<h2>Receiver Reliability</h2>
<div class="ulist">
<ul>
<li><p>
Reliable Receiver<div class="ulist">
<ul>
<li>Acknowledges a reliable source that data has been received and replicated</li>
<li>A reliable source replicates its own data and is able to resend it</li>
</ul>
</div></p></li>
<li><p>
Unreliable Receiver<div class="ulist">
<ul>
<li>Does not acknowledge a source</li>
<li>Data loss is possible</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>There is exactly one <em>Receiver</em> per stream.
Different data stream sources have different receivers.</p></div>
<div class="paragraph"><p>This is a classification of receivers based on reliability.</p></div>
<div class="paragraph"><p>Note that, even if a source is reliable (can accept acknowledgments, replicates its data, can resend its data),
its corresponding receiver can be designed to be unreliable (to avoid extra complexity) when data loss is not a problem.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-first-application">
<h2>The WordCount Problem in a Stream</h2>
<div class="imageblock center">
<div class="content">
<img alt="tag cloud" src="images/spark/streaming/first-application/tag-cloud.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To demonstrate the basics of Spark Streaming, we will count words in a stream.
This problem is indeed similar to the classic <em>WordCount</em> problem except
that counting is done on micro-batches of records collected over a small time interval.
While the problem seems simple, it can be useful to assess the current trend.</p></div>
<div class="paragraph"><p>Let us assume that we are dealing with a stream of genres generated by users who access
movies on our <em>KillrVideo</em> website.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Application design steps</strong></p></div>
<div class="olist arabic">
<ol class="arabic">
<li>
Import classes
</li>
<li>
Initialize a <em>SparkConf</em> object
</li>
<li>
Initialize <em>StreamingContext</em> and <em>SparkContext</em>
</li>
<li>
Create a <em>DStream</em> object
</li>
<li>
Define computation
</li>
<li>
Start computation
</li>
</ol>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>These are steps we will take to design the application.</p></div>
</div>
</div>
</section>
<section class="slide" id="step-1-importing-classes">
<h2>Step 1: Importing Classes</h2>
<div class="paragraph"><p><strong>Necessary imports</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>// Spark
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._

// Spark Streaming
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._

// Spark-Cassandra Connector
import com.datastax.spark.connector._
import com.datastax.spark.connector.streaming._</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Step 1 imports classes that are needed to use:</p></div>
<div class="ulist">
<ul>
<li>Spark Core API</li>
<li>Spark Streaming API</li>
<li>Spark-Cassandra Connector API</li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="step-2-initializing-a-sparkconf-object">
<h2>Step 2: Initializing a SparkConf Object</h2>
<div class="paragraph"><p><strong>Connection configuration and execution properties</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val conf = new SparkConf(true)
      .setAppName("Streaming Example")
      .setMaster("spark://127.0.0.1:7077")
      .set("spark.cassandra.connection.host", "127.0.0.1")
      .set("spark.cleaner.ttl", "3600")
      .setJars(Array("your-app.jar"))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>We create a Spark configuration object and set the following properties:</p></div>
<div class="olist arabic">
<ol class="arabic">
<li>
Application name
</li>
<li>
Spark Master URL
</li>
<li>
A Cassandra host (can be any node in a Cassandra cluster as they all are peers)
</li>
<li>
<p><em>spark.cleaner.ttl</em></p>
<div class="ulist">
<ul>
<li>Duration (seconds) of how long Spark will remember any metadata (stages, tasks, cached RDDs)</li>
<li>Spark stores metadata for the duration of application lifetime; if our streaming application
runs for many weeks, months and so forth, we want to limit how much metadata is stored</li>
</ul>
</div>
</li>
<li>
An application JAR file to distribute to a cluster
</li>
</ol>
</div>
<div class="paragraph"><p>There are many other configurable properties for Applications, Spark, Spark Streaming, and Spark-Cassandra Connector.</p></div>
</div>
</div>
</section>
<section class="slide" id="step-3-initializing-streamingcontext-and-sparkcontext">
<h2>Step 3: Initializing StreamingContext and SparkContext</h2>
<div class="paragraph"><p><strong>Two alternative ways</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))
val sc  = ssc.sparkContext</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val sc  = new SparkContext(conf)
val ssc = new StreamingContext(sc, Seconds(4))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>SparkContext</em> is an entry point for Spark functionality.</p></div>
<div class="paragraph"><p><em>StreamingContext</em> is an entry point for Spark Streaming functionality.</p></div>
<div class="paragraph"><p>Approach 1:</p></div>
<div class="ulist">
<ul>
<li>Creating a <em>StreamingContext</em> object for the <em>SparkConf</em> object and the batch interval of 4 seconds</li>
<li>An underlying <em>SparkContext</em> object is created automatically and can be accessed via property <em>ssc.sparkContext</em></li>
</ul>
</div>
<div class="paragraph"><p>Approach 2:</p></div>
<div class="ulist">
<ul>
<li>Creating a <em>SparkContext</em> object for the <em>SparkConf</em> object</li>
<li>Creating a <em>StreamingContext</em> object for the <em>SparkContext</em> object and the batch interval of 4 seconds</li>
</ul>
</div>
<div class="paragraph"><p>Choosing between these two approaches is a matter of preference.</p></div>
</div>
</div>
</section>
<section class="slide" id="step-4-creating-a-dstream-object">
<h2>Step 4: Creating a DStream Object</h2>
<div class="paragraph"><p><strong>Receiving stream data via a TCP socket</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val stream = ssc.socketTextStream("127.0.0.1", 9999)</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>The simplest and quickest way to setup a socket data stream for testing purposes is
to run <code>nc -lk 9999</code> in a separate terminal and type in records manually. It is also relatively
straightforward to write a custom shell script that generates stream data.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Depending on a data stream source, you may have to use different API calls. For advanced sources, such as
Kafka, Flume, Kinesis, Twitter, ZeroMQ, and MQTT, additional libraries are required.</p></div>
<div class="paragraph"><p>In our simple example, we are creating a <em>DStream</em> object that receives data via a TCP socket defined by a host and a port.</p></div>
<div class="paragraph"><p>You can create multiple streams (<em>DStream</em> objects) as needed.</p></div>
</div>
</div>
</section>
<section class="slide" id="step-5-defining-computation">
<h2>Step 5: Defining Computation</h2>
<div class="paragraph"><p><strong>Counting words in micro-batches</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>stream.flatMap(record =&gt; record.split(" "))
      .map(word =&gt; (word,1))
      .reduceByKey(_ + _)
      .print()</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This is the most interesting part.</p></div>
<div class="paragraph"><p>This code transforms the input stream of micro-batches using
<em>flatMap</em> into a new stream where RDD elements are individual words.
It then maps words to key-value pairs and counts words using transformation <em>reduceByKey</em> resulting in a new <em>DStream</em>.
Finally, it outputs the first 10 elements of each micro-batch RDD to the screen using output operation <em>print</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="step-6-starting-computation">
<h2>Step 6: Starting Computation</h2>
<div class="paragraph"><p><strong>Computation will be terminated manually or due to an error</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>ssc.start()

ssc.awaitTermination()</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Once you have the stream source and computation defined,
this is how you start computation and wait for its termination, which can be manual or due to an error.</p></div>
</div>
</div>
</section>
<section class="slide" id="complete-example">
<h2>Complete Example</h2>
<div class="paragraph"><p><strong>StreamingExample.scala</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._
import com.datastax.spark.connector._
import com.datastax.spark.connector.streaming._

object StreamingExample {
  def main(args: Array[String]) {

     val conf = new SparkConf(true)
        .setAppName("Streaming Example")
        .setMaster("spark://127.0.0.1:7077")
        .set("spark.cassandra.connection.host", "127.0.0.1")
        .set("spark.cleaner.ttl", "3600")
        .setJars(Array("//target/scala-2.10/streaming-example_2.10-0.1.jar"))

     val ssc = new StreamingContext(conf, Seconds(4))

     val stream = ssc.socketTextStream("127.0.0.1", 9999)

     stream.flatMap(record =&gt; record.split(" "))
           .map(word =&gt; (word,1))
           .reduceByKey(_ + _)
           .print()

     ssc.start()
     ssc.awaitTermination()
  }
}</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here is a complete and very simple Spark Streaming application.</p></div>
<div class="paragraph"><p>Of course, the hard-coded values for Spark Master, Cassandra node, and stream socket should rather
be passed as parameters to the main method.</p></div>
<div class="paragraph"><p>This Scala file should be placed into a <em>src</em> folder of a project. The project can be compiled and packaged using
SBT (Scala/Simple Build Tool). The jar package is then submitted to the DSE cluster using <em>dse spark-submit</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="sample-output">
<h2>Sample Output</h2>
<div class="paragraph"><p><strong>Results of processing two micro-batches</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>-------------------------------------------
Time: 1443548140000 ms
-------------------------------------------
(Drama,22)
(Comedy,12)
(Action,16)
(Romance,9)
(Dance,11)
(Family,18)
(Adventure,7)
(Horror,10)
(Thriller,19)
(Fantasy,11)
...

-------------------------------------------
Time: 1443548144000 ms
-------------------------------------------
(Drama,11)
(Comedy,14)
(Action,13)
(Romance,2)
(Dance,9)
(Family,12)
(Adventure,9)
(Science,2)
(Thriller,3)
(Fantasy,8)
...</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>print</em> outputs first 10 elements from each micro-batch. The output is not sorted.</p></div>
<div class="paragraph"><p>Notice that the batch timestamps differ by 4000 ms or 4 seconds.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-dstream-stateless-transformations">
<h2>Operations on DStreams</h2>
<div class="paragraph"><p><strong>Two types of DStream operations</strong></p></div>
<div class="ulist">
<ul>
<li><p>
<em>Transformations</em><div class="ulist">
<ul>
<li><strong>Stateless transformations</strong></li>
<li>Stateful transformations</li>
</ul>
</div></p></li>
<li><em>Output operations</em></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark defines two types of operations on DStreams&#8201;&#8212;&#8201;transformations and output operations.</p></div>
<div class="paragraph"><p>Transformations transform input DStreams into new DStreams. They are evaluated lazily.
There are two types of DStream transformations: stateless and stateful.</p></div>
<div class="paragraph"><p>Output operations trigger computation. Their results are printed to the screen or
written to a stable storage.</p></div>
<div class="paragraph"><p>This presentation focuses on common stateless transformations.</p></div>
</div>
</div>
</section>
<section class="slide" id="how-do-stateless-transformations-work">
<h2>How Do Stateless Transformations Work?</h2>
<div class="paragraph"><p><strong>Transformation of a batch does not depend on any other batch in a DStream</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="stateless transformation" src="images/spark/streaming/dstream-stateless-transformations/stateless-transformation.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Transformation of a batch does not depend on any other batch in a DStream.</p></div>
<div class="paragraph"><p>The illustration is for a unary stateless transformation. It shows a one-to-one correspondence for
input and output batches.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformations-on-dstreams-of-generic-rdds">
<h2>Transformations on DStreams of Generic RDDs</h2>
<div class="paragraph"><p><strong>Basic transformations</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>filter</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by selecting those elements of the source DStream
on which a function<em>f</em>returns <em>true</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>map</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by applying a function <em>f</em> on each element of
the source DStream. There is a <em>one-to-one</em> correspondence between input and output elements.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>flatMap</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by applying a function <em>f</em> on each element of
the source DStream. There is a <em>one-to-many</em> correspondence between input and output elements
if <em>f</em> returns a <em>Seq</em> with more than one element.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>count</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by RDDs with a single element representing a number of elements
in respective RDDs of the source DStream.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>countByValue</strong>([<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by counting each distinct value in the source DStream.
The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>reduce</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by RDDs with a single element representing an aggregate value
computed by a function <em>f</em> from elements
in respective RDDs of the source DStream.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>union</strong>(<em>otherDStream</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed taking a union of respective RDDs in the source DStream and <em>otherDStream</em>.
The source DStream and <em>otherDStream</em> must agree on time.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>These transformations on DStreams of generic RDDs should be familiar as there are similar operations (transformations or actions) on RDDs.</p></div>
<div class="paragraph"><p>Each batch is processed independently from other batches in an input DStream by these transformations.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Advanced transformations</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>transform</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by applying an RDD-to-RDD function <em>f</em> to every RDD of the source DStream.
The function can apply any RDD transformations available in Spark, as well as custom RDD transformations.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>transformWith</strong>(<em>otherDStream</em>,<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A binary version of <em>transform</em> that works on two input DStreams.
The source DStream and <em>otherDStream</em> must agree on time.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>These are DStream transformations that are very generic and powerful.</p></div>
<div class="paragraph"><p>Not all Spark Core transformations on RDDs have equivalent Spark Streaming transformations on DStreams.
This is when <em>transform</em> and <em>transformWith</em> shine!</p></div>
<div class="paragraph"><p>For example, we can use <em>transform</em> to join each RDD in a DStream with a Cassandra table.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-joining-dstream-and-cassandra-table">
<h2>Example: Joining DStream and Cassandra Table</h2>
<div class="paragraph"><p><strong>Augmenting a DStream with data stored in Cassandra</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="joining" src="images/spark/streaming/dstream-stateless-transformations/joining.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To demonstrate a couple of the discussed transformations, let us solve the following problem.</p></div>
<div class="paragraph"><p>Given an input stream of movie identifiers (UUIDs), we need to count how many times each movie
appeared in a 4-second batch and join the result with Cassandra table <em>movies</em> to add
movie titles and years to a transformed DStream.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Using <em>countByValue</em> and <em>transform</em></strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream = ssc.socketTextStream(streamHost, 9999)
                .countByValue()

val movies = ssc.cassandraTable("killr_video","movies")
                .select("movie_id","title","release_year")
                .as( (id:UUID, t:String, y:Int)=&gt;(id.toString(),(t,y)) )
                .partitionBy(new HashPartitioner(2*ssc.sparkContext.defaultParallelism))
                .cache


stream.transform(rdd =&gt; rdd.join(movies).map{case(id,(c,(t,y))) =&gt; (id,t,y,c)})
      .print</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Counting is done by transformation <em>countByValue</em>.</p></div>
<div class="paragraph"><p>RDD <em>movies</em> is pre-partitioned and cached because it is used in many joins with different RDD batches in the DStream.</p></div>
<div class="paragraph"><p>The join is performed inside of <em>transform</em>.</p></div>
<div class="paragraph"><p>Finally, output operation <em>print</em> triggers computation and displays results to the screen.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformations-on-dstreams-of-pair-rdds">
<h2>Transformations on DStreams of Pair RDDs</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:50%" />
<col style="width:50%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>mapValues</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top" rowspan="5"><p class="tableblock">A new DStream is formed by applying the corresponding transformation to each Pair RDD of the source DStream.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>flatMapValues</strong>(<em>f</em>)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>reduceByKey</strong>(<em>f</em>,[<em>numTasks</em>])</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>groupByKey</strong>([<em>numTasks</em>])</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>combineByKey</strong>(
<em>createCombinerF</em>,
<em>mergeValueF</em>,
<em>mergeCombinersF</em>,
[<em>numTasks</em>])</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>cogroup</strong>(<em>otherDStream</em>, [<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top" rowspan="4"><p class="tableblock">A new DStream is formed by applying the corresponding binary transformation on
Pair RDDs from the source DStream and  <em>otherDStream</em>. The source DStream and <em>otherDStream</em> must agree on time.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>join</strong>(<em>otherDStream</em>, [<em>numTasks</em>])</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>leftOuterJoin</strong>(<em>otherDStream</em>, [<em>numTasks</em>])</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>rightOuterJoin</strong>(<em>otherDStream</em>, [<em>numTasks</em>])</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>These transformations on DStreams of Key-Value Pair RDDs should be familiar as there are similar operations (transformations or actions) on Pair RDDs.</p></div>
<div class="paragraph"><p>Each batch is processed independently from other batches in an input DStream by these transformations.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-joining-two-dstreams">
<h2>Example: Joining Two DStreams</h2>
<div class="paragraph"><p><strong>Aggregating data from two DStreams</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="joining streams" src="images/spark/streaming/dstream-stateless-transformations/joining-streams.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To demonstrate a couple of the discussed transformations, let us solve the following problem.</p></div>
<div class="paragraph"><p>We have two streams that are coming either from different sources or from the same source but the stream
is split into two streams to have better scalability with two receivers.</p></div>
<div class="paragraph"><p>Given two input streams of movie identifiers (UUIDs), we need to pre-aggregate how many times each movie
appeared in a 4-second batch, join respective batches from the streams, and do final aggregation (summation).</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Using <em>countByValue</em>, <em>join</em> and <em>mapValues</em></strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream1 = ssc.socketTextStream(streamHost, 9999)
val stream2 = ssc.socketTextStream(streamHost, 9989)

stream1.countByValue()
       .join(stream2.countByValue())
       .mapValues{case (v1,v2) =&gt; v1 + v2}
       .print</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Counting is done by transformation <em>countByValue</em>, resulting in Pair RDDs.</p></div>
<div class="paragraph"><p>The join is performed by transformation <em>join</em>.</p></div>
<div class="paragraph"><p>Summation of counts from two streams is done inside of transformation <em>mapValues</em>.</p></div>
<div class="paragraph"><p>Finally, output operation <em>print</em> triggers computation and displays results to the screen.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-dstream-stateful-transformations">
<h2>Operations on DStreams</h2>
<div class="paragraph"><p><strong>Two types of DStream operations</strong></p></div>
<div class="ulist">
<ul>
<li><p>
<em>Transformations</em><div class="ulist">
<ul>
<li>Stateless transformations</li>
<li><p>
Stateful transformations<div class="ulist">
<ul>
<li><strong>Transformation <em>updateStateByKey</em></strong></li>
<li>Window transformations</li>
</ul>
</div></p></li>
</ul>
</div></p></li>
<li><em>Output operations</em></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark defines two types of operations on DStreams&#8201;&#8212;&#8201;transformations and output operations.</p></div>
<div class="paragraph"><p>Transformations transform input DStreams into new DStreams. They are evaluated lazily.
There are two types of DStream transformations: stateless and stateful.</p></div>
<div class="paragraph"><p>Stateful transformations include <em>updateStateByKey</em> and window transformations.</p></div>
<div class="paragraph"><p>Output operations trigger computation. Their results are printed to the screen or
written to a stable storage.</p></div>
<div class="paragraph"><p>This presentation focuses on stateful transformation <em>updateStateByKey</em>. Stateful window transformations
will be discussed in a separate presentation.</p></div>
</div>
</div>
</section>
<section class="slide" id="stateful-transformations">
<h2>Stateful Transformations</h2>
<div class="paragraph"><p><strong>Maintaining a "state" to combine data across multiple batches in a DStream</strong></p></div>
<div class="ulist">
<ul>
<li>Transformation <em>updateStateByKey</em></li>
<li>Window transformations (discussed separately)</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Stateful transformations have to combine data from multiple batches in a DStream
to generate results. To do that efficiently, they maintain a state or "remember" previously seen data that can
be combined with most current data.</p></div>
<div class="paragraph"><p>The main stateful transformation we will discuss is <em>updateStateByKey</em>.
Window transformations will be introduced
in a separate presentation.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformation-em-updatestatebykey-em">
<h2>Transformation <em>updateStateByKey</em></h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>updateStateByKey</strong>(<em>updateF</em>, [<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new "state" DStream is formed by
applying a state update function <em>updateF</em> on the previous state of the key and
the new values of each key in the source DStream of Pair RDDs. The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>def updateF( newValues: Seq[stateType], oldState: Option[stateType]) : Option[stateType]
{
  //Compute and return a new state for a key
}</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>updateStateByKey</em> is used to maintain arbitrary state data for each key,
while continuously updating it with new information from incoming batches.</p></div>
<div class="paragraph"><p><em>stateType</em> is an arbitrary data type.</p></div>
<div class="paragraph"><p><em>updateF</em> is a function that, for a given key,
updates the state using the previous state and the new values from input stream for this key</p></div>
</div>
</div>
</section>
<section class="slide" id="important-requirement-for-em-updatestatebykey-em">
<h2>Important Requirement for <em>updateStateByKey</em></h2>
<div class="paragraph"><p><strong>Checkpointing must be enabled</strong></p></div>
<div class="ulist">
<ul>
<li>Checkpointing is a fault-tolerance mechanism in Spark Streaming</li>
<li>Checkpointing requires Spark to periodically save metadata and data into CFS</li>
<li>Configuring a checkpoint directory</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

// ...

ssc.checkpoint("checkpoint_directory")

ssc.start()
ssc.awaitTermination()</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Checkpointing is a fault-tolerance mechanism in Spark Streaming to avoid excessive re-computation of
previously processed data due to a failure.
Checkpointing saves RDDs generated by some stateful transformations that, otherwise,
would have to be recomputed through a very long lineage chain.</p></div>
<div class="paragraph"><p>Besides <em>updateStateByKey</em>, many but not all window transformations require mandatory checkpointing.
Window transformations are discussed in a separate presentation.</p></div>
<div class="paragraph"><p>Specifying a checkpoint directory enables checkpointing. A single statement is required before starting computation.</p></div>
</div>
</div>
</section>
<section class="slide" id="running-count-of-distinct-elements-in-a-dstream">
<h2>Running Count of Distinct Elements in a DStream</h2>
<div class="paragraph"><p><strong>Maintaining a state for each movie across stream batches</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="running count" src="images/spark/streaming/dstream-stateful-transformations/running-count.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To demonstrate transformation <em>updateStateByKey</em>, let us count how many times each movie identifier (UUID) was seen up to the current moment.
To process the most current batch, we have to have counts from all previous batches (aka state).</p></div>
<div class="paragraph"><p>Transformation <em>countByValue</em> is stateless. It simply counts movies in a current batch and returns a DStream
of Pair RDDs.</p></div>
<div class="paragraph"><p>Transformation <em>updateStateByKey</em> is stateful. For each movie, it adds a count from the current batch and
a count from the previous state to generate a new state. For example, 10 + 4 = 14 as shown in the figure.
<em>updateStateByKey</em> generates a "state" DStream.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Defining the state update function and computation</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>def updateMovieCount(newValues: Seq[Long], oldCount: Option[Long])
: Option[Long] =
{
   if (newValues.isEmpty) Some(oldCount.getOrElse(0L))
   else Some(oldCount.getOrElse(0L) + newValues(0))
}</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream = ssc.socketTextStream(streamHost, 9999)

stream.countByValue()
      .updateStateByKey[Long](updateMovieCount _)
      .print</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>updateMovieCount</em> is executed on each individual movie UUID that is known to exist in a state or appears in
a current batch. It takes new values (in our example, we can have at most one value due to pre-aggregation with <em>countByValue</em>)
for the movie-key from the current batch and combines them with the old state-count.
It returns a new state-count for the movie-key.</p></div>
<div class="paragraph"><p><em>countByKey</em> generates a DStream of key-value pairs, where <em>key</em> is a movie UUID and <em>value</em> is how many times
this UUID appeared in a current input batch.</p></div>
<div class="paragraph"><p><em>updateStateByKey</em> computes a new state from a new input batch and a previous state. The result is a "state" DStream.</p></div>
<div class="paragraph"><p><em>print</em> outputs first 10 elements of each batch to the screen.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Sample output</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>-------------------------------------------
Time: 1443846676000 ms
-------------------------------------------
(3391c072-af52-4d3d-b909-ea6dfdfe475b,467)
(f7b1982a-3614-4ada-b5f1-a63291d10ad8,470)
(81074a01-602d-4d4c-a57b-8ff713042478,479)
(349cc28f-2a20-4e4b-862a-a4ce9f2916e2,474)
(f8729686-a166-48b5-9609-698592482b60,460)
(87032a77-0ccd-4161-ae69-44d70ce4c7da,465)
(6284b27a-a4ff-4118-b006-6327d438a1aa,433)
(f2c8c1f4-7c91-4644-bfc4-fa578957ff2d,479)
(e18a3eb1-a4aa-497b-8427-39677b4e92c7,498)
(d3c8af3f-206b-45e9-a2bd-38eaeb05e389,435)
...

-------------------------------------------
Time: 1443846680000 ms
-------------------------------------------
(3391c072-af52-4d3d-b909-ea6dfdfe475b,469)
(f7b1982a-3614-4ada-b5f1-a63291d10ad8,471)
(81074a01-602d-4d4c-a57b-8ff713042478,483)
(349cc28f-2a20-4e4b-862a-a4ce9f2916e2,483)
(f8729686-a166-48b5-9609-698592482b60,464)
(87032a77-0ccd-4161-ae69-44d70ce4c7da,469)
(6284b27a-a4ff-4118-b006-6327d438a1aa,436)
(f2c8c1f4-7c91-4644-bfc4-fa578957ff2d,481)
(e18a3eb1-a4aa-497b-8427-39677b4e92c7,505)
(d3c8af3f-206b-45e9-a2bd-38eaeb05e389,440)
...</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Results from two batches in a "state" DStream.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-dstream-window-transformations">
<h2>Operations on DStreams</h2>
<div class="paragraph"><p><strong>Two types of DStream operations</strong></p></div>
<div class="ulist">
<ul>
<li><p>
<em>Transformations</em><div class="ulist">
<ul>
<li>Stateless transformations</li>
<li><p>
Stateful transformations<div class="ulist">
<ul>
<li>Transformation <em>updateStateByKey</em></li>
<li><strong>Window transformations</strong></li>
</ul>
</div></p></li>
</ul>
</div></p></li>
<li><em>Output operations</em></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark defines two types of operations on DStreams&#8201;&#8212;&#8201;transformations and output operations.</p></div>
<div class="paragraph"><p>Transformations transform input DStreams into new DStreams. They are evaluated lazily.
There are two types of DStream transformations: stateless and stateful.</p></div>
<div class="paragraph"><p>Stateful transformations include <em>updateStateByKey</em> and window transformations.</p></div>
<div class="paragraph"><p>Output operations trigger computation. Their results are printed to the screen or
written to a stable storage.</p></div>
<div class="paragraph"><p>This presentation focuses on window transformations.</p></div>
</div>
</div>
</section>
<section class="slide" id="how-do-window-transformations-work">
<h2>How Do Window Transformations Work?</h2>
<div class="paragraph"><p><strong>Window transformation computes a single RDD out of all windowed RDDs</strong></p></div>
<div class="ulist">
<ul>
<li>Window length or duration: <em>windowLength</em> = <em>n</em> x <em>batchInterval</em> (e.g., <em>n</em> = <em>2</em>)</li>
<li>Window sliding interval or sliding duration: <em>slideInterval</em> = <em>m</em> x <em>batchInterval</em> (e.g., <em>m</em> = <em>1</em>)</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="window transformation 1" src="images/spark/streaming/dstream-window-transformations/window-transformation-1.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Window transformations combine data from all windowed batches in a DStream
to generate a new batch.</p></div>
<div class="paragraph"><p>A window is defined through these characteristics:</p></div>
<div class="ulist">
<ul>
<li><p>
Window has length or duration<div class="ulist">
<ul>
<li>Specified in time units&#8201;&#8212;&#8201;must be a multiple of a DStream batch interval</li>
<li><em>windowLength</em> = <em>n</em> x <em>batchInterval</em></li>
<li>Defines how many batches are included in windowed transformation</li>
<li>Example in the slide: <em>windowLength</em> = <em>2</em> x <em>batchInterval</em></li>
</ul>
</div></p></li>
<li><p>
Window has sliding interval or sliding duration<div class="ulist">
<ul>
<li>Specified in time units&#8201;&#8212;&#8201;must be a multiple of a DStream batch interval</li>
<li><em>slideInterval</em> = <em>m</em> x <em>batchInterval</em></li>
<li>Defines by how many batches to slide a window or how frequently to compute a windowed transformation</li>
<li>Example in the slide: <em>slideInterval</em> = <em>1</em> x <em>batchInterval</em></li>
</ul>
</div></p></li>
</ul>
</div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Window transformation computes a single RDD out of all windowed RDDs</strong></p></div>
<div class="ulist">
<ul>
<li>Window length or duration: <em>windowLength</em> = <em>n</em> x <em>batchInterval</em> (e.g., <em>n</em> = <em>2</em>)</li>
<li>Window sliding interval or sliding duration: <em>slideInterval</em> = <em>m</em> x <em>batchInterval</em> (e.g., <em>m</em> = <em>1</em>)</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="window transformation 2" src="images/spark/streaming/dstream-window-transformations/window-transformation-2.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">

</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Window transformation computes a single RDD out of all windowed RDDs</strong></p></div>
<div class="ulist">
<ul>
<li>Window length or duration: <em>windowLength</em> = <em>n</em> x <em>batchInterval</em> (e.g., <em>n</em> = <em>2</em>)</li>
<li>Window sliding interval or sliding duration: <em>slideInterval</em> = <em>m</em> x <em>batchInterval</em> (e.g., <em>m</em> = <em>1</em>)</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="window transformation 3" src="images/spark/streaming/dstream-window-transformations/window-transformation-3.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">

</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Window transformation computes a single RDD out of all windowed RDDs</strong></p></div>
<div class="ulist">
<ul>
<li>Window length or duration: <em>windowLength</em> = <em>n</em> x <em>batchInterval</em> (e.g., <em>n</em> = <em>2</em>)</li>
<li>Window sliding interval or sliding duration: <em>slideInterval</em> = <em>m</em> x <em>batchInterval</em> (e.g., <em>m</em> = <em>1</em>)</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="window transformation 4" src="images/spark/streaming/dstream-window-transformations/window-transformation-4.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">

</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Window transformation computes a single RDD out of all windowed RDDs</strong></p></div>
<div class="ulist">
<ul>
<li>Window length or duration: <em>windowLength</em> = <em>n</em> x <em>batchInterval</em> (e.g., <em>n</em> = <em>2</em>)</li>
<li>Window sliding interval or sliding duration: <em>slideInterval</em> = <em>m</em> x <em>batchInterval</em> (e.g., <em>m</em> = <em>1</em>)</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="window transformation 5" src="images/spark/streaming/dstream-window-transformations/window-transformation-5.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">

</div>
</div>
</section>
<section class="slide" id="choosing-a-window-length-and-a-sliding-interval">
<h2>Choosing a Window Length and a Sliding Interval</h2>
<div class="paragraph"><p><strong>General insights</strong></p></div>
<div class="ulist">
<ul>
<li><p>
Window length<div class="ulist">
<ul>
<li>Minute-scale window length is reasonable</li>
<li>Hour-scale window length is not recommended</li>
</ul>
</div></p></li>
<li><p>
Window sliding interval<div class="ulist">
<ul>
<li>Should meet application requirements</li>
<li>Should meet performance requirements</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="ulist">
<ul>
<li><p>
Window length<div class="ulist">
<ul>
<li>Minute-scale window length is reasonable</li>
<li><p>
Hour-scale window length is not recommended<div class="ulist">
<ul>
<li>Takes longer to process</li>
<li>Requires large batch intervals for stable processing</li>
<li>Requires a lot of lineage to recover from a failure</li>
<li>Alternative Solution: aggregate data in Cassandra instead</li>
</ul>
</div></p></li>
</ul>
</div></p></li>
<li><p>
Window sliding interval<div class="ulist">
<ul>
<li>Should meet application requirements</li>
<li>Should meet performance requirements (smaller sliding intervals require more computation)</li>
</ul>
</div></p></li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="transformations-on-dstreams-of-generic-rdds-2">
<h2>Transformations on DStreams of Generic RDDs</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>window</strong>( <em>windowLength</em>, [<em>slideInterval</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by combining all windowed batches of the source DStream into
a single batch in a transformed DStream. The default <em>slideInterval</em> is the batch interval of the source DStream.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>countByWindow</strong>( <em>windowLength</em>, <em>slideInterval</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by RDDs with a single element representing a number of elements
in all windowed batches of the source DStream.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>countByValueAndWindow</strong>( <em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by RDDs containing a count for each distinct element
in all windowed batches of the source DStream. The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>reduceByWindow</strong>( <em>f</em>, <em>windowLength</em>, <em>slideInterval</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by RDDs with a single element generated by aggregating elements
in all windowed batches of the source DStream using an associative reduce function <em>f</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>reduceByWindow</strong>( <em>f</em>, <em>invF</em>, <em>windowLength</em>, <em>slideInterval</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A more efficient version of <em>reduceByWindow</em> with an inverse reduce function <em>invF</em> that allows incremental reduction.
While <em>f</em> is used to reduce new values entering a window, <em>invF</em> is used to "inverse reduce" old values leaving the same window.
This transformation is applicable when a reduce function is invertible (e.g., + is invertible with ).</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>window</em> is the simplest and most generic window transformation that can be used to implement
other window transformations. It simply creates a new DStream with batches whose intervals equal to a window length.</p></div>
<div class="paragraph"><p><em>countByWindow</em> is used to count elements falling into a window.</p></div>
<div class="paragraph"><p><em>countByValueAndWindow</em> is used to find how many times a value appears in a window.</p></div>
<div class="paragraph"><p><em>reduceByWindow</em> is for aggregating all elements in a window using a reduce function. Two versions of this
transformation exist: one with a reduce function only and one with both reduce and its  inverse function.
While the incremental version is, in general, more efficient,
it is only applicable when a reduce function has an inverse function.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-counting-elements-in-a-window">
<h2>Example: Counting Elements in a Window</h2>
<div class="paragraph"><p><strong>Three alternative solutions</strong></p></div>
<div class="listingblock">
<div class="title"><em>Using countByWindow</em></div>
<div class="content">
<pre class="CodeRay"><code>stream.countByWindow(Seconds(60), Seconds(4)).print()</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Using reduceByWindow</em></div>
<div class="content">
<pre class="CodeRay"><code>stream.map(m =&gt; 1).reduceByWindow({(a,b) =&gt; a+b}, Seconds(60), Seconds(4)).print()</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Using incremental reduceByWindow</em></div>
<div class="content">
<pre class="CodeRay"><code>stream.map(m =&gt; 1).reduceByWindow({(a,b) =&gt; a+b}, {(a,b) =&gt; a-b}, Seconds(60), Seconds(4)).print()</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Sample output</em></div>
<div class="content">
<pre class="CodeRay"><code>-------------------------------------------
Time: 1444015700000 ms
-------------------------------------------
2921

-------------------------------------------
Time: 1444015704000 ms
-------------------------------------------
2892

-------------------------------------------
Time: 1444015708000 ms
-------------------------------------------
2974</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Solving this simple problem to demonstrate three window transformations on DStreams
of generic RDDs.</p></div>
<div class="paragraph"><p>Notice the 60-second <em>windowLength</em> and 4-second <em>slideInterval</em>. The <em>batchInterval</em> is also 4 seconds (not shown in the slide).</p></div>
<div class="paragraph"><p>Also, notice that the total count is not necessarily increasing. When a window slides, some batches are added and some
batches are removed from a window, so the count may vary.</p></div>
</div>
</div>
</section>
<section class="slide" id="how-do-incremental-window-transformations-work">
<h2>How Do Incremental Window Transformations Work?</h2>
<div class="paragraph"><p><strong>reduceByWindow({(a,b) &#8658; a+b}, {(a,b) &#8658; a-b}, 2 x batchInterval, batchInterval)</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="incremental transformation 1" src="images/spark/streaming/dstream-window-transformations/incremental-transformation-1.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This example uses transformation <em>reduceByWindow</em> with reduce function <strong>+</strong> and inverse
reduce function <strong>-</strong> to compute a sum of all elements in windowed batches. The incremental transformation
relies on a "state" that remembers previously computed results.</p></div>
<div class="paragraph"><p>Let us see how the reduce function, its inverse, and "state" are used.</p></div>
<div class="paragraph"><p>Keep in mind that for this simple illustration, using an incremental transformation will not be more
beneficial than using a non-incremental transformations. The incremental effect becomes noticeable
for transformations with lengthy windows with short slide intervals.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>reduceByWindow({(a,b) &#8658; a+b}, {(a,b) &#8658; a-b}, 2 x batchInterval, batchInterval)</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="incremental transformation 2" src="images/spark/streaming/dstream-window-transformations/incremental-transformation-2.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Summation for a batch that entered the window PLUS previous window result/state MINUS summation for a batch that left the window.</p></div>
<div class="paragraph"><p>Compute 1+1+1 (from RDD1) = 3.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>reduceByWindow({(a,b) &#8658; a+b}, {(a,b) &#8658; a-b}, 2 x batchInterval, batchInterval)</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="incremental transformation 3" src="images/spark/streaming/dstream-window-transformations/incremental-transformation-3.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Summation for a batch that entered the window PLUS previous window result/state MINUS summation for a batch that left the window.</p></div>
<div class="paragraph"><p>Compute 1+1+1 (from RDD2) + 3 (previous state) = 6.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>reduceByWindow({(a,b) &#8658; a+b}, {(a,b) &#8658; a-b}, 2 x batchInterval, batchInterval)</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="incremental transformation 4" src="images/spark/streaming/dstream-window-transformations/incremental-transformation-4.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Summation for a batch that entered the window PLUS previous window result/state MINUS summation for a batch that left the window.</p></div>
<div class="paragraph"><p>Compute 1+1+1 (from RDD3) + 6 (previous state) -1-1-1 (from RDD1) = 6.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>reduceByWindow({(a,b) &#8658; a+b}, {(a,b) &#8658; a-b}, 2 x batchInterval, batchInterval)</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="incremental transformation 5" src="images/spark/streaming/dstream-window-transformations/incremental-transformation-5.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Summation for a batch that entered the window PLUS previous window result/state MINUS summation for a batch that left the window.</p></div>
<div class="paragraph"><p>Compute 1+1+1+1 (from RDD4) + 6 (previous state) -1-1-1 (from RDD2) = 7.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformations-on-dstreams-of-pair-rdds-2">
<h2>Transformations on DStreams of Pair RDDs</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>reduceByKeyAndWindow</strong>( <em>f</em>, <em>windowLength</em>,<em>slideInterval</em>, [<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by aggregating values with the same key
in all windowed batches of the source DStream using an associative reduce function <em>f</em>.
The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>reduceByKeyAndWindow</strong>( <em>f</em>,<em>invF</em>, <em>windowLength</em>,<em>slideInterval</em>, [<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">An incremental version of <em>reduceByKeyAndWindow</em> with an inverse reduce function.
The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>groupByKeyAndWindow</strong>( <em>windowLength</em>,<em>slideInterval</em>, [<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by grouping values with the same key
in all windowed batches of the source DStream.
The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>These transformations on DStreams of Key-Value Pair RDDs would be conceptually equivalent to applying
<em>reduceByKey</em> and <em>groupByKey</em>, respectively, on each window in a DStream.</p></div>
<div class="paragraph"><p>They generate new DStreams of Key-Value Pair RDDs.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-counting-elements-per-key-in-a-window">
<h2>Example: Counting Elements per Key in a Window</h2>
<div class="paragraph"><p><strong>Two alternative solutions</strong></p></div>
<div class="listingblock">
<div class="title"><em>Using reduceByKeyAndWindow</em></div>
<div class="content">
<pre class="CodeRay"><code>stream.countByValue()
      .reduceByKeyAndWindow({(a:Long,b:Long) =&gt; a+b}, Seconds(60), Seconds(4))
      .print()</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Using incremental reduceByKeyAndWindow</em></div>
<div class="content">
<pre class="CodeRay"><code>stream.countByValue()
      .reduceByKeyAndWindow({(a,b) =&gt; a+b}, {(a,b) =&gt; a-b}, Seconds(60), Seconds(4))
      .print()</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Sample output</em></div>
<div class="content">
<pre class="CodeRay"><code>-------------------------------------------
Time: 1444021448000 ms
-------------------------------------------
(3391c072-af52-4d3d-b909-ea6dfdfe475b,94)
(f7b1982a-3614-4ada-b5f1-a63291d10ad8,71)
(81074a01-602d-4d4c-a57b-8ff713042478,60)
(349cc28f-2a20-4e4b-862a-a4ce9f2916e2,78)
(f8729686-a166-48b5-9609-698592482b60,62)
(87032a77-0ccd-4161-ae69-44d70ce4c7da,85)
(6284b27a-a4ff-4118-b006-6327d438a1aa,73)
(f2c8c1f4-7c91-4644-bfc4-fa578957ff2d,73)
(e18a3eb1-a4aa-497b-8427-39677b4e92c7,62)
(d3c8af3f-206b-45e9-a2bd-38eaeb05e389,64)
...

-------------------------------------------
Time: 1444021452000 ms
-------------------------------------------
(3391c072-af52-4d3d-b909-ea6dfdfe475b,98)
(f7b1982a-3614-4ada-b5f1-a63291d10ad8,78)
(81074a01-602d-4d4c-a57b-8ff713042478,66)
(349cc28f-2a20-4e4b-862a-a4ce9f2916e2,82)
(f8729686-a166-48b5-9609-698592482b60,67)
(87032a77-0ccd-4161-ae69-44d70ce4c7da,88)
(6284b27a-a4ff-4118-b006-6327d438a1aa,75)
(f2c8c1f4-7c91-4644-bfc4-fa578957ff2d,77)
(e18a3eb1-a4aa-497b-8427-39677b4e92c7,66)
(d3c8af3f-206b-45e9-a2bd-38eaeb05e389,69)
...</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Solving this simple problem to demonstrate two window transformations on DStreams
of Pair RDDs.</p></div>
<div class="paragraph"><p>Notice the 60-second <em>windowLength</em> and 4-second <em>slideInterval</em>. The <em>batchInterval</em> is also 4 seconds (not shown in the slide).</p></div>
</div>
</div>
</section>
<section class="slide" id="important-checkpointing-requirements">
<h2>Important Checkpointing Requirements</h2>
<div class="paragraph"><p><strong>Checkpointing is mandatory for <em>countByWindow</em>, <em>countByValueAndWindow</em>, and incremental versions of <em>reduceByWindow</em> and <em>reduceByKeyAndWindow</em></strong></p></div>
<div class="ulist">
<ul>
<li>Checkpointing is a fault-tolerance mechanism in Spark Streaming</li>
<li>Checkpointing requires Spark to periodically save metadata and data into CFS</li>
<li>Configuring a checkpoint directory</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

// ...

ssc.checkpoint("checkpoint_directory")

ssc.start()
ssc.awaitTermination()</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Checkpointing is a fault-tolerance mechanism in Spark Streaming to avoid excessive re-computation of
previously processed data due to a failure.
Checkpointing saves RDDs generated by some stateful transformations that, otherwise,
would have to be recomputed through a very long lineage chain.</p></div>
<div class="paragraph"><p>Checkpointing is mandatory for many window transformations (see the slide), as well as
stateful transformation <em>updateStateByKey</em> (discussed in a separate presentation).</p></div>
<div class="paragraph"><p>Enabling checkpointing has a computational overhead and results in slower stream processing performance. However
checkpointing makes failure recovery much more efficient. It is all about trade-offs and
requirements of your specific use case.</p></div>
<div class="paragraph"><p>Specifying a checkpoint directory enables checkpointing. A single statement is required before starting computation.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-dstream-output-operations">
<h2>Operations on DStreams</h2>
<div class="paragraph"><p><strong>Two types of DStream operations</strong></p></div>
<div class="ulist">
<ul>
<li><p>
<em>Transformations</em><div class="ulist">
<ul>
<li>Stateless transformations</li>
<li>Stateful transformations</li>
</ul>
</div></p></li>
<li><p>
<em>Output operations</em><div class="ulist">
<ul>
<li><strong>Spark output operations</strong></li>
<li><strong>Spark-Cassandra Connector output operations</strong></li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark defines two types of operations on DStreams&#8201;&#8212;&#8201;transformations and output operations.</p></div>
<div class="paragraph"><p>Transformations transform input DStreams into new DStreams. They are evaluated lazily.
There are two types of DStream transformations: stateless and stateful.</p></div>
<div class="paragraph"><p>Output operations trigger computation. Their results are printed to the screen or
written to a stable storage. We can distinguish between output operations provided by Spark and
Spark-Cassandra Connector.</p></div>
<div class="paragraph"><p>This presentation focuses on output operations.</p></div>
</div>
</div>
</section>
<section class="slide" id="common-output-operations">
<h2>Common Output Operations</h2>
<div class="paragraph"><p><strong>Spark and Spark-Cassandra Connector output operations</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:40%" />
<col style="width:60%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Output Operation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>print</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Prints the first 10 elements of each RDD generated in the source DStream.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>saveAsTextFiles</strong>(<em>prefix</em>,[<em>suffix</em>])
<strong>saveAsObjectFiles</strong>(<em>prefix</em>,[<em>suffix</em>])
<strong>saveAsHadoopFiles</strong>(<em>prefix</em>,[<em>suffix</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Saves each RDD in the source DStream as a text file, Sequence file, or Hadoop file, respectively.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>foreachRDD</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Applies a function <em>f</em> to each RDD in the source DStream and pushes data to an external system.
The function can use any transformations or actions on its input RDDs.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>saveToCassandra</strong>(<em>keyspace</em>,<em>table</em>,[<em>columns</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Saves each RDD in the source DStream into a Cassandra table. This
output operation is defined in <em>Spark-Cassandra Connector</em>.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here are some frequently used output operations that are evaluated in parallel on the source DStream
and whose results are either printed or pushed to a file or a database.</p></div>
<div class="paragraph"><p>You might have seen <em>print</em> before&#8201;&#8212;&#8201;it simply outputs the first 10 elements of each RDD in the source DStream.</p></div>
<div class="paragraph"><p>We will demonstrate <em>saveToCassandra</em> and <em>foreachRDD</em> in the following examples.</p></div>
</div>
</div>
</section>
<section class="slide" id="saving-a-raw-data-stream-into-cassandra">
<h2>Saving a Raw Data Stream into Cassandra</h2>
<div class="paragraph"><p><strong>Storying individual movie clicks</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="saving raw" src="images/spark/streaming/dstream-output-operations/saving-raw.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Let us solve a useful problem of storying a raw stream of movie clicks into a Cassandra table.</p></div>
<div class="paragraph"><p>Given an input stream of movie identifiers (UUIDs), we need to generate a <em>click_id</em> (TIMEUUID) for each movie identifier
and save all data into table <em>clicks_by_movie</em>.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>STEP 1: Creating a table</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val cc = com.datastax.spark.connector.cql.CassandraConnector(conf)

cc.withSessionDo { session =&gt;
  session.execute("CREATE TABLE IF NOT EXISTS " +
                  "killr_video.clicks_by_movie ( " +
                  "movie_id UUID, " +
                  "click_id TIMEUUID, " +
                  "PRIMARY KEY(movie_id,click_id));")
}</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To create the <em>clicks_by_movie</em> table in our streaming application, we use <em>CassandraConnector</em>
that allows executing CQL statements like CREATE TABLE in this example.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>STEP 2: Creating, augmenting and saving a data stream</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream = ssc.socketTextStream(streamHost, 9999)

stream.map(m =&gt; (java.util.UUID.fromString(m),
                 com.datastax.driver.core.utils.UUIDs.timeBased()))
      .saveToCassandra("killr_video","clicks_by_movie",
                       SomeColumns("movie_id","click_id"))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Notice how we deal with UUIDs and TIMEUUIDs in transformation <em>map</em>.</p></div>
<div class="paragraph"><p>Each RDD in the stream is stored into Cassandra using output operation <em>saveToCassandra</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="saving-an-aggregated-data-stream-into-cassandra">
<h2>Saving an Aggregated Data Stream into Cassandra</h2>
<div class="paragraph"><p><strong>Storying click aggregates per movie</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="saving aggregates" src="images/spark/streaming/dstream-output-operations/saving-aggregates.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Let us solve a useful problem of aggregating data in a stream of movie clicks and storying results
into a Cassandra table.</p></div>
<div class="paragraph"><p>Given an input stream of movie identifiers (UUIDs), we need to
pre-compute how many times each movie appears in a batch
and save the aggregates into table <em>clicks_per_movie</em>.</p></div>
<div class="paragraph"><p>Notice that <em>total_clicks</em> is a column of
type <em>COUNTER</em>&#8201;&#8212;&#8201;aggregates from different batches for each movie will be added up by Cassandra!</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>STEP 1: Creating a table</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val cc = com.datastax.spark.connector.cql.CassandraConnector(conf)

cc.withSessionDo { session =&gt;
  session.execute("CREATE TABLE IF NOT EXISTS " +
                  "killr_video.clicks_per_movie ( " +
                  "movie_id UUID, " +
                  "total_clicks COUNTER, " +
                  "PRIMARY KEY(movie_id));")
}</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To create the <em>clicks_per_movie</em> table in our streaming application, we use <em>CassandraConnector</em>
that allows executing CQL statements like CREATE TABLE in this example.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>STEP 2: Creating, pre-aggregating and saving a data stream</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream = ssc.socketTextStream(streamHost, 9999)

stream.map(id =&gt; (java.util.UUID.fromString(id),1))
      .reduceByKey(_ + _)
      .saveToCassandra("killr_video","clicks_per_movie",
                       SomeColumns("movie_id","total_clicks"))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>map</em> is used to transform a stream of generic RDDs to a stream of key-value Pair RDDs.</p></div>
<div class="paragraph"><p>Pre-aggregation for each batch is done in transformation <em>reduceByKey</em>.</p></div>
<div class="paragraph"><p>Each RDD in the stream is stored into Cassandra using output operation <em>saveToCassandra</em>. Cassandra completes
the final stage of aggregation using counters.</p></div>
</div>
</div>
</section>
<section class="slide" id="the-role-of-em-foreachrdd-em-output-operation">
<h2>The Role of <em>foreachRDD</em> Output Operation</h2>
<div class="paragraph"><p><strong>Most generic output operation</strong></p></div>
<div class="listingblock">
<div class="title"><em>Saving an Aggregated Data Stream into Cassandra</em></div>
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream = ssc.socketTextStream(streamHost, 9999)

stream.foreachRDD(rdd =&gt;
       rdd.map(id =&gt; (java.util.UUID.fromString(id),1))
          .reduceByKey(_ + _)
          .saveToCassandra("killr_video","clicks_per_movie",
                           SomeColumns("movie_id","total_clicks"))
       )</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="ulist">
<ul>
<li><p>
<em>foreachRDD</em> is the most generic output operation that allows
using any Spark Core transformations and actions on RDDs inside of DStream.<div class="ulist">
<ul>
<li>Not all Spark Core operations on RDDs have equivalent Spark Streaming operations on DStreams -
this is when <em>foreachRDD</em> is the solution!</li>
<li><em>foreachRDD</em> can be used to define custom computation on RDDs.</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="paragraph"><p>The code solves the previous problem using <em>foreachRDD</em>. Note that, this time, <em>map</em>, <em>reduceByKey</em>, and
<em>saveToCassandra</em> are RDD operations and not DStream operations like in the previous example.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-checkpointing-and-recovery">
<h2>Checkpointing</h2>
<div class="paragraph"><p><strong>Important fault-tolerance mechanism in Spark Streaming</strong></p></div>
<div class="ulist">
<ul>
<li>Requires Spark to periodically save metadata and data into CFS</li>
<li>Limits the state that needs to be recomputed on failure</li>
<li>Enables application recovery from a checkpoint</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Checkpointing is a fault-tolerance mechanism in Spark Streaming to avoid excessive re-computation of
previously processed data due to a failure.</p></div>
<div class="ulist">
<ul>
<li><p>
Metadata<div class="ulist">
<ul>
<li>Streaming application configuration</li>
<li>DStream operations that define a streaming application</li>
<li>Incomplete batches of data</li>
</ul>
</div></p></li>
<li><p>
Data<div class="ulist">
<ul>
<li>DStream RDDs generated by transformations</li>
</ul>
</div></p></li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="transformations-that-require-checkpointing">
<h2>Transformations that Require Checkpointing</h2>
<div class="paragraph"><p><strong>Mandatory checkpointing</strong></p></div>
<div class="ulist">
<ul>
<li>Stateful transformation <em>updateStateByKey</em></li>
<li><p>
Many window transformations<div class="ulist">
<ul>
<li><em>countByWindow</em></li>
<li><em>countByValueAndWindow</em></li>
<li>incremental <em>reduceByWindow</em></li>
<li>incremental <em>reduceByKeyAndWindow</em></li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Checkpoining is mandatory for a number of window transformations (see the slide), as well as
stateful transformation <em>updateStateByKey</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="enabling-checkpointing">
<h2>Enabling Checkpointing</h2>
<div class="paragraph"><p><strong>Setting a checkpointing directory</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

// ...

ssc.checkpoint("checkpoint_directory")

ssc.start()
ssc.awaitTermination()</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Checkpoing has a computational overhead and results in slower stream processing performance. However
checkpointing makes failure recovery much more efficient. It is all about trade-offs and
requirements of your specific use case.</p></div>
<div class="paragraph"><p>Checkpointing affects processing times. Consider not enabling it for applications that do not
require checkpointing</p></div>
<div class="ulist">
<ul>
<li>No stateful transformations</li>
<li>Driver fault tolerance is not critical</li>
<li>Some data loss is acceptable</li>
</ul>
</div>
<div class="paragraph"><p>Specifying a checkpoint directory enables checkpointing. A single statement is required before starting computation.</p></div>
<div class="paragraph"><p>In DSE, checkpoint directory is created in CFS by default.</p></div>
</div>
</div>
</section>
<section class="slide" id="configuring-a-checkpointing-interval">
<h2>Configuring a Checkpointing Interval</h2>
<div class="paragraph"><p><strong>Checkpointing interval can be adjusted for each DStream</strong></p></div>
<div class="ulist">
<ul>
<li>Default checkpointing interval is a multiple of the batch interval that is at least 10 seconds</li>
<li>Frequent checkpointing = slower performance, faster failure recovery</li>
<li>Infrequent checkpointing = faster performance, slower failure recovery</li>
<li>General recommendation: <em>5</em>-<em>10</em> x <em>slideInterval</em></li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>stream.checkpoint(Seconds(20))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Checkpointing is performed periodically. A checkpointing interval is customizable for each DStream.</p></div>
</div>
</div>
</section>
<section class="slide" id="recovering-from-a-checkpoint">
<h2>Recovering from a Checkpoint</h2>
<div class="paragraph"><p><strong>Function <em>main</em></strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>def main(args: Array[String]) {
    val sparkMasterHost = args(0)
    val cassandraHost   = args(1)
    val streamHost      = args(2)
    val checkpointDir   = args(3)

    val ssc = StreamingContext.getOrCreate(checkpointDir,
              () =&gt; {createStreamingContext(sparkMasterHost,
                     cassandraHost, streamHost, checkpointDir)})

    ssc.start()
    ssc.awaitTermination()
}</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>When checkpointing is enabled, an application can take an advantage of recovering from a
checkpoint. However, to do so, the application should be designed a bit differently.</p></div>
<div class="paragraph"><p>Take a look at the <em>main</em> function. The most important part is how a <em>StreamingContext</em> is created
using method <em>getOrCreate</em>:</p></div>
<div class="ulist">
<ul>
<li>When the program is started for the first time, it creates a new <em>StreamingContext</em> using function <em>createStreamingContext</em> that we define in the next slide.</li>
<li>When the program is restarted after a failure, it recreates a <em>StreamingContext</em> from the checkpoint data in the checkpoint directory.</li>
</ul>
</div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Function <em>createStreamingContext</em></strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>def createStreamingContext(
    sparkMasterHost: String, cassandraHost: String,
    streamHost: String, checkpointDir: String)
    : StreamingContext = {

    val conf = new SparkConf(true)
      .setAppName("Checkpointing Demo")
      .setMaster("spark://" + sparkMasterHost + ":7077")
      .set("spark.cassandra.connection.host", cassandraHost)
      .set("spark.cleaner.ttl", "3600")
      .setJars(Array(System.getProperty("user.dir") + "/target/scala-2.10/checkpointing-demo_2.10-0.1.jar"))

    val ssc = new StreamingContext(conf, Seconds(4))

    val stream = ssc.socketTextStream(streamHost, 9999)
    stream.checkpoint(Seconds(20))

    // Define computation

    ssc.checkpoint(checkpointDir)
    ssc
}</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This function is only called once, when an application is started for the first time.</p></div>
<div class="paragraph"><p>The code should be familiar.
It creates the <em>SparkConf</em> and <em>StreamingContext</em> objects, as well as sets up streams and computation.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-dstream-persistence">
<h2>The Challenge: Suboptimal Code</h2>
<div class="paragraph"><p><strong>Counting clicks per movie using Spark Streaming and Cassandra</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="clicks per movie" src="images/spark/streaming/dstream-persistence/clicks_per_movie.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream = ssc.socketTextStream(streamHost, 9999)
                .map(id =&gt; (java.util.UUID.fromString(id),1))
                .reduceByKey(_ + _)

stream.saveToCassandra("killr_video","clicks_per_movie",
                 SomeColumns("movie_id","total_clicks"))

stream.print</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Study this simple program that serves as our running example in this presentation.</p></div>
<div class="paragraph"><p>We are pre-aggregating counts per movie for each batch using Spark Streaming (transformation
<em>reduceByKey</em>) and calculating final counts using counters in Cassandra table <em>clicks_per_movie</em>.
We are also printing the first 10 partial aggregates to the screen.</p></div>
<div class="paragraph"><p>Notice that we are using two output operations (<em>saveToCassandra</em> and <em>print</em>), which requires recomputing results of transformations
<em>map</em> and <em>reduceByKey</em> twice. This is the same issue that we have seen with RDDs and multiple actions.</p></div>
<div class="paragraph"><p>This program is simple and it works. However, it is suboptimal and we will optimize this code to run faster!</p></div>
</div>
</div>
</section>
<section class="slide" id="dstream-persistence">
<h2>DStream Persistence</h2>
<div class="paragraph"><p><strong>Important optimization for DStreams used with multiple output operations</strong></p></div>
<div class="ulist">
<ul>
<li>You can instruct Spark Streaming to cache or persist RDDs in a DStream</li>
<li>Persisted DStream is kept serialized in memory (by default) once it is computed for the first time</li>
<li>Persisted DStream is reused by other operations without recomputing</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>The persistence mechanism should be used to avoid recomputing the same DStream multiple times.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The mechanism that allows us to materialize and reuse a DStream in Spark Streaming is called <em>DStream persistence</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="dstream-persistence-api">
<h2>DStream Persistence API</h2>
<div class="paragraph"><p><strong>Transformations <em>persist</em> and <em>cache</em></strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>persist</strong>([<em>storageLevel</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persists the source DStream according to a storage level specified by the optional parameter. The default storage level is
<em>org.apache.spark.storage.StorageLevel.MEMORY_ONLY_SER</em>, which prescribes persisting
elements of RDDs in the source DStream as serialized Java objects in the JVM.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>cache</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Same as <em>persist</em>() or <em>persist</em>(<em>StorageLevel.MEMORY_ONLY_SER</em>).</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>When using <em>cache</em>() or <em>persist</em>(), if data does not fit into memory,
some partitions will not be cached and will be recomputed on the fly when needed.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here are the two transformations of the DStream Persistence API.</p></div>
<div class="paragraph"><p>More storage levels for <em>persist</em> are discussed on the next slide.</p></div>
<div class="paragraph"><p><em>cache</em> is a convenient synonym of <em>persist</em> with the default storage level.</p></div>
<div class="paragraph"><p>Spark Streaming unpersists DStream RDDs automatically, as soon as partitions
are no longer needed for computation.</p></div>
<div class="paragraph"><p>The <strong>important</strong> difference between DStream Persistence API and RDD Persistence API
is that the former uses <em>MEMORY_ONLY_SER</em> as the default storage level, while the latter
uses <em>MEMORY_ONLY</em> as the default storage level.</p></div>
</div>
</div>
</section>
<section class="slide" id="storage-levels-2">
<h2>Storage Levels</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Storage Level</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>MEMORY_ONLY</em> <em>MEMORY_ONLY_SER</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting elements of RDDs in a DStreams as <em>deserialized</em> or <em>serialized</em> Java objects in the JVM, respectively. Partitions
that do not fit into memory are not cached and recomputed when needed.
<em>MEMORY_ONLY_SER</em> is more space-efficient but more CPU-intensive than <em>MEMORY_ONLY</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>MEMORY_AND_DISK</em> <em>MEMORY_AND_DISK_SER</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting elements of RDDs in a DStreams as <em>deserialized</em> or <em>serialized</em> Java objects in the JVM, respectively. Partitions
that do not fit into memory are spilled to disk.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>DISK_ONLY</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting a Dstream on disk.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>MEMORY_ONLY_2</em>, <em>MEMORY_AND_DISK_2</em>, &#8230;&#8203;</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Same as the respective storage levels above, but with replication on two nodes in a cluster.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>OFF_HEAP</em> (experimental)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting elements of RDDs in a DStreams in <em>serialized</em> format in <em>Tachyon</em> (a memory-centric distributed storage system).</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>In some cases, recomputing partitions may be faster than reading persisted partitions from disk!</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Study the different storage level possibilities for <em>persist</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="automatically-persisted-dstreams">
<h2>Automatically Persisted DStreams</h2>
<div class="paragraph"><p><strong>Explicit use of <em>persist</em> or <em>cache</em> is not required</strong></p></div>
<div class="ulist">
<ul>
<li><p>
Spark automatically persists DStreams generated by stateful transformations<div class="ulist">
<ul>
<li>Transformation <em>updateStateByKey</em></li>
<li>Window transformations</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Stateful transformations require recomputing the same batches in a DStream multiple times.
Therefore, Spark automatically persists DStreams generated by stateful transformations in memory.</p></div>
<div class="paragraph"><p>Explicit use of <em>persist</em> or <em>cache</em> is not required.</p></div>
</div>
</div>
</section>
<section class="slide" id="challenge-solution-2">
<h2>Challenge Solution</h2>
<div class="paragraph"><p><strong>Counting clicks per movie using Spark Streaming and Cassandra</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="clicks per movie" src="images/spark/streaming/dstream-persistence/clicks_per_movie.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream = ssc.socketTextStream(streamHost, 9999)
                .map(id =&gt; (java.util.UUID.fromString(id),1))
                .reduceByKey(_ + _)
                .cache

stream.saveToCassandra("killr_video","clicks_per_movie",
                 SomeColumns("movie_id","total_clicks"))

stream.print</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here is our challenge solution! We only added caching for the DStream
to make our code run faster.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-controlling-parallelism">
<h2>Controlling Parallelism</h2>
<div class="paragraph"><p><strong>Two types of tasks</strong></p></div>
<div class="ulist">
<ul>
<li>Receiver tasks = number of receivers</li>
<li>Data processing tasks = number of partitions</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>The "right" number of tasks is frequently found empirically.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>There are two types of tasks executed by <em>Executors</em> in Spark Streaming applications.</p></div>
<div class="paragraph"><p>We can control parallelism by controlling the number of receivers and the number of partitions.
There is always only one receiver per stream. The number of partitions can be controlled for
each batch/RDD in a DStream.</p></div>
</div>
</div>
</section>
<section class="slide" id="number-of-receivers">
<h2>Number of Receivers</h2>
<div class="paragraph"><p><strong>Creating multiple DStreams results in multiple receivers</strong></p></div>
<div class="ulist">
<ul>
<li>Receiving multiple streams from the same source</li>
<li>Having multiple data stream sources</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val stream1 = ssc.socketTextStream(...)
val stream2 = KafkaUtils.createStream(...)
val stream3 = KafkaUtils.createStream(...)
val stream4 = stream2.union(stream3)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To increase the number of receiver tasks, you have to increase the number of streams in your program.</p></div>
<div class="paragraph"><p>In some cases, it is straightforward to do
if an input stream source can be used to generate multiple streams (e.g., subscribe to multiple topics in a publish/subscribe system).</p></div>
<div class="paragraph"><p>In other cases, additional work may be required,
such as "manually" splitting the source into multiple sources (e.g., data in different locations in CFS).</p></div>
<div class="paragraph"><p>This example has two stream sources: TCP socket and Kafka. There are two Kafka streams that read different topics
from the same source and are unified into a single stream with <em>union</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="number-of-partitions">
<h2>Number of Partitions</h2>
<div class="paragraph"><p><strong>Three mechanisms for controlling partitioning in a DStream</strong></p></div>
<div class="listingblock">
<div class="title"><em>Default parallelism</em></div>
<div class="content">
<pre class="CodeRay"><code>val conf = new SparkConf(true).set("spark.default.parallelism","10")</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Explicit repartitioning</em></div>
<div class="content">
<pre class="CodeRay"><code>stream.repartition(2 * ssc.sparkContext.defaultParallelism)</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Operation parameters</em></div>
<div class="content">
<pre class="CodeRay"><code>stream.countByValue(2 * ssc.sparkContext.defaultParallelism)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>All three mechanisms prescribe how many partitions are created in each RDD of a DStream.
Optimal partitioning results in optimal performance.</p></div>
<div class="paragraph"><p>By controlling a number of partitions in RDDs of a DStream, we affect a number
of tasks that execute our operations and, ultimately, the level of parallelism for
each operation. If a task is running for too long and fails, re-executing the task will still
take long time. If tasks are running for a few milliseconds, then it is likely that scheduling time
will have a considerable impact on overall performance of an application. Finding a balance between
task execution and scheduling times is the first main reason to pay attention to partitioning.</p></div>
<div class="paragraph"><p>Note that there is only one repartitioning transformation for DStreams&#8201;&#8212;&#8201;<em>repartition</em>.
There are not equivalents of RDD&#8217;s <em>coalesce</em> and
<em>partitionBy</em> for DStreams.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-sql-spark-sql-basics">
<h2>Spark SQL</h2>
<div class="paragraph"><p><strong>A relational engine on top of Spark</strong></p></div>
<div class="ulist">
<ul>
<li>Starting point: <em>SQLContext</em></li>
<li>Data representation: DataFrame</li>
<li>Structured queries: SQL, language-integrated queries, HiveQL</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark SQL is a Spark module that is used to query and process structured data.</p></div>
<div class="paragraph"><p><em>SQLContext</em> is an entry point to Spark SQL functionality and is a Cassandra-aware SQL context
available via Spark-Cassandra Connector.</p></div>
<div class="paragraph"><p><em>SQLContext</em> is a Hive-aware SQL context.</p></div>
<div class="paragraph"><p>DataFrames are structured data containers that are alike to tables in relational databases.
DataFrame is the main programming abstraction in Spark SQL.</p></div>
</div>
</div>
</section>
<section class="slide" id="sqlcontext">
<h2>SQLContext</h2>
<div class="listingblock">
<div class="title"><em>Spark shell</em></div>
<div class="content">
<pre class="CodeRay"><code>scala&gt; sqlContext
org.apache.spark.sql.HiveContext</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Standalone application</em></div>
<div class="content">
<pre class="CodeRay"><code>import org.apache.spark.sql.SQLContext
// ...

val conf = new SparkConf(true)
   .setAppName("SQL Example").setMaster("spark://127.0.0.1:7077")
   .set("spark.cassandra.connection.host", "127.0.0.1")
   .setJars(Array("your-app.jar"))

   val sc = new SparkContext(conf)
   val csc = new SQLContext(sc)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This is the main entry point we will use in most cases.</p></div>
<div class="paragraph"><p>A predefined <em>SQLContext</em> object named <em>sqlContext</em> is available in DSE Spark shell.</p></div>
<div class="paragraph"><p>A standalone application needs to initialize its own <em>SQLContext</em> object.</p></div>
<div class="paragraph"><p>Most of our examples use <em>csc</em> in Spark shell.</p></div>
</div>
</div>
</section>
<section class="slide" id="dataframe">
<h2>DataFrame</h2>
<div class="paragraph"><p><strong>Main programming abstraction in Spark SQL</strong></p></div>
<div class="ulist">
<ul>
<li>Distributed collection of data organized into named columns</li>
<li>Similar to a table in a relational database</li>
<li>Has schema, rows, and rich API</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>scala&gt; val movies = sqlContext.sql("SELECT * FROM killr_video.movies")
movies: org.apache.spark.sql.DataFrame =
[movie_id: uuid, genres: array&lt;string&gt;, rating: float, release_year: int, title: string]</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>DataFrame is a representation of structured dataset that is usually a result of an SQL query.</p></div>
<div class="paragraph"><p>DataFrame has a schema and a rich API that we will discuss in a separate presentation.</p></div>
<div class="paragraph"><p>DataFrame is similar to RDD (also distributed collection of data) but has a different organization to
achieve better performance.</p></div>
<div class="paragraph"><p>The example shows a DataFrame returned by an SQL query.</p></div>
</div>
</div>
</section>
<section class="slide" id="sql-queries">
<h2>SQL Queries</h2>
<div class="paragraph"><p><strong>Declarative approach</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>sqlContext.sql(" SELECT COUNT(*) AS total         " +
        " FROM killr_video.movies_by_actor " +
        " WHERE actor = 'Johnny Depp'      ")
   .show


   +-----+
   |total|
   +-----+
   |   54|
   +-----+</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>SQL is convenient way to query Cassandra tables!</p></div>
<div class="paragraph"><p>Method <em>sql</em> takes an SQL query and returns a DataFrame with results.
The query returns the number of Johnny Depp&#8217;s movies in our Cassandra database.</p></div>
<div class="paragraph"><p>Action <em>show</em> displays the top 20 rows of DataFrame in a tabular form.</p></div>
</div>
</div>
</section>
<section class="slide" id="language-integrated-queries">
<h2>Language-Integrated Queries</h2>
<div class="paragraph"><p><strong>Functional approach</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val movies = sqlContext
    .read
    .format("org.apache.spark.sql.cassandra")
    .options(Map( "keyspace" -&gt; "killr_video", "table" -&gt; "movies_by_actor" ))
    .load

movies.filter("actor = 'Johnny Depp'")
      .agg(Map("*" -&gt; "count"))
      .withColumnRenamed("COUNT(1)", "total")
      .show

   +-----+
   |total|
   +-----+
   |   54|
   +-----+</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Language-integrated queries use various methods of <em>SQLContext</em> (<em>read</em>),
<em>DataFrameReader</em> (<em>format</em>, <em>options</em>, and <em>load</em>), and
<em>DataFrame</em>(<em>filter</em>, <em>agg</em>, <em>withColumnRenamed</em>, and <em>show</em>) to express
the same query as in the previous slide.</p></div>
</div>
</div>
</section>
<section class="slide" id="hiveql-queries">
<h2>HiveQL Queries</h2>
<div class="paragraph"><p><strong>Similar to SQL</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>sqlContext.sql(" SELECT COUNT(*) AS total         " +
       " FROM killr_video.movies_by_actor " +
       " WHERE actor = 'Johnny Depp'      ")
  .show


   +-----+
   |total|
   +-----+
   |   54|
   +-----+</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>HiveQL is syntactically very similar to SQL.</p></div>
<div class="paragraph"><p>The query returns the number of Johnny Depp&#8217;s movies in our Cassandra database.</p></div>
<div class="paragraph"><p>Action <em>show</em> displays the top 20 rows of DataFrame in a tabular form.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-sql-dataframe-creating">
<h2>Working with DataFrames</h2>
<div class="paragraph"><p><strong>Structured data processing</strong></p></div>
<div class="ulist">
<ul>
<li><p>
DataFrame<div class="ulist">
<ul>
<li>Distributed collection of data organized into named columns</li>
<li>Similar to a table in a relational database</li>
</ul>
</div></p></li>
<li><p>
Working with DataFrames<div class="ulist">
<ul>
<li><strong>Creating DataFrames</strong></li>
<li>Accessing schema and rows</li>
<li>RDD operations</li>
<li>Language-integrated queries</li>
<li>Saving DataFrames to Cassandra</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>DataFrame is a representation of structured dataset that is usually a result of an SQL query.
DataFrame is similar to RDD (also distributed collection of data) but has a different organization to
achieve better performance.</p></div>
<div class="paragraph"><p>DataFrames have a rich API that is categorized in the slide.</p></div>
<div class="paragraph"><p>This presentation discuses how DataFrames are created.</p></div>
</div>
</div>
</section>
<section class="slide" id="creating-a-dataframe-from-an-rdd">
<h2>Creating a DataFrame from an RDD</h2>
<div class="paragraph"><p><strong>Inferring schema using reflection</strong></p></div>
<div class="listingblock">
<div class="title"><em>An RDD of case class objects</em></div>
<div class="content">
<pre class="CodeRay"><code>import sqlContext.implicits._

case class Movie(title: String, year: Int)

val rdd = sc.parallelize(Array( Movie("Alice in Wonderland", 2010), ... ))
// rdd: org.apache.spark.rdd.RDD[Movie]

val df  = rdd.toDF()
// df: org.apache.spark.sql.DataFrame = [title: string, year: int]</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In this example, we are using function <em>toDF</em> with no arguments. Column names
are inferred from the case class.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Inferring schema using reflection</strong></p></div>
<div class="listingblock">
<div class="title"><em>An RDD of tuples</em></div>
<div class="content">
<pre class="CodeRay"><code>import sqlContext.implicits._

val rdd = sc.parallelize(Array( ("Alice in Wonderland", 2010), ... ))
// rdd: org.apache.spark.rdd.RDD[(String, Int)]

val df  = rdd.toDF("title", "year")
// df: org.apache.spark.sql.DataFrame = [title: string, year: int]</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In this example, we are using function <em>toDF</em> with column names as arguments.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Specifying schema programmatically</strong></p></div>
<div class="listingblock">
<div class="title"><em>An RDD of Rows</em></div>
<div class="content">
<pre class="CodeRay"><code>import org.apache.spark.sql.Row
import org.apache.spark.sql.types._

val rdd = sc.parallelize(Array( ("Alice in Wonderland", 2010), ... ))
            .map{case(t,y) =&gt; Row(t,y)}
// rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]

val schema = StructType( List (
                 StructField("title", StringType,  false),
                 StructField("year",  IntegerType, false) ) )
                 // true = nullable, false = not nullable

val df = sqlContext.createDataFrame(rdd, schema)
// df: org.apache.spark.sql.DataFrame = [title: string, year: int]</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>In this example, we are using method <em>createDataFrame</em> to apply a schema to an RDD of Rows.</p></div>
</div>
</div>
</section>
<section class="slide" id="creating-a-dataframe-from-a-cassandra-table">
<h2>Creating a DataFrame from a Cassandra Table</h2>
<div class="paragraph"><p><strong>Specifying an SQL query</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val df = sqlContext.sql("SELECT * FROM killr_video.movies")

// df: org.apache.spark.sql.DataFrame =
// [movie_id: uuid, genres: array&lt;string&gt;, rating: float, release_year: int, title: string]</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Depending on an SQL query, this approach can create a DataFrame based on multiple tables, too.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Using <em>DataFrameReader</em></strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val df = sqlContext.read
            .format("org.apache.spark.sql.cassandra")
            .options(Map( "keyspace" -&gt; "killr_video", "table" -&gt; "movies" ))
            .load

// df: org.apache.spark.sql.DataFrame =
// [movie_id: uuid, genres: array&lt;string&gt;, rating: float, release_year: int, title: string]</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Method <em>read</em> returns a <em>DataFrameReader</em> that is provided with a specific format and options to load data from
Cassandra into a DataFrame.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-sql-dataframe-schema-rows">
<h2>Working with DataFrames</h2>
<div class="paragraph"><p><strong>Structured data processing</strong></p></div>
<div class="ulist">
<ul>
<li><p>
DataFrame<div class="ulist">
<ul>
<li>Distributed collection of data organized into named columns</li>
<li>Similar to a table in a relational database</li>
</ul>
</div></p></li>
<li><p>
Working with DataFrames<div class="ulist">
<ul>
<li>Creating DataFrames</li>
<li><strong>Accessing schema and rows</strong></li>
<li>RDD operations</li>
<li>Language-integrated queries</li>
<li>Saving DataFrames to Cassandra</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>DataFrame is a representation of structured dataset that is usually a result of an SQL query.
DataFrame is similar to RDD (also distributed collection of data) but has a different organization to
achieve better performance.</p></div>
<div class="paragraph"><p>DataFrames have a rich API that is categorized in the slide.</p></div>
<div class="paragraph"><p>This presentation deals with schema and rows in DataFrames.</p></div>
</div>
</div>
</section>
<section class="slide" id="dataframe-schema">
<h2>DataFrame Schema</h2>
<div class="paragraph"><p><strong>Human-readable output</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/spark-sql/dataframe-schema-rows/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val df = sqlContext.sql("SELECT * FROM killr_video.movies")
df.printSchema()</code></pre>
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>root
 |-- movie_id: uuid (nullable = true)
 |-- genres: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- rating: float (nullable = true)
 |-- release_year: integer (nullable = true)
 |-- title: string (nullable = true)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Method <em>printSchema</em> prints the schema to the console in a tree format.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Column names and types</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/spark-sql/dataframe-schema-rows/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val df = sqlContext.sql("SELECT * FROM killr_video.movies")
val schema = df.dtypes

// schema: Array[(String, String)] =
//   Array((movie_id,UUIDType),
//         (genres,ArrayType(StringType,true)),
//         (rating,FloatType),
//         (release_year,IntegerType),
//         (title,StringType))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Method <em>dtypes</em> returns all column names and their data types as an array.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Complete schema definition</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/spark-sql/dataframe-schema-rows/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val df = sqlContext.sql("SELECT * FROM killr_video.movies")
val schema = df.schema

// schema: org.apache.spark.sql.types.StructType =
//   StructType(StructField(movie_id,UUIDType,true),
//              StructField(genres,ArrayType(StringType,true),true),
//              StructField(rating,FloatType,true),
//              StructField(release_year,IntegerType,true),
//              StructField(title,StringType,true))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Method <em>schema</em> returns the schema of a DataFrame.</p></div>
</div>
</div>
</section>
<section class="slide" id="dataframe-rows">
<h2>DataFrame Rows</h2>
<div class="paragraph"><p><strong>Accessing primitive values</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/spark-sql/dataframe-schema-rows/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val df = sqlContext.sql(" SELECT title, release_year " +
                 " FROM killr_video.movies " +
                 " WHERE title = 'Alice in Wonderland'")
// df: org.apache.spark.sql.DataFrame = [title: string, release_year: int]

val row = df.first
// row: org.apache.spark.sql.Row = [Alice in Wonderland,2010]

println(row(0))          // Alice in Wonderland

println(row.isNullAt(1)) // false

println(row.getInt(1))   // 2010</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Two approaches:</p></div>
<div class="ulist">
<ul>
<li>Using an index of a column in a row</li>
<li>Using getters like getString(i:Int), getInt(i:Int), getDouble(i:Int), etc.</li>
</ul>
</div>
<div class="paragraph"><p>The second approach is less flexible as not all data types may have getters defined (e.g., collection columns).
In addition, only the second approach throws an exception when accessing a null value, so
the use of <em>isNullAt</em> may be required.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Accessing complex values</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/spark-sql/dataframe-schema-rows/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val df = sqlContext.sql(" SELECT genres " +
                 " FROM killr_video.movies " +
                 " WHERE title = 'Alice in Wonderland'")
// df: org.apache.spark.sql.DataFrame = [genres: array&lt;string&gt;]

val row = df.first
// row: org.apache.spark.sql.Row = [ArrayBuffer(Adventure, Family, Fantasy)]

row(0).asInstanceOf[Seq[String]].foreach(println)
// Adventure
// Family
// Fantasy</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Collection column <em>genres</em> is used as an example.</p></div>
<div class="paragraph"><p><strong>Explicit</strong> type conversion using <em>asInstanceOf</em> is required.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-sql-dataframe-rdd-operations">
<h2>Working with DataFrames</h2>
<div class="paragraph"><p><strong>Structured data processing</strong></p></div>
<div class="ulist">
<ul>
<li><p>
DataFrame<div class="ulist">
<ul>
<li>Distributed collection of data organized into named columns</li>
<li>Similar to a table in a relational database</li>
</ul>
</div></p></li>
<li><p>
Working with DataFrames<div class="ulist">
<ul>
<li>Creating DataFrames</li>
<li>Accessing schema and rows</li>
<li><strong>RDD operations</strong></li>
<li>Language-integrated queries</li>
<li>Saving DataFrames to Cassandra</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>DataFrame is a representation of structured dataset that is usually a result of an SQL query.
DataFrame is similar to RDD (also distributed collection of data) but has a different organization to
achieve better performance.</p></div>
<div class="paragraph"><p>DataFrames have a rich API that is categorized in the slide.</p></div>
<div class="paragraph"><p>This presentation discusses RDD-like operations on DataFrames.</p></div>
</div>
</div>
</section>
<section class="slide" id="repartitioning-and-persistence-transformations">
<h2>Repartitioning and Persistence Transformations</h2>
<div class="paragraph"><p><strong>DataFrame in, DataFrame out</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>repartition</strong>(<em>numPartitions</em>)</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">DataFrame repartitioning transformations</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>coalesce</strong>(<em>numPartitions</em>)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>persist</strong>([<em>storageLevel</em>])</p></td>
<td class="tableblock halign-left valign-top" rowspan="3"><p class="tableblock">DataFrame persistence transformations</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>cache</strong>()</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>unpersist</strong>()</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The semantics is identical to the corresponding RDD transformations.</p></div>
<div class="paragraph"><p>Similar to RDDs, repartitioning is done to increase or decrease parallelism.
Persistence transformations are used to materialized DataFrames that are reused
by multiple actions to avoid recomputation.</p></div>
</div>
</div>
</section>
<section class="slide" id="dataframe-to-rdd-transformations">
<h2>DataFrame-to-RDD Transformations</h2>
<div class="paragraph"><p><strong>DataFrame in, RDD out</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>map</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD is formed by applying a function <em>f</em> on each row of
the source DataFrame. There is a <em>one-to-one</em> correspondence between DataFrame rows and RDD elements.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>flatMap</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD is formed by applying a function <em>f</em> on each row of
the source DataFrame. There is a <em>one-to-many</em> correspondence between DataFrame rows and RDD elements
if <em>f</em> returns a collection with more than one element.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>rdd</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD of rows is formed from the source DataFrame.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>toJSON</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new RDD of JSON strings is formed by converting rows to JSON in the source DataFrame.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The semantics of <em>map</em> and <em>flatMap</em> is identical to the corresponding RDD transformations.</p></div>
<div class="paragraph"><p>Transformations <em>rdd</em> and <em>toJSON</em> return RDDs of row objects and JSON strings, respectively.</p></div>
</div>
</div>
</section>
<section class="slide" id="dataframe-actions">
<h2>DataFrame Actions</h2>
<div class="paragraph"><p><strong>Triggering computation</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Action</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>collect</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns an <em>Array</em> with all rows of the source DataFrame.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>count</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns a total number of rows in the source DataFrame.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>first</strong>() or <strong>head</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns the first row of the source DataFrame.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>take</strong>(<em>n</em>) or <strong>head</strong>(<em>n</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Returns an <em>Array</em> with the first<em>n</em>rows of the source DataFrame.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>show</strong>([<em>n</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Displays the first <em>n</em> rows of the source DataFrame in a tabular form.
Strings more than 20 characters are truncated.
By default, <em>n</em> = <em>20</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>foreach</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Executes a function<em>f</em>on each row of the source DataFrame.
The function usually implements a side effect, such as updating an <em>accumulator</em> variable
or interacting with an external system.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Most of these DataFrame actions have RDD counterparts and should be familiar.</p></div>
<div class="paragraph"><p><em>show</em> is probably the most interesting new actions that displays data in a tabular
form, which makes sense for structured data. We will use this action a lot in our examples.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-5">
<h2>Example</h2>
<div class="paragraph"><p><strong>Counting and displaying Johnny Depp&#8217;s movies</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-sql/dataframe-rdd-operations/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val df = sqlContext.sql(" SELECT title, release_year, rating " +
                 " FROM killr_video.movies_by_actor " +
                 " WHERE actor = 'Johnny Depp'")
            .coalesce(1).cache
println("Total: " + df.count)
df.show(4)

// Total: 54
// +--------------------+------------+------+
// |               title|release_year|rating|
// +--------------------+------------+------+
// |Pirates of the Ca...|        2017|  null|
// |Alice Through the...|        2016|  null|
// |         Yoga Hosers|        2015|  null|
// |           Mortdecai|        2015|   5.5|
// +--------------------+------------+------+</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>We are using <em>coalesce</em> to decrease the number of partitions to 1 because the dataset
retrieved from Cassandra is tiny and parallelism will not be of much use.</p></div>
<div class="paragraph"><p>We are using <em>cache</em> to materialize and store the DataFrame in main memory because
two actions, <em>count</em> and <em>show</em>, are used.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-sql-dataframe-language-integrated">
<h2>Working with DataFrames</h2>
<div class="paragraph"><p><strong>Structured data processing</strong></p></div>
<div class="ulist">
<ul>
<li><p>
DataFrame<div class="ulist">
<ul>
<li>Distributed collection of data organized into named columns</li>
<li>Similar to a table in a relational database</li>
</ul>
</div></p></li>
<li><p>
Working with DataFrames<div class="ulist">
<ul>
<li>Creating DataFrames</li>
<li>Accessing schema and rows</li>
<li>RDD operations</li>
<li><strong>Language-integrated queries</strong></li>
<li>Saving DataFrames to Cassandra</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>DataFrame is a representation of structured dataset that is usually a result of an SQL query.
DataFrame is similar to RDD (also distributed collection of data) but has a different organization to
achieve better performance.</p></div>
<div class="paragraph"><p>DataFrames have a rich API that is categorized in the slide.</p></div>
<div class="paragraph"><p>This presentation discusses the DataFrame language-intergrated query API.</p></div>
</div>
</div>
</section>
<section class="slide" id="dataframe-query-api">
<h2>DataFrame Query API</h2>
<div class="paragraph"><p><strong>Unary transformations</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>select</strong>(<em>columns</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DataFrame is formed by selecting a set of <em>columns</em> from the source DataFrame.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>withColumnRenamed</strong>( <em>oldColumn</em>, <em>newColumn</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DataFrame is formed by renaming a column in the source DataFrame.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>distinct</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DataFrame is formed by eliminating duplicate rows in the source DataFrame.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>where</strong>(<em>condition</em>) or <strong>filter</strong>(<em>condition</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DataFrame is formed by rows from the source DataFrame that satisfy a <em>condition</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>agg</strong>(<em>column-aggregates</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DataFrame is formed by applying aggregate functions to columns in the source DataFrame.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>groupBy</strong>(<em>columns</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DataFrame is formed by grouping data based on <em>columns</em> in the source DataFrame.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>orderBy</strong>(<em>columns</em>) or <strong>sort</strong>(<em>columns</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DataFrame is formed by sorting rows based on <em>columns</em> of the source DataFrame
in the ascending (default) or descending order.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>limit</strong>(<em>n</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DataFrame is formed by the first <em>n</em> rows of the source DataFrame.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>If DataFrame is equivalent to a table in a relational database, these transformations
is an implementation of the relational algebra.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Binary transformations</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>join</strong>(<em>otherDF</em>, [<em>condition</em>], [<em>type</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DataFrame is formed by joining the source DataFrame and <em>otherDF</em> based on an optional <em>condition</em>.
The optional join <em>type</em> parameter allows switching between inner (default) and outer joins.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>unionAll</strong>(<em>otherDF</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DataFrame is formed by all rows of the source DataFrame and <em>otherDF</em>.
The source DataFrame and <em>otherDF</em> must be union-compatible. Duplicate rows in the result are retained.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>intersect</strong>(<em>otherDF</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DataFrame is formed by only those rows that appear in both the source DataFrame and <em>otherDF</em>.
The source DataFrame and <em>otherDF</em> must be union-compatible. Duplicate rows in the result are eliminated.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>except</strong>(<em>otherDF</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DataFrame is formed by rows that appear in the source DataFrame but not in <em>otherDF</em>.
The source DataFrame and <em>otherDF</em> must be union-compatible. Duplicate rows in the result are eliminated.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>If DataFrame is equivalent to a table in a relational database, these transformations
is an implementation of the relational algebra.</p></div>
</div>
</div>
</section>
<section class="slide" id="supporting-functions">
<h2>Supporting Functions</h2>
<div class="paragraph"><p><strong>import org.apache.spark.sql.functions._</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Category</th>
<th class="tableblock halign-left valign-top">Sample functions</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Aggregate functions</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>avg</em>, <em>count</em>, <em>max</em>, <em>min</em>, <em>sum</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Collection functions</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>array_contains</em>, <em>sort_array</em>, <em>size</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Date-time functions</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>current_date</em>, <em>current_timestamp</em>, <em>second</em>, <em>hour</em>, <em>month</em>, <em>year</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Math functions</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>ceil</em>, <em>floor</em>, <em>round</em>, <em>pow</em>, <em>sqrt</em>, <em>log</em>, <em>sum</em>, <em>sin</em>, <em>cos</em>, <em>tan</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Sorting functions</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>asc</em>, <em>desc</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">String functions</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>concat</em>, <em>length</em>, <em>substring</em>, <em>trim</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">UDF functions</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>udf</em>, <em>callUDF</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Window functions</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>denseRank</em>, <em>percentRank</em>, <em>rank</em>, <em>lag</em>, <em>lean</em></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Miscellaneous functions</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>col</em>, <em>column</em>, <em>rand</em>, <em>randn</em></p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>There are dozens of those functions that can be imported to your program.
This is a very incomplete list.
We will not be able to cover each one in any great detail.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-6">
<h2>Example</h2>
<div class="paragraph"><p><strong>STEP 1: Creating a DataFrame from a Cassandra table</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-sql/dataframe-language-integrated/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val df = sqlContext.read
            .format("org.apache.spark.sql.cassandra")
            .options(Map( "keyspace" -&gt; "killr_video",
                          "table" -&gt; "movies_by_actor" ))
            .load</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Creating a DataFrame using <em>DataFrameReader</em>.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>STEP 2: Executing a language-integrated query</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-sql/dataframe-language-integrated/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>import org.apache.spark.sql.functions._

df.filter("actor = 'Johnny Depp'")
  .groupBy("release_year")
  .agg(Map("*" -&gt; "count", "rating" -&gt; "avg"))
  .withColumnRenamed("COUNT(1)", "total_movies")
  .withColumnRenamed("AVG(rating)", "average_rating")
  .select("release_year", "total_movies", "average_rating")
  .orderBy(desc("total_movies"), desc("average_rating"))
  .limit(3)
  .show

// +------------+------------+-----------------+
// |release_year|total_movies|   average_rating|
// +------------+------------+-----------------+
// |        2004|           4|6.850000023841858|
// |        2000|           3|6.933333396911621|
// |        2011|           3|6.733333269755046|
// +------------+------------+-----------------+</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>We are looking for Johnny Depp&#8217;s 3 most productive years in terms of the number of released movies and
their average ratings.</p></div>
<div class="paragraph"><p>Of course, we could have solved this problem using SQL, but this is a topic of another presentation.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-sql-dataframe-saving">
<h2>Working with DataFrames</h2>
<div class="paragraph"><p><strong>Structured data processing</strong></p></div>
<div class="ulist">
<ul>
<li><p>
DataFrame<div class="ulist">
<ul>
<li>Distributed collection of data organized into named columns</li>
<li>Similar to a table in a relational database</li>
</ul>
</div></p></li>
<li><p>
Working with DataFrames<div class="ulist">
<ul>
<li>Creating DataFrames</li>
<li>Accessing schema and rows</li>
<li>RDD operations</li>
<li>Language-integrated queries</li>
<li><strong>Saving DataFrames to Cassandra</strong></li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>DataFrame is a representation of structured dataset that is usually a result of an SQL query.
DataFrame is similar to RDD (also distributed collection of data) but has a different organization to
achieve better performance.</p></div>
<div class="paragraph"><p>DataFrames have a rich API that is categorized in the slide.</p></div>
<div class="paragraph"><p>This presentation discusses how to save a DataFrame into a Cassandra table.</p></div>
</div>
</div>
</section>
<section class="slide" id="running-example">
<h2>Running Example</h2>
<div class="paragraph"><p><strong>Simple ETL scenario</strong></p></div>
<div class="olist arabic">
<ol class="arabic">
<li>
Reading from table <em>movies</em>
</li>
<li>
Filtering movies by genre "Family"
</li>
<li>
Saving into table <em>family_movies</em>
</li>
</ol>
</div>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="challenge" src="images/spark/spark-sql/dataframe-saving/challenge.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>We will learn by example.</p></div>
</div>
</div>
</section>
<section class="slide" id="reading-from-a-cassandra-table">
<h2>Reading from a Cassandra Table</h2>
<div class="paragraph"><p><strong>Using <em>DataFrameReader</em></strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/spark-sql/dataframe-saving/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val movieDF = sqlContext.read
                 .format("org.apache.spark.sql.cassandra")
                 .options(Map( "keyspace" -&gt; "killr_video",
                               "table" -&gt; "movies" ))
                 .load</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Creating a DataFrame using <em>DataFrameReader</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="filtering-a-dataframe">
<h2>Filtering a DataFrame</h2>
<div class="paragraph"><p><strong>Using language-integrated query API</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>import org.apache.spark.sql.functions._

val familyDF = movieDF.filter(col("genres").contains("Family"))

familyDF.show

// +--------------------+--------------------+------+------------+--------------------+
// |            movie_id|              genres|rating|release_year|               title|
// +--------------------+--------------------+------+------------+--------------------+
// |3391c072-af52-4d3...|ArrayBuffer(Adven...|  null|        2016|Alice Through the...|
// |87032a77-0ccd-416...|ArrayBuffer(Biogr...|   7.8|        2004|   Finding Neverland|
// |f8ecbd4a-c3e0-41e...|ArrayBuffer(Adven...|   6.5|        2010| Alice in Wonderland|
// |89ccddea-a845-499...|ArrayBuffer(Adven...|   6.7|        2005|Charlie and the C...|
// +--------------------+--------------------+------+------------+--------------------+</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>DataFrame <em>familyDF</em> now contains only family-genre movies.</p></div>
<div class="paragraph"><p>Action <em>show</em> is used to give a preview. We will save <em>familyDF</em> into a Cassandra table next.</p></div>
</div>
</div>
</section>
<section class="slide" id="saving-a-dataframe-into-a-cassandra-table">
<h2>Saving a DataFrame into a Cassandra Table</h2>
<div class="paragraph"><p><strong>Using <em>DataFrameWriter</em></strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="family movies" src="images/spark/spark-sql/dataframe-saving/family_movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>familyDF.write
        .format("org.apache.spark.sql.cassandra")
        .options(Map( "keyspace" -&gt; "killr_video",
                      "table" -&gt; "family_movies" ))
        .save</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Method <em>write</em> returns a <em>DataFrameWriter</em> that is provided with a specific format and options to save data into
a Cassandra table.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-sql-querying-cassandra-sql">
<h2>Executing SQL Queries</h2>
<div class="paragraph"><p><strong><em>SQLContext</em> API</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Method</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>setKeyspace</strong>(<em>keyspace</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Sets a default Cassandra <em>keyspace</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>sql</strong>(<em>query</em>) or <strong>cassandraSql</strong>(<em>query</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Executes an SQL <em>query</em> and returns a DataFrame.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>If your program queries one keyspace, it is convenient to use <em>setKeyspace</em>.
Otherwise, qualifying table names with keyspace name would do the job.</p></div>
<div class="paragraph"><p><em>sql</em> and <em>cassandraSql</em> are equivalent.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong><em>DataFrame</em> API</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Method</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>registerTempTable</strong>( <em>tableName</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Registers the source DataFrame as a temporary table that can be used in SQL queries by name <em>tableName</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>explain</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Prints a physical query execution plan to the console.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here are two more useful calls.</p></div>
<div class="paragraph"><p>You can always register a DataFrame as a temporary table that can then be used in SQL queries.
Temporary tables are not stored to Cassandra. They exist in Spark only.</p></div>
<div class="paragraph"><p><em>explain</em> prints a physical query execution plan to the console. You can also access DataFrame&#8217;s property <em>queryExecution</em>
to take a look at a logical execution plan.</p></div>
</div>
</div>
</section>
<section class="slide" id="sql-syntax">
<h2>SQL Syntax</h2>
<div class="paragraph"><p><strong>Simplified grammar</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>SELECT [DISTINCT] [column-names] | [wildcard]
FROM [kesypace name.]table-name
[JOIN-clause table-name ON join-condition]
[WHERE condition]
[GROUP BY column-names]
[HAVING condition]
[ORDER BY column-names [ASC | DESC]]
[LIMIT number-of-rows]

JOIN-clause --&gt; [JOIN | INNER JOIN | LEFT SEMI JOIN |
LEFT [OUTER] JOIN | RIGHT [OUTER] JOIN | FULL [OUTER] JOIN]

SELECT statement-1
[UNION | UNION ALL | UNION DISTINCT | INTERSECT | EXCEPT]
SELECT statement-2</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>We assume you are familiar with SQL.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-querying-a-cassandra-table">
<h2>Example: Querying a Cassandra Table</h2>
<div class="paragraph"><p><strong>Find the largest rating for Johnny Depp&#8217;s movie</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-sql/querying-cassandra-sql/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>sqlContext.setKeyspace("killr_video")

val maxDF = sqlContext.sql(
    " SELECT actor, MAX(rating) AS max_rating " +
    " FROM movies_by_actor                    " +
    " WHERE actor = 'Johnny Depp'             " +
    " GROUP BY actor                          " )

maxDF.show

// +-----------+----------+
// |      actor|max_rating|
// +-----------+----------+
// |Johnny Depp|       8.6|
// +-----------+----------+</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Simple query over a single table with grouping and aggregation for demonstration purposes.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-querying-a-temporary-table">
<h2>Example: Querying a Temporary Table</h2>
<div class="paragraph"><p><strong>Find Johnny Depp&#8217;s movies with ratings higher than the largest rating - 1</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-sql/querying-cassandra-sql/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>maxDF.registerTempTable("max_rating")

val movieDF = sqlContext.sql(
    " SELECT M.actor, title, release_year, rating    " +
    " FROM max_rating AS R JOIN movies_by_actor AS M " +
    " ON (R.actor = M.actor)                         " +
    " WHERE rating &gt; max_rating - 1                  " +
    " ORDER BY release_year DESC, rating DESC        " )

movieDF.show(2)
// +-----------+--------------------+------------+------+
// |      actor|               title|release_year|rating|
// +-----------+--------------------+------------+------+
// |Johnny Depp|   Finding Neverland|        2004|   7.8|
// |Johnny Depp|Pirates of the Ca...|        2003|   8.1|
// +-----------+--------------------+------------+------+</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The DataFrame is registered as a temporary table and joined with the Cassandra table.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-query-execution-plan">
<h2>Example: Query Execution Plan</h2>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>movieDF.explain

// Sample output:
== Physical Plan ==
Project [actor#116,title#121,release_year#117]
 Sort [release_year#117 DESC,rating#120 DESC], true
  Exchange (RangePartitioning 200)
   Project [actor#116,title#121,release_year#117,rating#120]
    Filter (rating#120 &gt; (max_rating#98 - 1.0))
     ShuffledHashJoin [actor#2], [actor#116], BuildRight
      Aggregate false, [actor#2], [actor#2,MAX(PartialMax#123) AS max_rating#98]
       Exchange (HashPartitioning 200)
        Aggregate true, [actor#2], [actor#2,MAX(rating#6) AS PartialMax#123]
         Filter (actor#2 = Johnny Depp)
          PhysicalRDD [actor#2,rating#6], MapPartitionsRDD[202] at explain at &lt;console&gt;:57
      Exchange (HashPartitioning 200)
       PhysicalRDD [rating#120,actor#116,title#121,release_year#117], MapPartitionsRDD[204] at explain at &lt;console&gt;:57</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This information may be useful if you know how query optimization in Spark works.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-spark-sql-writing-efficient-sql">
<h2>Predicate Pushdown Optimizations</h2>
<div class="paragraph"><p><strong>Automatic optimizations by <em>Spark-Cassandra Connector</em></strong></p></div>
<div class="ulist">
<ul>
<li>Filtering on a partition key is pushed down to Cassandra</li>
<li>Filtering on a clustering key is pushed down to Cassandra</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p><em>Spark-Cassandra Connector</em> pushes any predicate that is valid in CQL down to Cassandra.
Choosing the best Cassandra table for a query can substantially improve performance.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>Spark-Cassandra Connector</em> automatically pushs down valid WHERE clauses to Cassandra as long as
the pushdown option is enabled (it is enabled by default).</p></div>
</div>
</div>
</section>
<section class="slide" id="choosing-the-best-table-for-a-query">
<h2>Choosing the Best Table for a Query</h2>
<div class="paragraph"><p><strong>One query, two tables</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>SELECT release_year, title
FROM killr_video.???
WHERE actor = 'Johnny Depp' AND release_year &lt; 2015
ORDER BY release_year DESC</code></pre>
</div>
</div>
<div class="imageblock" style="float: left">
<div class="content">
<img alt="schema" src="images/spark/spark-sql/writing-efficient-sql/schema.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Query: Find release years and titles of Johnny Depp&#8217;s movies released before 2015 and show results
in descending release year order.</p></div>
<div class="paragraph"><p>The query is expressed in SQL with the ??? placeholder for a table name.</p></div>
<div class="paragraph"><p>Both tables can be used to answer the query. Which one will you choose?
Pay attention to table partition and clustering keys.</p></div>
<div class="paragraph"><p>Understanding the Cassandra data and query models is very important!</p></div>
</div>
</div>
</section>
<section class="slide" id="clustering-key-predicate-can-be-pushed">
<h2>Clustering Key Predicate can be Pushed</h2>
<div class="paragraph"><p><strong>actor = 'Johnny Depp'</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="actors by movie" src="images/spark/spark-sql/writing-efficient-sql/actors_by_movie.svg" />
</div>
</div>
<div class="listingblock left">
<div class="title"><em>SQL query</em></div>
<div class="content">
<pre class="CodeRay"><code>sqlContext.sql(" SELECT release_year, title " +
        " FROM killr_video.actors_by_movie " +
        " WHERE actor = 'Johnny Depp' AND release_year &lt; 2015 " +
        " ORDER BY release_year DESC").show</code></pre>
</div>
</div>
<div class="listingblock left">
<div class="title"><em>Language-integrated query</em></div>
<div class="content">
<pre class="CodeRay"><code>import org.apache.spark.sql.functions._
val df = sqlContext.read.format("org.apache.spark.sql.cassandra")
            .options(Map( "keyspace" -&gt; "killr_video",
                          "table" -&gt; "actors_by_movie" )).load
df.filter("actor = 'Johnny Depp' AND release_year &lt; 2015")
  .select("release_year", "title")
  .orderBy(desc("release_year")).show</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Both SQL and language-integrated queries are equivalent. They return equivalent but
not necessarily identical results: ordering of rows within the same year group may be different.</p></div>
<div class="paragraph"><p>This is not the best table choice.</p></div>
<div class="paragraph"><p>Even though <em>actor = 'Johnny Depp'</em> can be pushed down to Cassandra, the query is still
expensive because Cassandra will have to retrieve matching rows from many partitions.
Once results are delivered from Cassandra, Spark still has to filter based on <em>release_year &lt; 2015</em>
and perform sorting.</p></div>
</div>
</div>
</section>
<section class="slide" id="all-predicates-can-be-pushed">
<h2>All Predicates can be Pushed</h2>
<div class="paragraph"><p><strong>actor = 'Johnny Depp' AND release_year &lt; 2015</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies by actor" src="images/spark/spark-sql/writing-efficient-sql/movies_by_actor.svg" />
</div>
</div>
<div class="listingblock left">
<div class="title"><em>SQL query</em></div>
<div class="content">
<pre class="CodeRay"><code>sqlContext.sql(" SELECT release_year, title " +
        " FROM killr_video.movies_by_actor " +
        " WHERE actor = 'Johnny Depp' AND release_year &lt; 2015").show</code></pre>
</div>
</div>
<div class="listingblock left">
<div class="title"><em>Language-integrated query</em></div>
<div class="content">
<pre class="CodeRay"><code>val df = sqlContext.read.format("org.apache.spark.sql.cassandra")
            .options(Map( "keyspace" -&gt; "killr_video",
                          "table" -&gt; "movies_by_actor" )).load
df.filter("actor = 'Johnny Depp' AND release_year &lt; 2015")
  .select("release_year", "title").show</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Better table choice = faster and simpler query!</p></div>
<div class="paragraph"><p>Much better performance: the whole WHERE clause can be pushed down to Cassandra;
Cassandra will access only one partition very efficiently.</p></div>
<div class="paragraph"><p>Simpler query: because data in the Cassandra partition for Johnny Depp is already ordered
by clustering column <em>release_year</em>, no need to use ORDER BY.</p></div>
</div>
</div>
</section>
<section class="slide" id="sample-output-2">
<h2>Sample Output</h2>
<div class="paragraph"><p><strong>All queries return equivalent results</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>+------------+--------------------+
|release_year|               title|
+------------+--------------------+
|        2014|       Transcendence|
|        2014|      Into the Woods|
|        2014|                Tusk|
|        2013|          Lucky Them|
|        2013|     The Lone Ranger|
|        2012|        Dark Shadows|
|        2011|               Rango|
|        2011|       The Rum Diary|
|        2011|Pirates of the Ca...|
|        2010|         The Tourist|
|        2010| Alice in Wonderland|
|        2009|The Imaginarium o...|
|        2009|      Public Enemies|
|        2007|Pirates of the Ca...|
|        2007|Sweeney Todd: The...|
|        2006|Pirates of the Ca...|
|        2005|Charlie and the C...|
|        2004|       The Libertine|
|        2004|       Secret Window|
|        2004|   Finding Neverland|
+------------+--------------------+</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The four queries return equivalent but
not necessarily identical results: ordering of rows within the same year group may be different.</p></div>
<div class="paragraph"><p>The outputs are equivalent, the response times are not!</p></div>
</div>
</div>
</section>
<div aria-role="navigation">
<a class="deck-prev-link" href="#" title="Previous">
<i class="icon-chevron-with-circle-left"></i>
</a>
<a class="deck-next-link" href="#" title="Next">
<i class="icon-chevron-with-circle-right"></i>
</a>
</div>
</div>
<script src="deck.js/jquery.min.js"></script>
<script src="deck.js/d3.v2.js"></script>
<script src="deck.js/jquery-ui.min.js"></script>
<script src="deck.js/core/deck.core.js"></script>
<script src="deck.js/extensions/scale/deck.scale.js"></script>
<script src="deck.js/extensions/navigation/deck.navigation.js"></script>
<script src="deck.js/extensions/split/deck.split.js"></script>
<script src="deck.js/extensions/animation/deck.animation.js"></script>
<script src="deck.js/extensions/deck.js-notes/deck.notes.js"></script>
<script src="deck.js/extensions/goto/deck.goto.js"></script>
<script src="deck.js/extensions/clone/deck.clone.js"></script>
<script src="deck.js/extensions/svg/svg.min.js"></script>
<script src="js/course.js"></script>
<footer>
<div class="flex-element deck-course">
<p>&copy; 2016 DataStax. Use only with permission. &bull;
<span class="course-title">DS320: Spark</span></p>
</div>
<div class="flex-element deck-brand">
<a href="http://academy.datastax.com" target="blank">DataStax Academy</a>
</div>
<div class="deck-progressbar">
<span></span>
</div>
</footer>
<script type="text/javascript">
  //<![CDATA[
    (function($, deck, undefined) {
      $.deck.defaults.keys['previous'] = [8, 33, 37, 39];
      $.deck.defaults.keys['next'] = [13, 32, 34, 39];
    
      $.extend(true, $[deck].defaults, {
          countNested: false
      });
    
      $.deck('.slide');
      $.deck('disableScale');
    })(jQuery, 'deck');
  //]]>
</script>
<script type="text/javascript">
  //<![CDATA[
    $(document).bind('deck.change', function(event, from, to) {
      var width = to / ($.deck('getSlides').length - 1) * 100;
      $('.deck-progressbar span').css('width', width + '%');
    });
  //]]>
</script>
</body>
</html>